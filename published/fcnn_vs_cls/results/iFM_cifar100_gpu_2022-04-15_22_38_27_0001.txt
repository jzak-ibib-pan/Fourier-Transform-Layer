Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      5.931657 ||   0.1662 ||   0.3780 ||  0.746994 ||  0.750678 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.787630 ||   0.1483 ||   0.3658 ||  0.741246 ||  0.747640 ||      3.629339 ||      0.1735 ||   0.4080 ||  0.836106 ||  0.840905 ||    19.011568 || 
    Epoch 01    --      3.394813 ||   0.2197 ||   0.4747 ||  0.859750 ||  0.867252 ||      3.397188 ||      0.2182 ||   0.4748 ||  0.864666 ||  0.867778 ||    16.664395 || 
    Epoch 02    --      3.208395 ||   0.2530 ||   0.5213 ||  0.879653 ||  0.886218 ||      3.239551 ||      0.2537 ||   0.5153 ||  0.877665 ||  0.881738 ||    16.957851 || 
    Epoch 03    --      3.063417 ||   0.2849 ||   0.5589 ||  0.892268 ||  0.898088 ||      3.101496 ||      0.2701 ||   0.5445 ||  0.892606 ||  0.895764 ||    17.208414 || 
    Epoch 04    --      2.933204 ||   0.3096 ||   0.5857 ||  0.903808 ||  0.909096 ||      2.987664 ||      0.2971 ||   0.5797 ||  0.902643 ||  0.904153 ||    16.802724 || 
    Epoch 05    --      2.810705 ||   0.3340 ||   0.6171 ||  0.913195 ||  0.917945 ||      2.847821 ||      0.3246 ||   0.6009 ||  0.914135 ||  0.916123 ||    16.636620 || 
    Epoch 06    --      2.684152 ||   0.3562 ||   0.6460 ||  0.922362 ||  0.926376 ||      2.766368 ||      0.3349 ||   0.6205 ||  0.918683 ||  0.920967 ||    17.289774 || 
    Epoch 07    --      2.575902 ||   0.3839 ||   0.6663 ||  0.929283 ||  0.933057 ||      2.622947 ||      0.3662 ||   0.6544 ||  0.929593 ||  0.930877 ||    17.038624 || 
    Epoch 08    --      2.460011 ||   0.4070 ||   0.6915 ||  0.936577 ||  0.939998 ||      2.528333 ||      0.3903 ||   0.6728 ||  0.934064 ||  0.935249 ||    16.843269 || 
    Epoch 09    --      2.346305 ||   0.4307 ||   0.7117 ||  0.942926 ||  0.946048 ||      2.414335 ||      0.4067 ||   0.6941 ||  0.942295 ||  0.942813 ||    16.783565 || 
    Epoch 10    --      2.230076 ||   0.4576 ||   0.7337 ||  0.948831 ||  0.951727 ||      2.304775 ||      0.4383 ||   0.7232 ||  0.947139 ||  0.948444 ||    16.911973 || 
    Epoch 11    --      2.126792 ||   0.4817 ||   0.7527 ||  0.954042 ||  0.956609 ||      2.235590 ||      0.4485 ||   0.7329 ||  0.950721 ||  0.950620 ||    17.061425 || 
    Epoch 12    --      2.025900 ||   0.5036 ||   0.7714 ||  0.958585 ||  0.961057 ||      2.124706 ||      0.4745 ||   0.7530 ||  0.956342 ||  0.956267 ||    16.848122 || 
    Epoch 13    --      1.928354 ||   0.5260 ||   0.7879 ||  0.962608 ||  0.964844 ||      2.074405 ||      0.4817 ||   0.7638 ||  0.959977 ||  0.959322 ||    16.488334 || 
    Epoch 14    --      1.829453 ||   0.5533 ||   0.8041 ||  0.966363 ||  0.968345 ||      1.946352 ||      0.5160 ||   0.7851 ||  0.963470 ||  0.963805 ||    17.372674 || 
    Epoch 15    --      1.742976 ||   0.5717 ||   0.8190 ||  0.969962 ||  0.971742 ||      1.826340 ||      0.5525 ||   0.8056 ||  0.968475 ||  0.969362 ||    17.051889 || 
    Epoch 16    --      1.658277 ||   0.5909 ||   0.8308 ||  0.972886 ||  0.974564 ||      1.757985 ||      0.5605 ||   0.8232 ||  0.970429 ||  0.970715 ||    16.963942 || 
    Epoch 17    --      1.575009 ||   0.6104 ||   0.8440 ||  0.975629 ||  0.977147 ||      1.674148 ||      0.5777 ||   0.8317 ||  0.973448 ||  0.973864 ||    16.957532 || 
    Epoch 18    --      1.502340 ||   0.6273 ||   0.8572 ||  0.978079 ||  0.979509 ||      1.591745 ||      0.6030 ||   0.8446 ||  0.975986 ||  0.976280 ||    16.961633 || 
    Epoch 19    --      1.418432 ||   0.6486 ||   0.8696 ||  0.980648 ||  0.981886 ||      1.569154 ||      0.6038 ||   0.8495 ||  0.976491 ||  0.976637 ||    16.812252 || 
    Epoch 20    --      1.419068 ||   0.6484 ||   0.8689 ||  0.980363 ||  0.981749 ||      1.549873 ||      0.6045 ||   0.8463 ||  0.977507 ||  0.977369 ||    16.894579 || 
    Epoch 21    --      1.353519 ||   0.6646 ||   0.8789 ||  0.981781 ||  0.983007 ||      1.435213 ||      0.6292 ||   0.8688 ||  0.981472 ||  0.981469 ||    17.744398 || 
    Epoch 22    --      1.283099 ||   0.6831 ||   0.8882 ||  0.984137 ||  0.985209 ||      1.366986 ||      0.6546 ||   0.8758 ||  0.982203 ||  0.982670 ||    17.095364 || 
    Epoch 23    --      1.219038 ||   0.7018 ||   0.8974 ||  0.985482 ||  0.986502 ||      1.318705 ||      0.6642 ||   0.8817 ||  0.983987 ||  0.984238 ||    16.688798 || 
    Epoch 24    --      1.161390 ||   0.7145 ||   0.9052 ||  0.987347 ||  0.988279 ||      1.241257 ||      0.6883 ||   0.8992 ||  0.985065 ||  0.985464 ||    17.240249 || 
    Epoch 25    --      1.097079 ||   0.7311 ||   0.9137 ||  0.988516 ||  0.989368 ||      1.192739 ||      0.7029 ||   0.9003 ||  0.987364 ||  0.987719 ||    17.045030 || 
    Epoch 26    --      1.040566 ||   0.7443 ||   0.9211 ||  0.989896 ||  0.990661 ||      1.137969 ||      0.7094 ||   0.9114 ||  0.988687 ||  0.988740 ||    16.957303 || 
    Epoch 27    --      0.982620 ||   0.7610 ||   0.9296 ||  0.990894 ||  0.991631 ||      1.095063 ||      0.7226 ||   0.9160 ||  0.988778 ||  0.989073 ||    16.816369 || 
    Epoch 28    --      0.932960 ||   0.7727 ||   0.9351 ||  0.991741 ||  0.992423 ||      1.053704 ||      0.7314 ||   0.9217 ||  0.990239 ||  0.990376 ||    17.187374 || 
    Epoch 29    --      0.888767 ||   0.7864 ||   0.9405 ||  0.992469 ||  0.993123 ||      0.985316 ||      0.7490 ||   0.9283 ||  0.991618 ||  0.991676 ||    17.019449 || 
    Epoch 30    --      0.839627 ||   0.7986 ||   0.9463 ||  0.993309 ||  0.993887 ||      0.927963 ||      0.7673 ||   0.9356 ||  0.992350 ||  0.992509 ||    16.427366 || 
    Epoch 31    --      0.797620 ||   0.8109 ||   0.9498 ||  0.993877 ||  0.994396 ||      0.856548 ||      0.7895 ||   0.9481 ||  0.993993 ||  0.994202 ||    16.705917 || 
    Epoch 32    --      0.747960 ||   0.8237 ||   0.9552 ||  0.994839 ||  0.995323 ||      0.835535 ||      0.7914 ||   0.9499 ||  0.993719 ||  0.993840 ||    16.710891 || 
    Epoch 33    --      0.706639 ||   0.8345 ||   0.9598 ||  0.995330 ||  0.995775 ||      0.782310 ||      0.8132 ||   0.9519 ||  0.994643 ||  0.994773 ||    16.953009 || 
    Epoch 34    --      0.668466 ||   0.8474 ||   0.9638 ||  0.995782 ||  0.996211 ||      0.755509 ||      0.8115 ||   0.9564 ||  0.995584 ||  0.995522 ||    16.801398 || 
    Epoch 35    --      0.669696 ||   0.8442 ||   0.9637 ||  0.995772 ||  0.996166 ||      0.755575 ||      0.8097 ||   0.9577 ||  0.995336 ||  0.995388 ||    16.878436 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
