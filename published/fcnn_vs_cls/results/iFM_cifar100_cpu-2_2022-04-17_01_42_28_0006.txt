Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.342175 ||   0.1615 ||   0.3698 ||  0.714737 ||  0.717117 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.787375 ||   0.1517 ||   0.3677 ||  0.742786 ||  0.749179 ||      3.620390 ||      0.1754 ||   0.4121 ||  0.837166 ||  0.841584 ||    98.174048 || 
    Epoch 01    --      3.392163 ||   0.2209 ||   0.4756 ||  0.859970 ||  0.867431 ||      3.407836 ||      0.2171 ||   0.4721 ||  0.864176 ||  0.866653 ||    97.062465 || 
    Epoch 02    --      3.205817 ||   0.2535 ||   0.5235 ||  0.879845 ||  0.886231 ||      3.246782 ||      0.2530 ||   0.5142 ||  0.877681 ||  0.881416 ||    96.373988 || 
    Epoch 03    --      3.063143 ||   0.2842 ||   0.5562 ||  0.892424 ||  0.898142 ||      3.105239 ||      0.2768 ||   0.5419 ||  0.892137 ||  0.895016 ||    96.592716 || 
    Epoch 04    --      2.936112 ||   0.3076 ||   0.5852 ||  0.903682 ||  0.908934 ||      2.990827 ||      0.3005 ||   0.5739 ||  0.902834 ||  0.903764 ||    96.514601 || 
    Epoch 05    --      2.816019 ||   0.3334 ||   0.6141 ||  0.912777 ||  0.917512 ||      2.855650 ||      0.3209 ||   0.5997 ||  0.913072 ||  0.915078 ||    96.629939 || 
    Epoch 06    --      2.694518 ||   0.3545 ||   0.6400 ||  0.921712 ||  0.925746 ||      2.782184 ||      0.3367 ||   0.6180 ||  0.918529 ||  0.920628 ||    96.177442 || 
    Epoch 07    --      2.590302 ||   0.3780 ||   0.6625 ||  0.928761 ||  0.932468 ||      2.641511 ||      0.3699 ||   0.6485 ||  0.928078 ||  0.929517 ||    96.408434 || 
    Epoch 08    --      2.478934 ||   0.4027 ||   0.6847 ||  0.935414 ||  0.938914 ||      2.557664 ||      0.3855 ||   0.6729 ||  0.932741 ||  0.933831 ||    96.438166 || 
    Epoch 09    --      2.372217 ||   0.4238 ||   0.7054 ||  0.941551 ||  0.944688 ||      2.435157 ||      0.4021 ||   0.6900 ||  0.941058 ||  0.941452 ||    97.971674 || 
    Epoch 10    --      2.260534 ||   0.4488 ||   0.7278 ||  0.947267 ||  0.950261 ||      2.329915 ||      0.4300 ||   0.7163 ||  0.945170 ||  0.946376 ||    97.910561 || 
    Epoch 11    --      2.161982 ||   0.4727 ||   0.7462 ||  0.952463 ||  0.955037 ||      2.275141 ||      0.4467 ||   0.7267 ||  0.949996 ||  0.949939 ||    97.905230 || 
    Epoch 12    --      2.067368 ||   0.4946 ||   0.7631 ||  0.956743 ||  0.959199 ||      2.156609 ||      0.4716 ||   0.7451 ||  0.954632 ||  0.954463 ||    98.217728 || 
    Epoch 13    --      1.971648 ||   0.5175 ||   0.7806 ||  0.960649 ||  0.962933 ||      2.099682 ||      0.4768 ||   0.7597 ||  0.958640 ||  0.958085 ||    97.764604 || 
    Epoch 14    --      1.875027 ||   0.5390 ||   0.7951 ||  0.964880 ||  0.966860 ||      1.979537 ||      0.5052 ||   0.7784 ||  0.962323 ||  0.962860 ||    97.920861 || 
    Epoch 15    --      1.791082 ||   0.5593 ||   0.8099 ||  0.968003 ||  0.969868 ||      1.864795 ||      0.5337 ||   0.8022 ||  0.967351 ||  0.967967 ||    97.864970 || 
    Epoch 16    --      1.703041 ||   0.5791 ||   0.8250 ||  0.971360 ||  0.973075 ||      1.807924 ||      0.5438 ||   0.8116 ||  0.969438 ||  0.969647 ||    97.936513 || 
    Epoch 17    --      1.626809 ||   0.5975 ||   0.8372 ||  0.974116 ||  0.975748 ||      1.701252 ||      0.5758 ||   0.8217 ||  0.972033 ||  0.972438 ||    97.686542 || 
    Epoch 18    --      1.550317 ||   0.6131 ||   0.8516 ||  0.976730 ||  0.978204 ||      1.642286 ||      0.5891 ||   0.8364 ||  0.974959 ||  0.975003 ||    98.202134 || 
    Epoch 19    --      1.466229 ||   0.6360 ||   0.8620 ||  0.979009 ||  0.980382 ||      1.606559 ||      0.5918 ||   0.8426 ||  0.976664 ||  0.976431 ||    97.905250 || 
    Epoch 20    --      1.393703 ||   0.6531 ||   0.8738 ||  0.981328 ||  0.982654 ||      1.515196 ||      0.6148 ||   0.8562 ||  0.979083 ||  0.979120 ||    97.889641 || 
    Epoch 21    --      1.331938 ||   0.6735 ||   0.8814 ||  0.982669 ||  0.983815 ||      1.428581 ||      0.6292 ||   0.8714 ||  0.982434 ||  0.982282 ||    97.686491 || 
    Epoch 22    --      1.264829 ||   0.6873 ||   0.8917 ||  0.984502 ||  0.985549 ||      1.347912 ||      0.6551 ||   0.8790 ||  0.983157 ||  0.983534 ||    97.672422 || 
    Epoch 23    --      1.198878 ||   0.7030 ||   0.9015 ||  0.986039 ||  0.987005 ||      1.309487 ||      0.6631 ||   0.8845 ||  0.984431 ||  0.984565 ||    97.780292 || 
    Epoch 24    --      1.141393 ||   0.7181 ||   0.9075 ||  0.987711 ||  0.988614 ||      1.237586 ||      0.6843 ||   0.8987 ||  0.985668 ||  0.985864 ||    98.186556 || 
    Epoch 25    --      1.082211 ||   0.7336 ||   0.9164 ||  0.988730 ||  0.989546 ||      1.162191 ||      0.7050 ||   0.9115 ||  0.987923 ||  0.988247 ||    97.858441 || 
    Epoch 26    --      1.023525 ||   0.7490 ||   0.9236 ||  0.990329 ||  0.991079 ||      1.139978 ||      0.7086 ||   0.9128 ||  0.988981 ||  0.988833 ||    97.796756 || 
    Epoch 27    --      0.967322 ||   0.7640 ||   0.9303 ||  0.991268 ||  0.991947 ||      1.093380 ||      0.7210 ||   0.9150 ||  0.989131 ||  0.989371 ||    97.655291 || 
    Epoch 28    --      0.921792 ||   0.7770 ||   0.9364 ||  0.991905 ||  0.992550 ||      1.028450 ||      0.7318 ||   0.9264 ||  0.991276 ||  0.991435 ||    98.453621 || 
    Epoch 29    --      0.876575 ||   0.7877 ||   0.9413 ||  0.992881 ||  0.993478 ||      0.970360 ||      0.7578 ||   0.9335 ||  0.991530 ||  0.991510 ||    98.295913 || 
    Epoch 30    --      0.828902 ||   0.7999 ||   0.9476 ||  0.993536 ||  0.994088 ||      0.906672 ||      0.7742 ||   0.9394 ||  0.993088 ||  0.993264 ||   100.374025 || 
    Epoch 31    --      0.782958 ||   0.8166 ||   0.9511 ||  0.994376 ||  0.994839 ||      0.847283 ||      0.7922 ||   0.9459 ||  0.993985 ||  0.994201 ||   101.030275 || 
    Epoch 32    --      0.741336 ||   0.8237 ||   0.9564 ||  0.994938 ||  0.995395 ||      0.811563 ||      0.7969 ||   0.9513 ||  0.994464 ||  0.994601 ||   100.863563 || 
    Epoch 33    --      0.696609 ||   0.8368 ||   0.9609 ||  0.995606 ||  0.996027 ||      0.797475 ||      0.8038 ||   0.9514 ||  0.994490 ||  0.994468 ||   101.457555 || 
    Epoch 34    --      0.660604 ||   0.8449 ||   0.9638 ||  0.995997 ||  0.996399 ||      0.744992 ||      0.8130 ||   0.9568 ||  0.995659 ||  0.995574 ||    99.936534 || 
    Epoch 35    --      0.628398 ||   0.8569 ||   0.9664 ||  0.996344 ||  0.996689 ||      0.699971 ||      0.8271 ||   0.9660 ||  0.996156 ||  0.996099 ||   101.079444 || 
    Epoch 36    --      0.585871 ||   0.8680 ||   0.9718 ||  0.996881 ||  0.997192 ||      0.672190 ||      0.8371 ||   0.9672 ||  0.996625 ||  0.996600 ||   100.106656 || 
    Epoch 37    --      0.551575 ||   0.8766 ||   0.9746 ||  0.997414 ||  0.997690 ||      0.644758 ||      0.8461 ||   0.9669 ||  0.996441 ||  0.996440 ||   101.090927 || 
    Epoch 38    --      0.518956 ||   0.8853 ||   0.9770 ||  0.997710 ||  0.997960 ||      0.599150 ||      0.8605 ||   0.9730 ||  0.996964 ||  0.997050 ||   101.106565 || 
    Epoch 39    --      0.495797 ||   0.8909 ||   0.9791 ||  0.997763 ||  0.997988 ||      0.558998 ||      0.8713 ||   0.9775 ||  0.997860 ||  0.997857 ||   101.054682 || 
    Epoch 40    --      0.463438 ||   0.9024 ||   0.9814 ||  0.998138 ||  0.998349 ||      0.543852 ||      0.8728 ||   0.9788 ||  0.997769 ||  0.997784 ||   100.903540 || 
    Epoch 41    --      0.434439 ||   0.9100 ||   0.9842 ||  0.998504 ||  0.998675 ||      0.525753 ||      0.8708 ||   0.9802 ||  0.997855 ||  0.997779 ||   101.200383 || 
    Epoch 42    --      0.438811 ||   0.9069 ||   0.9848 ||  0.998366 ||  0.998550 ||      0.518639 ||      0.8803 ||   0.9769 ||  0.997999 ||  0.997968 ||   102.325418 || 
    Epoch 43    --      0.410728 ||   0.9166 ||   0.9856 ||  0.998597 ||  0.998757 ||      0.482696 ||      0.8891 ||   0.9790 ||  0.998100 ||  0.998121 ||   101.123590 || 
    Epoch 44    --      0.387531 ||   0.9224 ||   0.9866 ||  0.998719 ||  0.998869 ||      0.439416 ||      0.9022 ||   0.9867 ||  0.998780 ||  0.998752 ||   101.190334 || 
    Epoch 45    --      0.367044 ||   0.9260 ||   0.9887 ||  0.998875 ||  0.998995 ||      0.405296 ||      0.9165 ||   0.9885 ||  0.998827 ||  0.998869 ||   100.966171 || 
    Epoch 46    --      0.343005 ||   0.9337 ||   0.9899 ||  0.999113 ||  0.999227 ||      0.391038 ||      0.9162 ||   0.9894 ||  0.999079 ||  0.999077 ||   100.841192 || 
    Epoch 47    --      0.342542 ||   0.9333 ||   0.9902 ||  0.999089 ||  0.999201 ||      0.402614 ||      0.9083 ||   0.9877 ||  0.999064 ||  0.999014 ||   101.247411 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
