Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.306867 ||   0.1565 ||   0.3619 ||  0.713449 ||  0.715744 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.779405 ||   0.1534 ||   0.3709 ||  0.742867 ||  0.749390 ||      3.621614 ||      0.1758 ||   0.4081 ||  0.836330 ||  0.840702 ||    98.623690 || 
    Epoch 01    --      3.395962 ||   0.2206 ||   0.4743 ||  0.859051 ||  0.866634 ||      3.406787 ||      0.2167 ||   0.4729 ||  0.863358 ||  0.865920 ||    96.717498 || 
    Epoch 02    --      3.213685 ||   0.2528 ||   0.5203 ||  0.878902 ||  0.885424 ||      3.250436 ||      0.2527 ||   0.5136 ||  0.876787 ||  0.880829 ||    97.826881 || 
    Epoch 03    --      3.073679 ||   0.2824 ||   0.5539 ||  0.891161 ||  0.896978 ||      3.118258 ||      0.2715 ||   0.5387 ||  0.890724 ||  0.893722 ||    97.764363 || 
    Epoch 04    --      2.950187 ||   0.3059 ||   0.5816 ||  0.902524 ||  0.907823 ||      3.007629 ||      0.2990 ||   0.5680 ||  0.900197 ||  0.901459 ||    97.592529 || 
    Epoch 05    --      2.833263 ||   0.3284 ||   0.6081 ||  0.911232 ||  0.916077 ||      2.869750 ||      0.3190 ||   0.5966 ||  0.912282 ||  0.914233 ||    97.530020 || 
    Epoch 06    --      2.715662 ||   0.3523 ||   0.6372 ||  0.920204 ||  0.924406 ||      2.790049 ||      0.3358 ||   0.6160 ||  0.917599 ||  0.919738 ||    99.201928 || 
    Epoch 07    --      2.610941 ||   0.3752 ||   0.6597 ||  0.927362 ||  0.931195 ||      2.662279 ||      0.3620 ||   0.6418 ||  0.926360 ||  0.927807 ||    99.030055 || 
    Epoch 08    --      2.502054 ||   0.3971 ||   0.6789 ||  0.934009 ||  0.937563 ||      2.573394 ||      0.3779 ||   0.6604 ||  0.932377 ||  0.933294 ||    99.961824 || 
    Epoch 09    --      2.393992 ||   0.4212 ||   0.6997 ||  0.940389 ||  0.943585 ||      2.458249 ||      0.3962 ||   0.6860 ||  0.940432 ||  0.940743 ||   100.405070 || 
    Epoch 10    --      2.281598 ||   0.4453 ||   0.7218 ||  0.946532 ||  0.949502 ||      2.347994 ||      0.4278 ||   0.7158 ||  0.944832 ||  0.945931 ||    99.920696 || 
    Epoch 11    --      2.179962 ||   0.4673 ||   0.7434 ||  0.951905 ||  0.954540 ||      2.282283 ||      0.4427 ||   0.7210 ||  0.948943 ||  0.948761 ||    99.767017 || 
    Epoch 12    --      2.083007 ||   0.4892 ||   0.7603 ||  0.956275 ||  0.958701 ||      2.170904 ||      0.4672 ||   0.7454 ||  0.954559 ||  0.954464 ||   100.485460 || 
    Epoch 13    --      1.987254 ||   0.5124 ||   0.7779 ||  0.960462 ||  0.962745 ||      2.106398 ||      0.4733 ||   0.7616 ||  0.958381 ||  0.958110 ||    97.514543 || 
    Epoch 14    --      1.888856 ||   0.5348 ||   0.7940 ||  0.964551 ||  0.966549 ||      1.985767 ||      0.5013 ||   0.7807 ||  0.962609 ||  0.962943 ||    97.561388 || 
    Epoch 15    --      1.802829 ||   0.5560 ||   0.8080 ||  0.967948 ||  0.969805 ||      1.876173 ||      0.5315 ||   0.8008 ||  0.967290 ||  0.967843 ||    97.795797 || 
    Epoch 16    --      1.712807 ||   0.5768 ||   0.8253 ||  0.971355 ||  0.973062 ||      1.802751 ||      0.5472 ||   0.8147 ||  0.969209 ||  0.969673 ||    97.342697 || 
    Epoch 17    --      1.635611 ||   0.5951 ||   0.8361 ||  0.973955 ||  0.975602 ||      1.715939 ||      0.5634 ||   0.8227 ||  0.972762 ||  0.972908 ||    98.858313 || 
    Epoch 18    --      1.556349 ||   0.6118 ||   0.8522 ||  0.976904 ||  0.978373 ||      1.642579 ||      0.5804 ||   0.8351 ||  0.975073 ||  0.975082 ||    99.280182 || 
    Epoch 19    --      1.470647 ||   0.6319 ||   0.8629 ||  0.979244 ||  0.980610 ||      1.590014 ||      0.5986 ||   0.8465 ||  0.976168 ||  0.976465 ||    99.702053 || 
    Epoch 20    --      1.397349 ||   0.6533 ||   0.8738 ||  0.981170 ||  0.982504 ||      1.515536 ||      0.6155 ||   0.8571 ||  0.978779 ||  0.978679 ||    99.390797 || 
    Epoch 21    --      1.335238 ||   0.6700 ||   0.8818 ||  0.982521 ||  0.983731 ||      1.413577 ||      0.6352 ||   0.8730 ||  0.982060 ||  0.981966 ||   101.155171 || 
    Epoch 22    --      1.261598 ||   0.6885 ||   0.8918 ||  0.984707 ||  0.985719 ||      1.371926 ||      0.6525 ||   0.8766 ||  0.982387 ||  0.982475 ||    99.436498 || 
    Epoch 23    --      1.199435 ||   0.7063 ||   0.9000 ||  0.986048 ||  0.987017 ||      1.299397 ||      0.6713 ||   0.8879 ||  0.984270 ||  0.984580 ||   100.827090 || 
    Epoch 24    --      1.141755 ||   0.7195 ||   0.9081 ||  0.987763 ||  0.988662 ||      1.221886 ||      0.6925 ||   0.9001 ||  0.985176 ||  0.985672 ||   100.811491 || 
    Epoch 25    --      1.080580 ||   0.7365 ||   0.9187 ||  0.988513 ||  0.989349 ||      1.165489 ||      0.7043 ||   0.9086 ||  0.987650 ||  0.988062 ||   100.858386 || 
    Epoch 26    --      1.022919 ||   0.7488 ||   0.9253 ||  0.990057 ||  0.990800 ||      1.152097 ||      0.7023 ||   0.9070 ||  0.988484 ||  0.988268 ||   100.994621 || 
    Epoch 27    --      1.018911 ||   0.7526 ||   0.9245 ||  0.989975 ||  0.990750 ||      1.145767 ||      0.7098 ||   0.9099 ||  0.987990 ||  0.988129 ||   101.264583 || 
    Epoch 28    --      0.973204 ||   0.7631 ||   0.9294 ||  0.990998 ||  0.991747 ||      1.071650 ||      0.7257 ||   0.9232 ||  0.990323 ||  0.990363 ||   100.812671 || 
    Epoch 29    --      0.924317 ||   0.7742 ||   0.9367 ||  0.991865 ||  0.992539 ||      1.015561 ||      0.7436 ||   0.9285 ||  0.990812 ||  0.990805 ||   101.128685 || 
    Epoch 30    --      0.875330 ||   0.7879 ||   0.9417 ||  0.992774 ||  0.993379 ||      0.951315 ||      0.7605 ||   0.9360 ||  0.992207 ||  0.992390 ||   100.864510 || 
    Epoch 31    --      0.828250 ||   0.8018 ||   0.9470 ||  0.993571 ||  0.994093 ||      0.904218 ||      0.7776 ||   0.9419 ||  0.992951 ||  0.993116 ||   100.889675 || 
    Epoch 32    --      0.782335 ||   0.8125 ||   0.9524 ||  0.994456 ||  0.994961 ||      0.854291 ||      0.7909 ||   0.9432 ||  0.993541 ||  0.993642 ||   100.794167 || 
    Epoch 33    --      0.735596 ||   0.8260 ||   0.9575 ||  0.995109 ||  0.995567 ||      0.818858 ||      0.7958 ||   0.9510 ||  0.993894 ||  0.993950 ||   101.044036 || 
    Epoch 34    --      0.696575 ||   0.8378 ||   0.9598 ||  0.995534 ||  0.995948 ||      0.797049 ||      0.7965 ||   0.9542 ||  0.994895 ||  0.994814 ||   100.566292 || 
    Epoch 35    --      0.699479 ||   0.8357 ||   0.9598 ||  0.995415 ||  0.995841 ||      0.786764 ||      0.8056 ||   0.9565 ||  0.995281 ||  0.995181 ||   100.919197 || 
    Epoch 36    --      0.659332 ||   0.8464 ||   0.9647 ||  0.996062 ||  0.996459 ||      0.735312 ||      0.8181 ||   0.9603 ||  0.995675 ||  0.995604 ||   100.934854 || 
    Epoch 37    --      0.618377 ||   0.8587 ||   0.9689 ||  0.996690 ||  0.997039 ||      0.735574 ||      0.8204 ||   0.9556 ||  0.995113 ||  0.994944 ||   100.731731 || 
    Epoch 38    --      0.582580 ||   0.8674 ||   0.9710 ||  0.996996 ||  0.997305 ||      0.690306 ||      0.8275 ||   0.9619 ||  0.996086 ||  0.996037 ||   100.856847 || 
    Epoch 39    --      0.558330 ||   0.8749 ||   0.9737 ||  0.997232 ||  0.997518 ||      0.618548 ||      0.8498 ||   0.9727 ||  0.997168 ||  0.997185 ||   100.934970 || 
    Epoch 40    --      0.516203 ||   0.8850 ||   0.9777 ||  0.997742 ||  0.997992 ||      0.614501 ||      0.8546 ||   0.9705 ||  0.996935 ||  0.996846 ||   101.919351 || 
    Epoch 41    --      0.490182 ||   0.8918 ||   0.9799 ||  0.997945 ||  0.998172 ||      0.574787 ||      0.8617 ||   0.9750 ||  0.997468 ||  0.997333 ||   100.763257 || 
    Epoch 42    --      0.464201 ||   0.8993 ||   0.9821 ||  0.998188 ||  0.998402 ||      0.538251 ||      0.8750 ||   0.9774 ||  0.997884 ||  0.997875 ||   100.497638 || 
    Epoch 43    --      0.433377 ||   0.9091 ||   0.9832 ||  0.998467 ||  0.998655 ||      0.503504 ||      0.8844 ||   0.9814 ||  0.997756 ||  0.997779 ||   101.008471 || 
    Epoch 44    --      0.411816 ||   0.9148 ||   0.9856 ||  0.998550 ||  0.998718 ||      0.460363 ||      0.8967 ||   0.9831 ||  0.998703 ||  0.998689 ||   100.763386 || 
    Epoch 45    --      0.389808 ||   0.9201 ||   0.9874 ||  0.998701 ||  0.998844 ||      0.436328 ||      0.9031 ||   0.9868 ||  0.998885 ||  0.998847 ||   101.060251 || 
    Epoch 46    --      0.363190 ||   0.9285 ||   0.9888 ||  0.998983 ||  0.999112 ||      0.415292 ||      0.9110 ||   0.9857 ||  0.998871 ||  0.998921 ||   100.919644 || 
    Epoch 47    --      0.341099 ||   0.9327 ||   0.9899 ||  0.999110 ||  0.999225 ||      0.402500 ||      0.9117 ||   0.9860 ||  0.998889 ||  0.998873 ||   100.950966 || 
    Epoch 48    --      0.342570 ||   0.9317 ||   0.9902 ||  0.999155 ||  0.999271 ||      0.404565 ||      0.9097 ||   0.9880 ||  0.998907 ||  0.998896 ||   110.575900 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
