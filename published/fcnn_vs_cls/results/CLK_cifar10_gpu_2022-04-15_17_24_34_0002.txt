Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                       10
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -           ['conv2d', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.3
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      2.281862 ||   0.2747 ||   0.8390 ||  0.756717 ||  0.758821 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      2.104012 ||   0.2915 ||   0.7989 ||  0.733822 ||  0.739953 ||      2.046902 ||      0.2359 ||   0.7813 ||  0.727739 ||  0.726160 ||    17.203635 || 
    Epoch 01    --      2.122375 ||   0.2219 ||   0.7209 ||  0.674834 ||  0.687754 ||      2.102729 ||      0.2780 ||   0.7855 ||  0.744185 ||  0.729175 ||    15.832617 || 
    Epoch 02    --      2.010590 ||   0.2590 ||   0.8027 ||  0.733554 ||  0.742697 ||      2.063847 ||      0.2737 ||   0.8052 ||  0.751549 ||  0.741491 ||    15.408793 || 
    Epoch 03    --      1.979578 ||   0.2678 ||   0.8115 ||  0.743709 ||  0.752046 ||      1.930559 ||      0.2839 ||   0.8347 ||  0.763503 ||  0.769664 ||    15.341716 || 
    Epoch 04    --      1.972609 ||   0.2652 ||   0.8165 ||  0.745582 ||  0.753381 ||      1.926493 ||      0.2823 ||   0.8295 ||  0.759468 ||  0.766376 ||    15.314359 || 
    Epoch 05    --      1.968081 ||   0.2711 ||   0.8160 ||  0.747973 ||  0.754744 ||      1.948255 ||      0.2798 ||   0.8366 ||  0.759463 ||  0.765611 ||    14.925135 || 
    Epoch 06    --      1.959789 ||   0.2674 ||   0.8202 ||  0.750543 ||  0.757278 ||      1.922529 ||      0.2686 ||   0.8279 ||  0.757754 ||  0.766338 ||    15.471702 || 
    Epoch 07    --      1.961749 ||   0.2684 ||   0.8180 ||  0.749783 ||  0.756061 ||      2.058870 ||      0.2141 ||   0.7508 ||  0.733746 ||  0.710322 ||    15.734060 || 
    Epoch 08    --      1.959918 ||   0.2672 ||   0.8205 ||  0.750763 ||  0.757075 ||      1.917012 ||      0.2696 ||   0.8404 ||  0.768801 ||  0.770952 ||    15.554154 || 
    Epoch 09    --      1.959212 ||   0.2662 ||   0.8230 ||  0.751350 ||  0.757696 ||      1.911059 ||      0.2869 ||   0.8432 ||  0.771172 ||  0.774638 ||    15.755294 || 
    Epoch 10    --      1.944115 ||   0.2706 ||   0.8263 ||  0.754730 ||  0.761077 ||      1.978168 ||      0.2906 ||   0.8286 ||  0.763204 ||  0.762449 ||    15.402706 || 
    Epoch 11    --      1.957114 ||   0.2666 ||   0.8184 ||  0.750686 ||  0.757329 ||      1.944978 ||      0.2512 ||   0.8261 ||  0.755063 ||  0.756073 ||    16.152176 || 
    Epoch 12    --      1.944044 ||   0.2690 ||   0.8256 ||  0.754346 ||  0.761543 ||      1.945531 ||      0.2610 ||   0.8084 ||  0.756427 ||  0.754159 ||    15.361368 || 
    Epoch 13    --      1.945923 ||   0.2704 ||   0.8263 ||  0.755166 ||  0.761907 ||      1.902193 ||      0.2771 ||   0.8434 ||  0.769248 ||  0.775607 ||    15.268524 || 
    Epoch 14    --      1.934800 ||   0.2738 ||   0.8292 ||  0.757310 ||  0.764117 ||      1.883337 ||      0.2834 ||   0.8510 ||  0.772134 ||  0.779262 ||    15.369469 || 
    Epoch 15    --      1.934432 ||   0.2709 ||   0.8252 ||  0.757663 ||  0.763875 ||      1.912809 ||      0.2759 ||   0.8491 ||  0.767177 ||  0.772831 ||    15.424931 || 
    Epoch 16    --      1.931875 ||   0.2711 ||   0.8320 ||  0.758671 ||  0.765338 ||      1.890334 ||      0.2908 ||   0.8493 ||  0.770748 ||  0.776545 ||    15.559360 || 
    Epoch 17    --      1.926651 ||   0.2764 ||   0.8328 ||  0.760181 ||  0.766802 ||      1.928271 ||      0.2597 ||   0.8293 ||  0.761096 ||  0.763535 ||    15.307892 || 
    Epoch 18    --      1.924718 ||   0.2731 ||   0.8324 ||  0.761090 ||  0.767660 ||      1.966589 ||      0.2462 ||   0.8080 ||  0.754737 ||  0.747196 ||    15.619073 || 
    Epoch 19    --      1.924530 ||   0.2729 ||   0.8323 ||  0.761053 ||  0.767509 ||      1.940822 ||      0.2529 ||   0.8265 ||  0.758513 ||  0.754908 ||    15.794024 || 
    Epoch 20    --      1.918841 ||   0.2767 ||   0.8331 ||  0.762672 ||  0.769077 ||      1.878869 ||      0.2860 ||   0.8501 ||  0.774821 ||  0.781198 ||    15.551367 || 
    Epoch 21    --      1.927165 ||   0.2753 ||   0.8320 ||  0.761174 ||  0.767130 ||      2.016196 ||      0.2279 ||   0.7749 ||  0.745319 ||  0.727916 ||    15.538602 || 
    Epoch 22    --      1.913372 ||   0.2754 ||   0.8338 ||  0.764006 ||  0.770467 ||      1.895085 ||      0.2753 ||   0.8422 ||  0.766404 ||  0.772890 ||    15.314001 || 
    Epoch 23    --      1.916893 ||   0.2765 ||   0.8350 ||  0.764207 ||  0.769892 ||      1.883261 ||      0.2753 ||   0.8470 ||  0.770087 ||  0.777394 ||    15.330512 || 
    Epoch 24    --      1.911389 ||   0.2783 ||   0.8370 ||  0.765258 ||  0.771674 ||      1.930299 ||      0.2550 ||   0.8247 ||  0.761796 ||  0.759147 ||    15.427562 || 
    Epoch 25    --      1.913997 ||   0.2757 ||   0.8363 ||  0.764901 ||  0.770686 ||      1.950497 ||      0.2948 ||   0.8378 ||  0.774717 ||  0.773419 ||    15.320233 || 
    Epoch 26    --      1.909282 ||   0.2811 ||   0.8396 ||  0.765855 ||  0.772192 ||      1.961261 ||      0.2412 ||   0.8024 ||  0.755051 ||  0.746847 ||    15.463614 || 
    Epoch 27    --      1.909915 ||   0.2779 ||   0.8372 ||  0.765617 ||  0.771494 ||      1.870575 ||      0.2818 ||   0.8505 ||  0.773590 ||  0.780164 ||    15.640544 || 
    Epoch 28    --      1.907410 ||   0.2794 ||   0.8386 ||  0.766340 ||  0.772357 ||      1.864484 ||      0.2870 ||   0.8547 ||  0.777880 ||  0.784076 ||    15.637655 || 
    Epoch 29    --      1.911425 ||   0.2765 ||   0.8362 ||  0.764829 ||  0.771119 ||      1.859362 ||      0.2928 ||   0.8591 ||  0.780904 ||  0.786792 ||    15.897956 || 
    Epoch 30    --      1.905854 ||   0.2785 ||   0.8389 ||  0.766365 ||  0.772672 ||      1.862876 ||      0.2840 ||   0.8515 ||  0.776286 ||  0.781921 ||    16.016452 || 
    Epoch 31    --      1.902605 ||   0.2794 ||   0.8383 ||  0.767058 ||  0.773126 ||      1.929776 ||      0.2570 ||   0.8185 ||  0.760301 ||  0.757670 ||    15.618508 || 
    Epoch 32    --      1.903460 ||   0.2805 ||   0.8392 ||  0.767070 ||  0.772851 ||      1.872527 ||      0.2822 ||   0.8453 ||  0.774375 ||  0.779477 ||    15.237981 || 
    Epoch 33    --      1.902139 ||   0.2806 ||   0.8382 ||  0.767310 ||  0.773869 ||      1.914151 ||      0.2852 ||   0.8463 ||  0.773117 ||  0.778395 ||    15.511780 || 
    Epoch 34    --      1.896167 ||   0.2814 ||   0.8394 ||  0.768956 ||  0.775045 ||      1.901006 ||      0.2908 ||   0.8508 ||  0.776381 ||  0.780253 ||    15.525085 || 
    Epoch 35    --      1.895785 ||   0.2845 ||   0.8414 ||  0.769405 ||  0.775584 ||      1.936585 ||      0.2479 ||   0.8110 ||  0.761208 ||  0.754116 ||    15.491176 || 
    Epoch 36    --      1.896064 ||   0.2801 ||   0.8383 ||  0.768442 ||  0.774990 ||      1.865009 ||      0.2810 ||   0.8572 ||  0.781036 ||  0.785074 ||    15.796363 || 
    Epoch 37    --      1.894406 ||   0.2826 ||   0.8395 ||  0.769837 ||  0.775713 ||      1.919040 ||      0.2588 ||   0.8212 ||  0.766342 ||  0.762188 ||    15.751567 || 
    Epoch 38    --      1.891196 ||   0.2817 ||   0.8426 ||  0.770375 ||  0.776663 ||      1.869157 ||      0.2949 ||   0.8554 ||  0.781105 ||  0.786213 ||    15.891742 || 
    Epoch 39    --      1.891658 ||   0.2814 ||   0.8414 ||  0.768936 ||  0.775658 ||      1.846444 ||      0.2965 ||   0.8614 ||  0.783887 ||  0.788273 ||    15.023468 || 
    Epoch 40    --      1.883142 ||   0.2883 ||   0.8453 ||  0.773250 ||  0.779438 ||      1.864344 ||      0.2857 ||   0.8488 ||  0.777683 ||  0.784081 ||    15.141884 || 
    Epoch 41    --      1.885219 ||   0.2856 ||   0.8421 ||  0.771969 ||  0.778573 ||      1.852225 ||      0.2879 ||   0.8502 ||  0.778919 ||  0.783927 ||    15.578703 || 
    Epoch 42    --      1.887350 ||   0.2862 ||   0.8420 ||  0.772309 ||  0.778313 ||      1.845194 ||      0.3006 ||   0.8583 ||  0.784372 ||  0.790875 ||    15.468425 || 
    Epoch 43    --      1.878765 ||   0.2897 ||   0.8451 ||  0.774575 ||  0.780535 ||      1.940350 ||      0.3071 ||   0.8507 ||  0.780825 ||  0.784011 ||    15.389957 || 
    Epoch 44    --      1.881368 ||   0.2893 ||   0.8442 ||  0.772859 ||  0.779745 ||      1.862262 ||      0.2994 ||   0.8585 ||  0.786206 ||  0.788073 ||    15.575218 || 
    Epoch 45    --      1.874752 ||   0.2904 ||   0.8462 ||  0.774492 ||  0.780746 ||      1.859438 ||      0.2906 ||   0.8559 ||  0.783978 ||  0.786281 ||    15.569222 || 
Layers list:
	conv2d                                   -                    (17, 17, 3, 20)|(20,)
	conv2d-filters                           -                                       20
	conv2d-kernel_size                       -                                       17
	conv2d-strides                           -                                   (1, 1)
	conv2d-padding                           -                                    valid
	conv2d-data_format                       -                                     None
	conv2d-dilation_rate                     -                                   (1, 1)
	conv2d-groups                            -                                        1
	conv2d-activation                        -                                     relu
	conv2d-use_bias                          -                                     True
	conv2d-kernel_initializer                -                                he_normal
	conv2d-bias_initializer                  -                                    zeros
	conv2d-kernel_regularizer                -                                     None
	conv2d-bias_regularizer                  -                                     None
	conv2d-activity_regularizer              -                                     None
	conv2d-kernel_constraint                 -                                     None
	conv2d-bias_constraint                   -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                         (5120, 10)|(10,)
	dense_2-units                            -                                       10
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 16, 16, 20)        17360     
_________________________________________________________________
flatten_2 (Flatten)          (None, 5120)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                51210     
=================================================================
Total params: 68,570
Trainable params: 68,570
Non-trainable params: 0
_________________________________________________________________
