Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.114623 ||   0.1586 ||   0.3820 ||  0.743710 ||  0.747221 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.788113 ||   0.1514 ||   0.3694 ||  0.741692 ||  0.747944 ||      3.619745 ||      0.1760 ||   0.4112 ||  0.837461 ||  0.841899 ||   102.514782 || 
    Epoch 01    --      3.391289 ||   0.2217 ||   0.4740 ||  0.860289 ||  0.867642 ||      3.399286 ||      0.2139 ||   0.4730 ||  0.865010 ||  0.867860 ||   100.780426 || 
    Epoch 02    --      3.209324 ||   0.2532 ||   0.5203 ||  0.879831 ||  0.886250 ||      3.243234 ||      0.2534 ||   0.5146 ||  0.878183 ||  0.881881 ||   100.717930 || 
    Epoch 03    --      3.068985 ||   0.2822 ||   0.5568 ||  0.892166 ||  0.897866 ||      3.112677 ||      0.2698 ||   0.5383 ||  0.891603 ||  0.894794 ||   100.889776 || 
    Epoch 04    --      2.945482 ||   0.3045 ||   0.5824 ||  0.903118 ||  0.908518 ||      2.995449 ||      0.2976 ||   0.5716 ||  0.902346 ||  0.903540 ||   101.014808 || 
    Epoch 05    --      2.830589 ||   0.3286 ||   0.6074 ||  0.911925 ||  0.916725 ||      2.862554 ||      0.3181 ||   0.5986 ||  0.912541 ||  0.914798 ||   101.905434 || 
    Epoch 06    --      2.708751 ||   0.3528 ||   0.6386 ||  0.920730 ||  0.924993 ||      2.798648 ||      0.3329 ||   0.6134 ||  0.917115 ||  0.919153 ||   100.483570 || 
    Epoch 07    --      2.608603 ||   0.3743 ||   0.6601 ||  0.927562 ||  0.931482 ||      2.659712 ||      0.3600 ||   0.6462 ||  0.927426 ||  0.928536 ||   100.952291 || 
    Epoch 08    --      2.500130 ||   0.3977 ||   0.6816 ||  0.934173 ||  0.937832 ||      2.567831 ||      0.3838 ||   0.6657 ||  0.932378 ||  0.933684 ||   100.921038 || 
    Epoch 09    --      2.391818 ||   0.4226 ||   0.7034 ||  0.940292 ||  0.943618 ||      2.462174 ||      0.3962 ||   0.6860 ||  0.940125 ||  0.940591 ||   102.296040 || 
    Epoch 10    --      2.283208 ||   0.4438 ||   0.7221 ||  0.946336 ||  0.949354 ||      2.344931 ||      0.4285 ||   0.7149 ||  0.944808 ||  0.945942 ||   101.874194 || 
    Epoch 11    --      2.181808 ||   0.4672 ||   0.7434 ||  0.951593 ||  0.954345 ||      2.294751 ||      0.4386 ||   0.7216 ||  0.948313 ||  0.948291 ||   102.202282 || 
    Epoch 12    --      2.087862 ||   0.4910 ||   0.7588 ||  0.955799 ||  0.958429 ||      2.182775 ||      0.4599 ||   0.7393 ||  0.954225 ||  0.954134 ||   103.655412 || 
    Epoch 13    --      1.995137 ||   0.5085 ||   0.7768 ||  0.959700 ||  0.962096 ||      2.139041 ||      0.4666 ||   0.7545 ||  0.957570 ||  0.956821 ||   102.686699 || 
    Epoch 14    --      1.898375 ||   0.5346 ||   0.7923 ||  0.964108 ||  0.966291 ||      2.009426 ||      0.5001 ||   0.7764 ||  0.961622 ||  0.961780 ||   104.061694 || 
    Epoch 15    --      1.814560 ||   0.5534 ||   0.8086 ||  0.967155 ||  0.969081 ||      1.896696 ||      0.5292 ||   0.7923 ||  0.966648 ||  0.967330 ||   104.514817 || 
    Epoch 16    --      1.728773 ||   0.5725 ||   0.8221 ||  0.970702 ||  0.972514 ||      1.835219 ||      0.5431 ||   0.8031 ||  0.968178 ||  0.968414 ||   104.089912 || 
    Epoch 17    --      1.651410 ||   0.5929 ||   0.8361 ||  0.973048 ||  0.974776 ||      1.735873 ||      0.5621 ||   0.8215 ||  0.971917 ||  0.972194 ||   103.670092 || 
    Epoch 18    --      1.578037 ||   0.6083 ||   0.8463 ||  0.975907 ||  0.977473 ||      1.657537 ||      0.5870 ||   0.8324 ||  0.973946 ||  0.974060 ||   104.858576 || 
    Epoch 19    --      1.492218 ||   0.6321 ||   0.8607 ||  0.978172 ||  0.979576 ||      1.628527 ||      0.5875 ||   0.8424 ||  0.975892 ||  0.975738 ||   104.077333 || 
    Epoch 20    --      1.492581 ||   0.6298 ||   0.8587 ||  0.978321 ||  0.979820 ||      1.621805 ||      0.5948 ||   0.8433 ||  0.975067 ||  0.975162 ||   104.296056 || 
    Epoch 21    --      1.430764 ||   0.6474 ||   0.8666 ||  0.979958 ||  0.981300 ||      1.505920 ||      0.6082 ||   0.8642 ||  0.979471 ||  0.979441 ||   104.037101 || 
    Epoch 22    --      1.357539 ||   0.6632 ||   0.8793 ||  0.981975 ||  0.983179 ||      1.452456 ||      0.6346 ||   0.8690 ||  0.980151 ||  0.980572 ||   104.049994 || 
    Epoch 23    --      1.290178 ||   0.6817 ||   0.8903 ||  0.983760 ||  0.984910 ||      1.390571 ||      0.6478 ||   0.8729 ||  0.981336 ||  0.981643 ||   104.066083 || 
    Epoch 24    --      1.234763 ||   0.6940 ||   0.8972 ||  0.985141 ||  0.986191 ||      1.313914 ||      0.6681 ||   0.8881 ||  0.983610 ||  0.983951 ||   103.921068 || 
    Epoch 25    --      1.169912 ||   0.7129 ||   0.9058 ||  0.986548 ||  0.987516 ||      1.250528 ||      0.6818 ||   0.8944 ||  0.985814 ||  0.986150 ||   103.811733 || 
    Epoch 26    --      1.111710 ||   0.7260 ||   0.9139 ||  0.988170 ||  0.989024 ||      1.221408 ||      0.6894 ||   0.8966 ||  0.987064 ||  0.986873 ||   104.280473 || 
    Epoch 27    --      1.050409 ||   0.7435 ||   0.9205 ||  0.989314 ||  0.990131 ||      1.168183 ||      0.7024 ||   0.9041 ||  0.987852 ||  0.988125 ||   104.030442 || 
    Epoch 28    --      1.003232 ||   0.7561 ||   0.9259 ||  0.990362 ||  0.991152 ||      1.127279 ||      0.7097 ||   0.9151 ||  0.988553 ||  0.988771 ||   103.921074 || 
    Epoch 29    --      0.954559 ||   0.7678 ||   0.9318 ||  0.991150 ||  0.991889 ||      1.060092 ||      0.7322 ||   0.9225 ||  0.990240 ||  0.990242 ||   104.030480 || 
    Epoch 30    --      0.905929 ||   0.7817 ||   0.9395 ||  0.992134 ||  0.992799 ||      0.995684 ||      0.7459 ||   0.9292 ||  0.991313 ||  0.991401 ||   104.020848 || 
    Epoch 31    --      0.857508 ||   0.7949 ||   0.9439 ||  0.992951 ||  0.993529 ||      0.942000 ||      0.7680 ||   0.9383 ||  0.991872 ||  0.992206 ||   103.976667 || 
    Epoch 32    --      0.812725 ||   0.8063 ||   0.9488 ||  0.993760 ||  0.994336 ||      0.888468 ||      0.7795 ||   0.9428 ||  0.992920 ||  0.993105 ||   103.999210 || 
    Epoch 33    --      0.768311 ||   0.8151 ||   0.9538 ||  0.994557 ||  0.995091 ||      0.850401 ||      0.7931 ||   0.9463 ||  0.993478 ||  0.993562 ||   104.358608 || 
    Epoch 34    --      0.727603 ||   0.8302 ||   0.9567 ||  0.994865 ||  0.995330 ||      0.806635 ||      0.8025 ||   0.9509 ||  0.994817 ||  0.994882 ||   105.071335 || 
    Epoch 35    --      0.688300 ||   0.8399 ||   0.9621 ||  0.995659 ||  0.996096 ||      0.775256 ||      0.8091 ||   0.9564 ||  0.995381 ||  0.995426 ||   104.796106 || 
    Epoch 36    --      0.653195 ||   0.8509 ||   0.9642 ||  0.995985 ||  0.996392 ||      0.722450 ||      0.8263 ||   0.9610 ||  0.995922 ||  0.996027 ||   103.577366 || 
    Epoch 37    --      0.612112 ||   0.8606 ||   0.9682 ||  0.996494 ||  0.996859 ||      0.729830 ||      0.8181 ||   0.9615 ||  0.995588 ||  0.995541 ||   103.983612 || 
    Epoch 38    --      0.614559 ||   0.8604 ||   0.9692 ||  0.996530 ||  0.996880 ||      0.710249 ||      0.8271 ||   0.9639 ||  0.995436 ||  0.995537 ||   104.186740 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
