Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.596944 ||   0.1642 ||   0.3737 ||  0.731600 ||  0.734243 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.772407 ||   0.1532 ||   0.3694 ||  0.745078 ||  0.751593 ||      3.607235 ||      0.1763 ||   0.4149 ||  0.838451 ||  0.842769 ||    18.686894 || 
    Epoch 01    --      3.388606 ||   0.2214 ||   0.4739 ||  0.860084 ||  0.867529 ||      3.398830 ||      0.2181 ||   0.4726 ||  0.864997 ||  0.867672 ||    17.276958 || 
    Epoch 02    --      3.205290 ||   0.2552 ||   0.5209 ||  0.879757 ||  0.886207 ||      3.241270 ||      0.2562 ||   0.5154 ||  0.877739 ||  0.881417 ||    17.134401 || 
    Epoch 03    --      3.062406 ||   0.2855 ||   0.5555 ||  0.892242 ||  0.897992 ||      3.099947 ||      0.2759 ||   0.5461 ||  0.891995 ||  0.894975 ||    16.818124 || 
    Epoch 04    --      2.933413 ||   0.3092 ||   0.5845 ||  0.903698 ||  0.908996 ||      2.981010 ||      0.2993 ||   0.5778 ||  0.903346 ||  0.904515 ||    17.222069 || 
    Epoch 05    --      2.809935 ||   0.3342 ||   0.6136 ||  0.913285 ||  0.917937 ||      2.852318 ||      0.3231 ||   0.6022 ||  0.912020 ||  0.914307 ||    16.783573 || 
    Epoch 06    --      2.686694 ||   0.3586 ||   0.6422 ||  0.921570 ||  0.925716 ||      2.769522 ||      0.3362 ||   0.6212 ||  0.919082 ||  0.921037 ||    16.910464 || 
    Epoch 07    --      2.579934 ||   0.3825 ||   0.6632 ||  0.928984 ||  0.932796 ||      2.633897 ||      0.3698 ||   0.6521 ||  0.928500 ||  0.929755 ||    17.097607 || 
    Epoch 08    --      2.468783 ||   0.4044 ||   0.6881 ||  0.935792 ||  0.939288 ||      2.540715 ||      0.3923 ||   0.6688 ||  0.933922 ||  0.934867 ||    17.178925 || 
    Epoch 09    --      2.358142 ||   0.4293 ||   0.7081 ||  0.942096 ||  0.945315 ||      2.415562 ||      0.4119 ||   0.6926 ||  0.941820 ||  0.942252 ||    16.922216 || 
    Epoch 10    --      2.247440 ||   0.4513 ||   0.7296 ||  0.948186 ||  0.951122 ||      2.307848 ||      0.4410 ||   0.7219 ||  0.946733 ||  0.947738 ||    16.723784 || 
    Epoch 11    --      2.144406 ||   0.4742 ||   0.7462 ||  0.953298 ||  0.955866 ||      2.259792 ||      0.4445 ||   0.7242 ||  0.950080 ||  0.949969 ||    17.019064 || 
    Epoch 12    --      2.047294 ||   0.4988 ||   0.7667 ||  0.957785 ||  0.960193 ||      2.142358 ||      0.4732 ||   0.7455 ||  0.955605 ||  0.955610 ||    17.003749 || 
    Epoch 13    --      1.953338 ||   0.5214 ||   0.7825 ||  0.961485 ||  0.963817 ||      2.079884 ||      0.4782 ||   0.7609 ||  0.959433 ||  0.958889 ||    16.876022 || 
    Epoch 14    --      1.854363 ||   0.5447 ||   0.7994 ||  0.965445 ||  0.967480 ||      1.970457 ||      0.5069 ||   0.7831 ||  0.962465 ||  0.962971 ||    16.785157 || 
    Epoch 15    --      1.774626 ||   0.5615 ||   0.8137 ||  0.968505 ||  0.970416 ||      1.833483 ||      0.5403 ||   0.8041 ||  0.968932 ||  0.969522 ||    17.096801 || 
    Epoch 16    --      1.685900 ||   0.5840 ||   0.8289 ||  0.972212 ||  0.973939 ||      1.769850 ||      0.5502 ||   0.8152 ||  0.971088 ||  0.971595 ||    16.903042 || 
    Epoch 17    --      1.607602 ||   0.6004 ||   0.8413 ||  0.974702 ||  0.976339 ||      1.680946 ||      0.5792 ||   0.8244 ||  0.972855 ||  0.973384 ||    16.635020 || 
    Epoch 18    --      1.527128 ||   0.6231 ||   0.8543 ||  0.977237 ||  0.978701 ||      1.640272 ||      0.5818 ||   0.8359 ||  0.974744 ||  0.974699 ||    17.087429 || 
    Epoch 19    --      1.446018 ||   0.6414 ||   0.8650 ||  0.979530 ||  0.980852 ||      1.575128 ||      0.6065 ||   0.8480 ||  0.976849 ||  0.976930 ||    16.672054 || 
    Epoch 20    --      1.376013 ||   0.6598 ||   0.8758 ||  0.981547 ||  0.982825 ||      1.500011 ||      0.6187 ||   0.8571 ||  0.978977 ||  0.978959 ||    16.729867 || 
    Epoch 21    --      1.312717 ||   0.6757 ||   0.8848 ||  0.983045 ||  0.984199 ||      1.393841 ||      0.6362 ||   0.8758 ||  0.982938 ||  0.982884 ||    17.088387 || 
    Epoch 22    --      1.241485 ||   0.6935 ||   0.8951 ||  0.985177 ||  0.986244 ||      1.344908 ||      0.6566 ||   0.8818 ||  0.983562 ||  0.983704 ||    17.083203 || 
    Epoch 23    --      1.179789 ||   0.7107 ||   0.9057 ||  0.986375 ||  0.987355 ||      1.285090 ||      0.6743 ||   0.8863 ||  0.984733 ||  0.984866 ||    17.240755 || 
    Epoch 24    --      1.121523 ||   0.7254 ||   0.9105 ||  0.987980 ||  0.988850 ||      1.219539 ||      0.6882 ||   0.8998 ||  0.985756 ||  0.986165 ||    16.845633 || 
    Epoch 25    --      1.065936 ||   0.7415 ||   0.9179 ||  0.988710 ||  0.989551 ||      1.144781 ||      0.7042 ||   0.9099 ||  0.988460 ||  0.988662 ||    17.050866 || 
    Epoch 26    --      1.009534 ||   0.7532 ||   0.9269 ||  0.990100 ||  0.990846 ||      1.114324 ||      0.7128 ||   0.9144 ||  0.989626 ||  0.989470 ||    16.642668 || 
    Epoch 27    --      0.949193 ||   0.7682 ||   0.9327 ||  0.991464 ||  0.992151 ||      1.079347 ||      0.7303 ||   0.9166 ||  0.989586 ||  0.989988 ||    16.977766 || 
    Epoch 28    --      0.905229 ||   0.7819 ||   0.9377 ||  0.992094 ||  0.992765 ||      1.011759 ||      0.7423 ||   0.9280 ||  0.991558 ||  0.991673 ||    16.947909 || 
    Epoch 29    --      0.862169 ||   0.7940 ||   0.9421 ||  0.992998 ||  0.993629 ||      0.955459 ||      0.7594 ||   0.9343 ||  0.992205 ||  0.992231 ||    16.734438 || 
    Epoch 30    --      0.817992 ||   0.8073 ||   0.9484 ||  0.993646 ||  0.994207 ||      0.885309 ||      0.7815 ||   0.9410 ||  0.993466 ||  0.993550 ||    16.676256 || 
    Epoch 31    --      0.770867 ||   0.8188 ||   0.9536 ||  0.994372 ||  0.994847 ||      0.823023 ||      0.7977 ||   0.9491 ||  0.994021 ||  0.994358 ||    17.194483 || 
    Epoch 32    --      0.724792 ||   0.8314 ||   0.9575 ||  0.994963 ||  0.995443 ||      0.823568 ||      0.7932 ||   0.9490 ||  0.994273 ||  0.994245 ||    16.768448 || 
    Epoch 33    --      0.724209 ||   0.8299 ||   0.9578 ||  0.995137 ||  0.995609 ||      0.814487 ||      0.7991 ||   0.9503 ||  0.994725 ||  0.994759 ||    16.716810 || 
    Epoch 34    --      0.683711 ||   0.8416 ||   0.9619 ||  0.995618 ||  0.996053 ||      0.768945 ||      0.8070 ||   0.9543 ||  0.995045 ||  0.994999 ||    17.006122 || 
    Epoch 35    --      0.648407 ||   0.8517 ||   0.9646 ||  0.996107 ||  0.996483 ||      0.751517 ||      0.8115 ||   0.9596 ||  0.995416 ||  0.995323 ||    17.157033 || 
    Epoch 36    --      0.609918 ||   0.8614 ||   0.9690 ||  0.996467 ||  0.996816 ||      0.709312 ||      0.8204 ||   0.9622 ||  0.996305 ||  0.996261 ||    17.259875 || 
    Epoch 37    --      0.575073 ||   0.8697 ||   0.9722 ||  0.997149 ||  0.997462 ||      0.678994 ||      0.8361 ||   0.9644 ||  0.996030 ||  0.995965 ||    16.641959 || 
    Epoch 38    --      0.539836 ||   0.8805 ||   0.9746 ||  0.997417 ||  0.997702 ||      0.637133 ||      0.8509 ||   0.9671 ||  0.996516 ||  0.996611 ||    16.716568 || 
    Epoch 39    --      0.518771 ||   0.8860 ||   0.9768 ||  0.997475 ||  0.997756 ||      0.571298 ||      0.8672 ||   0.9772 ||  0.997791 ||  0.997842 ||    16.525065 || 
    Epoch 40    --      0.481805 ||   0.8972 ||   0.9793 ||  0.998078 ||  0.998321 ||      0.568913 ||      0.8633 ||   0.9765 ||  0.997321 ||  0.997263 ||    16.765363 || 
    Epoch 41    --      0.480239 ||   0.8991 ||   0.9796 ||  0.998019 ||  0.998241 ||      0.577639 ||      0.8600 ||   0.9750 ||  0.997484 ||  0.997387 ||    16.939324 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
