Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -           ['conv2d', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.125665 ||   0.0262 ||   0.0908 ||  0.536109 ||  0.551998 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      4.607374 ||   0.0520 ||   0.1310 ||  0.557030 ||  0.558494 ||      4.606993 ||      0.0090 ||   0.0444 ||  0.500155 ||  0.485915 ||    92.436105 || 
    Epoch 01    --      4.606979 ||   0.0093 ||   0.0481 ||  0.494078 ||  0.495482 ||      4.603943 ||      0.0087 ||   0.0450 ||  0.501173 ||  0.491962 ||    91.311138 || 
    Epoch 02    --      4.603497 ||   0.0104 ||   0.0488 ||  0.493494 ||  0.495095 ||      4.604764 ||      0.0092 ||   0.0470 ||  0.501369 ||  0.487491 ||    92.467370 || 
    Epoch 03    --      4.601031 ||   0.0106 ||   0.0489 ||  0.495453 ||  0.497623 ||      4.602403 ||      0.0097 ||   0.0458 ||  0.504740 ||  0.491318 ||    91.154897 || 
    Epoch 04    --      4.597690 ||   0.0111 ||   0.0511 ||  0.496315 ||  0.498587 ||      4.598292 ||      0.0107 ||   0.0481 ||  0.505823 ||  0.495122 ||    90.733011 || 
    Epoch 05    --      4.595015 ||   0.0131 ||   0.0516 ||  0.498660 ||  0.500940 ||      4.591908 ||      0.0109 ||   0.0482 ||  0.505924 ||  0.496072 ||    91.375047 || 
    Epoch 06    --      4.589659 ||   0.0134 ||   0.0545 ||  0.500503 ||  0.502822 ||      4.590751 ||      0.0117 ||   0.0504 ||  0.509064 ||  0.497349 ||    91.264290 || 
    Epoch 07    --      4.583463 ||   0.0151 ||   0.0567 ||  0.504597 ||  0.507391 ||      4.576679 ||      0.0141 ||   0.0540 ||  0.512014 ||  0.498309 ||    91.338735 || 
    Epoch 08    --      4.575501 ||   0.0170 ||   0.0608 ||  0.509201 ||  0.512477 ||      4.571721 ||      0.0139 ||   0.0555 ||  0.516826 ||  0.506431 ||    90.639290 || 
    Epoch 09    --      4.566990 ||   0.0180 ||   0.0634 ||  0.513137 ||  0.516378 ||      4.556052 ||      0.0215 ||   0.0659 ||  0.521800 ||  0.516182 ||    91.889315 || 
    Epoch 10    --      4.553928 ||   0.0206 ||   0.0689 ||  0.518951 ||  0.523429 ||      4.556977 ||      0.0192 ||   0.0615 ||  0.522746 ||  0.514368 ||    91.217434 || 
    Epoch 11    --      4.542376 ||   0.0236 ||   0.0707 ||  0.523305 ||  0.528511 ||      4.555265 ||      0.0191 ||   0.0626 ||  0.521925 ||  0.514962 ||    91.311221 || 
    Epoch 12    --      4.535797 ||   0.0247 ||   0.0734 ||  0.525465 ||  0.530766 ||      4.523006 ||      0.0259 ||   0.0706 ||  0.533406 ||  0.528270 ||    90.858103 || 
    Epoch 13    --      4.519919 ||   0.0282 ||   0.0782 ||  0.530675 ||  0.536731 ||      4.519454 ||      0.0264 ||   0.0797 ||  0.540748 ||  0.533572 ||    91.540525 || 
    Epoch 14    --      4.510322 ||   0.0303 ||   0.0813 ||  0.534392 ||  0.541476 ||      4.495772 ||      0.0305 ||   0.0812 ||  0.543770 ||  0.542861 ||    91.342497 || 
    Epoch 15    --      4.494157 ||   0.0322 ||   0.0839 ||  0.537636 ||  0.544295 ||      4.497720 ||      0.0310 ||   0.0802 ||  0.541611 ||  0.540623 ||    91.779988 || 
    Epoch 16    --      4.482368 ||   0.0344 ||   0.0887 ||  0.543724 ||  0.552045 ||      4.469105 ||      0.0358 ||   0.0874 ||  0.552846 ||  0.552304 ||    90.936252 || 
    Epoch 17    --      4.464077 ||   0.0395 ||   0.0946 ||  0.550835 ||  0.558454 ||      4.473149 ||      0.0334 ||   0.0855 ||  0.546048 ||  0.549415 ||    91.545636 || 
    Epoch 18    --      4.448516 ||   0.0420 ||   0.0968 ||  0.552363 ||  0.561042 ||      4.460011 ||      0.0365 ||   0.0938 ||  0.556687 ||  0.559875 ||    91.211976 || 
    Epoch 19    --      4.441209 ||   0.0431 ||   0.1002 ||  0.554718 ||  0.564499 ||      4.432406 ||      0.0417 ||   0.1007 ||  0.565682 ||  0.569839 ||    91.201865 || 
    Epoch 20    --      4.423116 ||   0.0457 ||   0.1030 ||  0.559609 ||  0.569390 ||      4.442197 ||      0.0444 ||   0.1058 ||  0.566081 ||  0.569846 ||    91.467539 || 
    Epoch 21    --      4.405319 ||   0.0495 ||   0.1095 ||  0.567690 ||  0.577767 ||      4.409153 ||      0.0468 ||   0.1024 ||  0.564952 ||  0.567390 ||    90.780051 || 
    Epoch 22    --      4.388268 ||   0.0529 ||   0.1121 ||  0.570012 ||  0.580922 ||      4.409479 ||      0.0478 ||   0.1084 ||  0.564410 ||  0.569425 ||    92.053289 || 
    Epoch 23    --      4.380972 ||   0.0545 ||   0.1150 ||  0.568544 ||  0.580391 ||      4.360409 ||      0.0581 ||   0.1186 ||  0.582958 ||  0.587388 ||    91.092557 || 
    Epoch 24    --      4.357279 ||   0.0590 ||   0.1204 ||  0.577886 ||  0.589780 ||      4.353715 ||      0.0594 ||   0.1227 ||  0.584443 ||  0.592094 ||    91.225914 || 
    Epoch 25    --      4.351627 ||   0.0611 ||   0.1231 ||  0.578339 ||  0.590788 ||      4.382266 ||      0.0530 ||   0.1145 ||  0.573864 ||  0.578978 ||    90.757998 || 
    Epoch 26    --      4.341549 ||   0.0633 ||   0.1275 ||  0.581978 ||  0.594813 ||      4.311866 ||      0.0657 ||   0.1285 ||  0.594238 ||  0.600008 ||    91.370954 || 
    Epoch 27    --      4.321936 ||   0.0661 ||   0.1324 ||  0.585477 ||  0.597997 ||      4.300302 ||      0.0686 ||   0.1288 ||  0.590255 ||  0.599114 ||    91.686285 || 
    Epoch 28    --      4.296166 ||   0.0727 ||   0.1373 ||  0.593767 ||  0.606730 ||      4.322418 ||      0.0645 ||   0.1333 ||  0.591113 ||  0.597884 ||    91.295702 || 
    Epoch 29    --      4.283217 ||   0.0764 ||   0.1406 ||  0.595332 ||  0.609276 ||      4.271101 ||      0.0722 ||   0.1350 ||  0.601635 ||  0.608304 ||    91.326956 || 
    Epoch 30    --      4.266466 ||   0.0784 ||   0.1434 ||  0.598501 ||  0.614480 ||      4.260323 ||      0.0798 ||   0.1461 ||  0.606021 ||  0.614119 ||    91.480671 || 
    Epoch 31    --      4.249529 ||   0.0811 ||   0.1479 ||  0.603995 ||  0.618692 ||      4.241914 ||      0.0833 ||   0.1529 ||  0.609884 ||  0.621624 ||    91.248857 || 
    Epoch 32    --      4.232602 ||   0.0840 ||   0.1512 ||  0.607471 ||  0.622348 ||      4.280795 ||      0.0812 ||   0.1558 ||  0.615155 ||  0.628381 ||    91.201967 || 
    Epoch 33    --      4.202928 ||   0.0886 ||   0.1564 ||  0.612987 ||  0.629363 ||      4.201737 ||      0.0907 ||   0.1628 ||  0.623348 ||  0.633305 ||    90.733258 || 
    Epoch 34    --      4.192628 ||   0.0926 ||   0.1619 ||  0.617055 ||  0.634176 ||      4.278720 ||      0.0895 ||   0.1628 ||  0.621077 ||  0.633126 ||    91.295749 || 
    Epoch 35    --      4.183963 ||   0.0953 ||   0.1642 ||  0.617022 ||  0.633399 ||      4.178753 ||      0.0953 ||   0.1603 ||  0.617669 ||  0.628789 ||    91.951952 || 
    Epoch 36    --      4.164290 ||   0.0999 ||   0.1702 ||  0.623517 ||  0.641944 ||      4.186054 ||      0.0919 ||   0.1653 ||  0.622000 ||  0.630677 ||    91.295753 || 
    Epoch 37    --      4.151122 ||   0.1015 ||   0.1732 ||  0.624575 ||  0.641750 ||      4.148006 ||      0.0997 ||   0.1694 ||  0.630505 ||  0.644173 ||    91.623888 || 
    Epoch 38    --      4.133254 ||   0.1054 ||   0.1767 ||  0.627825 ||  0.645951 ||      4.136627 ||      0.1039 ||   0.1784 ||  0.635080 ||  0.649624 ||    90.761537 || 
    Epoch 39    --      4.114393 ||   0.1080 ||   0.1811 ||  0.632730 ||  0.651778 ||      4.142416 ||      0.1010 ||   0.1728 ||  0.632195 ||  0.643395 ||    91.420770 || 
    Epoch 40    --      4.113194 ||   0.1083 ||   0.1818 ||  0.633585 ||  0.651045 ||      4.101684 ||      0.1113 ||   0.1808 ||  0.637375 ||  0.652086 ||    91.248885 || 
    Epoch 41    --      4.103324 ||   0.1116 ||   0.1840 ||  0.635539 ||  0.654691 ||      4.058971 ||      0.1162 ||   0.1871 ||  0.647030 ||  0.658031 ||    91.608240 || 
    Epoch 42    --      4.074905 ||   0.1147 ||   0.1883 ||  0.641208 ||  0.660636 ||      4.125805 ||      0.1064 ||   0.1829 ||  0.640890 ||  0.653046 ||    91.670785 || 
    Epoch 43    --      4.075536 ||   0.1155 ||   0.1870 ||  0.639307 ||  0.658519 ||      4.129087 ||      0.1061 ||   0.1829 ||  0.635229 ||  0.649083 ||    91.332893 || 
Layers list:
	conv2d                                   -                    (17, 17, 3, 24)|(24,)
	conv2d-filters                           -                                       24
	conv2d-kernel_size                       -                                       17
	conv2d-strides                           -                                   (1, 1)
	conv2d-padding                           -                                    valid
	conv2d-data_format                       -                                     None
	conv2d-dilation_rate                     -                                   (1, 1)
	conv2d-groups                            -                                        1
	conv2d-activation                        -                                     relu
	conv2d-use_bias                          -                                     True
	conv2d-kernel_initializer                -                                he_normal
	conv2d-bias_initializer                  -                                    zeros
	conv2d-kernel_regularizer                -                                     None
	conv2d-bias_regularizer                  -                                     None
	conv2d-activity_regularizer              -                                     None
	conv2d-kernel_constraint                 -                                     None
	conv2d-bias_constraint                   -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 16, 16, 24)        20832     
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 635,332
Trainable params: 635,332
Non-trainable params: 0
_________________________________________________________________
