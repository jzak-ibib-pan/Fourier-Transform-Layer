Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -           ['conv2d', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.953529 ||   0.0246 ||   0.0906 ||  0.535724 ||  0.550835 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      4.607360 ||   0.0522 ||   0.1304 ||  0.556544 ||  0.557989 ||      4.607016 ||      0.0090 ||   0.0441 ||  0.499950 ||  0.485708 ||    95.574313 || 
    Epoch 01    --      4.606083 ||   0.0094 ||   0.0481 ||  0.493669 ||  0.495107 ||      4.605036 ||      0.0084 ||   0.0452 ||  0.501221 ||  0.491561 ||    94.672997 || 
    Epoch 02    --      4.604362 ||   0.0097 ||   0.0486 ||  0.493826 ||  0.495372 ||      4.603539 ||      0.0098 ||   0.0470 ||  0.502141 ||  0.488200 ||    95.683824 || 
    Epoch 03    --      4.601130 ||   0.0110 ||   0.0494 ||  0.496548 ||  0.498695 ||      4.599579 ||      0.0099 ||   0.0464 ||  0.504155 ||  0.490298 ||    96.039539 || 
    Epoch 04    --      4.595484 ||   0.0119 ||   0.0530 ||  0.498724 ||  0.501039 ||      4.592780 ||      0.0114 ||   0.0493 ||  0.509708 ||  0.500535 ||    95.527797 || 
    Epoch 05    --      4.590673 ||   0.0137 ||   0.0529 ||  0.501340 ||  0.503749 ||      4.589332 ||      0.0107 ||   0.0487 ||  0.506971 ||  0.497549 ||    96.059028 || 
    Epoch 06    --      4.585863 ||   0.0143 ||   0.0559 ||  0.503414 ||  0.505877 ||      4.581578 ||      0.0128 ||   0.0527 ||  0.512196 ||  0.500830 ||    97.746498 || 
    Epoch 07    --      4.579384 ||   0.0160 ||   0.0585 ||  0.508191 ||  0.511838 ||      4.575151 ||      0.0153 ||   0.0546 ||  0.512302 ||  0.498429 ||    95.784429 || 
    Epoch 08    --      4.572206 ||   0.0176 ||   0.0608 ||  0.508866 ||  0.512673 ||      4.569734 ||      0.0151 ||   0.0561 ||  0.517263 ||  0.506279 ||    96.106053 || 
    Epoch 09    --      4.563253 ||   0.0180 ||   0.0632 ||  0.513834 ||  0.517281 ||      4.554526 ||      0.0206 ||   0.0640 ||  0.520924 ||  0.514887 ||    95.551830 || 
    Epoch 10    --      4.555245 ||   0.0202 ||   0.0684 ||  0.517955 ||  0.522808 ||      4.556801 ||      0.0190 ||   0.0593 ||  0.522475 ||  0.515986 ||    95.981300 || 
    Epoch 11    --      4.543150 ||   0.0230 ||   0.0698 ||  0.521857 ||  0.527605 ||      4.557999 ||      0.0223 ||   0.0677 ||  0.527493 ||  0.521897 ||    95.871905 || 
    Epoch 12    --      4.534161 ||   0.0250 ||   0.0733 ||  0.523984 ||  0.529855 ||      4.532461 ||      0.0224 ||   0.0708 ||  0.532475 ||  0.526782 ||    96.284023 || 
    Epoch 13    --      4.525207 ||   0.0278 ||   0.0777 ||  0.528509 ||  0.535160 ||      4.523347 ||      0.0250 ||   0.0754 ||  0.534649 ||  0.529319 ||    96.043965 || 
    Epoch 14    --      4.508760 ||   0.0302 ||   0.0813 ||  0.531450 ||  0.538457 ||      4.518376 ||      0.0291 ||   0.0784 ||  0.537091 ||  0.535290 ||    95.558001 || 
    Epoch 15    --      4.501572 ||   0.0315 ||   0.0835 ||  0.535719 ||  0.543499 ||      4.515506 ||      0.0292 ||   0.0800 ||  0.544322 ||  0.544423 ||    95.998496 || 
    Epoch 16    --      4.484888 ||   0.0343 ||   0.0884 ||  0.541398 ||  0.550566 ||      4.490204 ||      0.0344 ||   0.0868 ||  0.550191 ||  0.551240 ||    96.028489 || 
    Epoch 17    --      4.469346 ||   0.0382 ||   0.0936 ||  0.547991 ||  0.556792 ||      4.472867 ||      0.0340 ||   0.0870 ||  0.548000 ||  0.550920 ||    95.834802 || 
    Epoch 18    --      4.451971 ||   0.0415 ||   0.0987 ||  0.552456 ||  0.561807 ||      4.459889 ||      0.0380 ||   0.0941 ||  0.557847 ||  0.562914 ||    95.981611 || 
    Epoch 19    --      4.443742 ||   0.0435 ||   0.1018 ||  0.556531 ||  0.567248 ||      4.438892 ||      0.0431 ||   0.1008 ||  0.562967 ||  0.567513 ||    96.687494 || 
    Epoch 20    --      4.423245 ||   0.0468 ||   0.1033 ||  0.558725 ||  0.570113 ||      4.437530 ||      0.0476 ||   0.1091 ||  0.571275 ||  0.578896 ||    95.653683 || 
    Epoch 21    --      4.410892 ||   0.0481 ||   0.1090 ||  0.564488 ||  0.576196 ||      4.479122 ||      0.0509 ||   0.1162 ||  0.573026 ||  0.579757 ||    95.997385 || 
    Epoch 22    --      4.395399 ||   0.0518 ||   0.1120 ||  0.567357 ||  0.579004 ||      4.395902 ||      0.0510 ||   0.1103 ||  0.572753 ||  0.579711 ||    95.872504 || 
    Epoch 23    --      4.380288 ||   0.0553 ||   0.1167 ||  0.570655 ||  0.583928 ||      4.363034 ||      0.0563 ||   0.1142 ||  0.581806 ||  0.587948 ||    95.981935 || 
    Epoch 24    --      4.360221 ||   0.0586 ||   0.1211 ||  0.577559 ||  0.590591 ||      4.355108 ||      0.0596 ||   0.1197 ||  0.585865 ||  0.596087 ||    95.555448 || 
    Epoch 25    --      4.340864 ||   0.0619 ||   0.1244 ||  0.581948 ||  0.595874 ||      4.380274 ||      0.0559 ||   0.1157 ||  0.577130 ||  0.584738 ||    96.885754 || 
    Epoch 26    --      4.335392 ||   0.0646 ||   0.1283 ||  0.583578 ||  0.597738 ||      4.375878 ||      0.0601 ||   0.1284 ||  0.587370 ||  0.594862 ||    96.138314 || 
    Epoch 27    --      4.322491 ||   0.0672 ||   0.1327 ||  0.587806 ||  0.601775 ||      4.327376 ||      0.0677 ||   0.1363 ||  0.599471 ||  0.608523 ||    96.041297 || 
    Epoch 28    --      4.288267 ||   0.0729 ||   0.1397 ||  0.594396 ||  0.609575 ||      4.322826 ||      0.0640 ||   0.1323 ||  0.593282 ||  0.603913 ||    96.684057 || 
    Epoch 29    --      4.292026 ||   0.0745 ||   0.1407 ||  0.595098 ||  0.609879 ||      4.246587 ||      0.0760 ||   0.1430 ||  0.607266 ||  0.618381 ||    96.420200 || 
    Epoch 30    --      4.265744 ||   0.0768 ||   0.1449 ||  0.599111 ||  0.615269 ||      4.337716 ||      0.0739 ||   0.1437 ||  0.600008 ||  0.607916 ||    96.794676 || 
    Epoch 31    --      4.251374 ||   0.0815 ||   0.1481 ||  0.602355 ||  0.618909 ||      4.260392 ||      0.0821 ||   0.1547 ||  0.607497 ||  0.619879 ||    96.857195 || 
    Epoch 32    --      4.231800 ||   0.0838 ||   0.1531 ||  0.607117 ||  0.623257 ||      4.249234 ||      0.0795 ||   0.1472 ||  0.608490 ||  0.620924 ||    97.216652 || 
    Epoch 33    --      4.228659 ||   0.0852 ||   0.1523 ||  0.608521 ||  0.624468 ||      4.235056 ||      0.0882 ||   0.1577 ||  0.614481 ||  0.625983 ||    96.076042 || 
    Epoch 34    --      4.198675 ||   0.0917 ||   0.1604 ||  0.614418 ||  0.631884 ||      4.220566 ||      0.0847 ||   0.1567 ||  0.614132 ||  0.625590 ||    95.638568 || 
    Epoch 35    --      4.192702 ||   0.0945 ||   0.1640 ||  0.617559 ||  0.635035 ||      4.179902 ||      0.0922 ||   0.1600 ||  0.618094 ||  0.629908 ||    96.185528 || 
    Epoch 36    --      4.168056 ||   0.0981 ||   0.1683 ||  0.620387 ||  0.639610 ||      4.178815 ||      0.0941 ||   0.1628 ||  0.621111 ||  0.630204 ||    96.107401 || 
    Epoch 37    --      4.154622 ||   0.1008 ||   0.1726 ||  0.622970 ||  0.641515 ||      4.195044 ||      0.0917 ||   0.1576 ||  0.614316 ||  0.626038 ||    96.579426 || 
    Epoch 38    --      4.152292 ||   0.1014 ||   0.1725 ||  0.623365 ||  0.642313 ||      4.164855 ||      0.0965 ||   0.1732 ||  0.625987 ||  0.639111 ||    96.091878 || 
    Epoch 39    --      4.124028 ||   0.1052 ||   0.1801 ||  0.629971 ||  0.649181 ||      4.128612 ||      0.1034 ||   0.1717 ||  0.631022 ||  0.644524 ||    95.685631 || 
    Epoch 40    --      4.128603 ||   0.1050 ||   0.1770 ||  0.628731 ||  0.647033 ||      4.088276 ||      0.1103 ||   0.1863 ||  0.639066 ||  0.656934 ||    96.076249 || 
    Epoch 41    --      4.102537 ||   0.1102 ||   0.1834 ||  0.634172 ||  0.654397 ||      4.125264 ||      0.1081 ||   0.1805 ||  0.635085 ||  0.646601 ||    96.049150 || 
    Epoch 42    --      4.102030 ||   0.1106 ||   0.1830 ||  0.633925 ||  0.653234 ||      4.115708 ||      0.1064 ||   0.1750 ||  0.630079 ||  0.642290 ||    95.873195 || 
Layers list:
	conv2d                                   -                    (17, 17, 3, 24)|(24,)
	conv2d-filters                           -                                       24
	conv2d-kernel_size                       -                                       17
	conv2d-strides                           -                                   (1, 1)
	conv2d-padding                           -                                    valid
	conv2d-data_format                       -                                     None
	conv2d-dilation_rate                     -                                   (1, 1)
	conv2d-groups                            -                                        1
	conv2d-activation                        -                                     relu
	conv2d-use_bias                          -                                     True
	conv2d-kernel_initializer                -                                he_normal
	conv2d-bias_initializer                  -                                    zeros
	conv2d-kernel_regularizer                -                                     None
	conv2d-bias_regularizer                  -                                     None
	conv2d-activity_regularizer              -                                     None
	conv2d-kernel_constraint                 -                                     None
	conv2d-bias_constraint                   -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 16, 16, 24)        20832     
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 635,332
Trainable params: 635,332
Non-trainable params: 0
_________________________________________________________________
