Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.222185 ||   0.1672 ||   0.3839 ||  0.742387 ||  0.745916 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.782981 ||   0.1519 ||   0.3717 ||  0.743341 ||  0.749776 ||      3.623685 ||      0.1766 ||   0.4085 ||  0.836733 ||  0.841622 ||    99.560191 || 
    Epoch 01    --      3.388500 ||   0.2208 ||   0.4771 ||  0.860494 ||  0.867973 ||      3.392992 ||      0.2214 ||   0.4756 ||  0.866247 ||  0.869077 ||    97.607139 || 
    Epoch 02    --      3.201046 ||   0.2546 ||   0.5231 ||  0.880471 ||  0.886988 ||      3.239983 ||      0.2534 ||   0.5160 ||  0.877668 ||  0.881579 ||    98.232140 || 
    Epoch 03    --      3.058221 ||   0.2879 ||   0.5584 ||  0.892870 ||  0.898578 ||      3.095613 ||      0.2738 ||   0.5442 ||  0.892873 ||  0.895828 ||    98.607205 || 
    Epoch 04    --      2.930719 ||   0.3070 ||   0.5857 ||  0.904103 ||  0.909397 ||      2.985088 ||      0.2998 ||   0.5768 ||  0.902625 ||  0.903951 ||    98.200953 || 
    Epoch 05    --      2.809929 ||   0.3334 ||   0.6140 ||  0.913395 ||  0.918198 ||      2.846364 ||      0.3225 ||   0.6028 ||  0.913215 ||  0.915264 ||    99.685323 || 
    Epoch 06    --      2.687440 ||   0.3566 ||   0.6433 ||  0.922070 ||  0.926134 ||      2.774045 ||      0.3354 ||   0.6216 ||  0.918133 ||  0.920391 ||    99.654194 || 
    Epoch 07    --      2.582268 ||   0.3813 ||   0.6656 ||  0.929032 ||  0.932893 ||      2.635772 ||      0.3684 ||   0.6525 ||  0.927805 ||  0.929386 ||   100.841644 || 
    Epoch 08    --      2.473700 ||   0.4028 ||   0.6872 ||  0.935500 ||  0.939066 ||      2.542572 ||      0.3841 ||   0.6718 ||  0.934413 ||  0.935542 ||   101.062364 || 
    Epoch 09    --      2.368685 ||   0.4261 ||   0.7088 ||  0.941654 ||  0.944783 ||      2.427869 ||      0.4101 ||   0.6891 ||  0.941046 ||  0.941557 ||   101.244640 || 
    Epoch 10    --      2.256510 ||   0.4502 ||   0.7294 ||  0.947546 ||  0.950514 ||      2.328776 ||      0.4316 ||   0.7165 ||  0.945882 ||  0.947032 ||   101.479183 || 
    Epoch 11    --      2.157291 ||   0.4715 ||   0.7459 ||  0.952669 ||  0.955336 ||      2.255797 ||      0.4531 ||   0.7271 ||  0.950152 ||  0.950327 ||   101.041628 || 
    Epoch 12    --      2.062370 ||   0.4963 ||   0.7642 ||  0.956952 ||  0.959395 ||      2.166483 ||      0.4665 ||   0.7404 ||  0.954514 ||  0.954254 ||   101.451201 || 
    Epoch 13    --      1.971042 ||   0.5147 ||   0.7812 ||  0.960641 ||  0.962958 ||      2.082255 ||      0.4787 ||   0.7602 ||  0.959585 ||  0.958872 ||   101.091869 || 
    Epoch 14    --      1.872306 ||   0.5397 ||   0.7965 ||  0.964944 ||  0.967006 ||      1.984254 ||      0.5066 ||   0.7819 ||  0.961911 ||  0.962187 ||   101.076261 || 
    Epoch 15    --      1.790669 ||   0.5593 ||   0.8106 ||  0.967915 ||  0.969803 ||      1.861364 ||      0.5301 ||   0.7985 ||  0.967896 ||  0.968641 ||   101.732540 || 
    Epoch 16    --      1.703078 ||   0.5796 ||   0.8245 ||  0.971536 ||  0.973248 ||      1.790247 ||      0.5486 ||   0.8154 ||  0.969712 ||  0.970467 ||   101.121268 || 
    Epoch 17    --      1.625027 ||   0.5961 ||   0.8374 ||  0.974143 ||  0.975771 ||      1.701727 ||      0.5704 ||   0.8234 ||  0.972846 ||  0.973228 ||   101.045100 || 
    Epoch 18    --      1.549601 ||   0.6143 ||   0.8514 ||  0.976627 ||  0.978100 ||      1.638628 ||      0.5873 ||   0.8368 ||  0.974933 ||  0.975243 ||   101.170162 || 
    Epoch 19    --      1.465272 ||   0.6358 ||   0.8637 ||  0.979022 ||  0.980370 ||      1.590230 ||      0.5993 ||   0.8439 ||  0.976640 ||  0.976821 ||   101.051782 || 
    Epoch 20    --      1.394568 ||   0.6528 ||   0.8724 ||  0.981040 ||  0.982368 ||      1.511404 ||      0.6196 ||   0.8593 ||  0.978903 ||  0.978838 ||   101.717052 || 
    Epoch 21    --      1.329983 ||   0.6728 ||   0.8822 ||  0.982657 ||  0.983786 ||      1.424299 ||      0.6312 ||   0.8707 ||  0.981806 ||  0.981702 ||   101.295194 || 
    Epoch 22    --      1.260069 ||   0.6892 ||   0.8922 ||  0.984664 ||  0.985690 ||      1.349544 ||      0.6634 ||   0.8793 ||  0.982657 ||  0.983170 ||   100.956372 || 
    Epoch 23    --      1.199180 ||   0.7052 ||   0.9020 ||  0.985974 ||  0.986961 ||      1.286894 ||      0.6709 ||   0.8850 ||  0.984796 ||  0.985111 ||   100.783898 || 
    Epoch 24    --      1.141825 ||   0.7189 ||   0.9093 ||  0.987520 ||  0.988402 ||      1.224739 ||      0.6898 ||   0.9020 ||  0.985848 ||  0.986169 ||   101.014029 || 
    Epoch 25    --      1.084608 ||   0.7335 ||   0.9161 ||  0.988730 ||  0.989522 ||      1.158780 ||      0.7039 ||   0.9066 ||  0.987788 ||  0.988008 ||   100.560909 || 
    Epoch 26    --      1.023784 ||   0.7498 ||   0.9238 ||  0.990096 ||  0.990807 ||      1.134299 ||      0.7130 ||   0.9133 ||  0.988551 ||  0.988519 ||    97.420342 || 
    Epoch 27    --      0.966542 ||   0.7640 ||   0.9322 ||  0.991191 ||  0.991881 ||      1.087771 ||      0.7246 ||   0.9164 ||  0.988832 ||  0.989125 ||    98.029760 || 
    Epoch 28    --      0.918487 ||   0.7776 ||   0.9362 ||  0.991782 ||  0.992420 ||      1.030525 ||      0.7359 ||   0.9248 ||  0.990533 ||  0.990672 ||    97.586531 || 
    Epoch 29    --      0.876238 ||   0.7878 ||   0.9410 ||  0.992794 ||  0.993402 ||      0.972359 ||      0.7555 ||   0.9350 ||  0.991704 ||  0.991665 ||    98.958293 || 
    Epoch 30    --      0.826974 ||   0.8030 ||   0.9479 ||  0.993431 ||  0.993985 ||      0.911499 ||      0.7731 ||   0.9394 ||  0.993141 ||  0.993238 ||    99.939954 || 
    Epoch 31    --      0.782342 ||   0.8148 ||   0.9520 ||  0.994204 ||  0.994701 ||      0.861159 ||      0.7858 ||   0.9460 ||  0.993351 ||  0.993519 ||   100.514171 || 
    Epoch 32    --      0.737217 ||   0.8288 ||   0.9552 ||  0.994892 ||  0.995359 ||      0.812252 ||      0.7979 ||   0.9524 ||  0.993963 ||  0.994166 ||   100.795458 || 
    Epoch 33    --      0.694309 ||   0.8404 ||   0.9614 ||  0.995537 ||  0.995961 ||      0.772545 ||      0.8155 ||   0.9540 ||  0.994754 ||  0.994813 ||   100.025716 || 
    Epoch 34    --      0.660906 ||   0.8478 ||   0.9637 ||  0.995819 ||  0.996206 ||      0.729434 ||      0.8230 ||   0.9607 ||  0.995686 ||  0.995629 ||   100.904866 || 
    Epoch 35    --      0.624696 ||   0.8557 ||   0.9679 ||  0.996490 ||  0.996813 ||      0.703369 ||      0.8253 ||   0.9643 ||  0.995888 ||  0.995876 ||    99.723330 || 
    Epoch 36    --      0.583756 ||   0.8692 ||   0.9717 ||  0.996727 ||  0.997038 ||      0.649581 ||      0.8497 ||   0.9662 ||  0.996518 ||  0.996602 ||   101.170512 || 
    Epoch 37    --      0.549567 ||   0.8778 ||   0.9749 ||  0.997272 ||  0.997548 ||      0.650901 ||      0.8402 ||   0.9655 ||  0.996621 ||  0.996566 ||   100.936139 || 
    Epoch 38    --      0.549046 ||   0.8792 ||   0.9747 ||  0.997112 ||  0.997391 ||      0.656728 ||      0.8384 ||   0.9666 ||  0.996740 ||  0.996717 ||   100.733029 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
