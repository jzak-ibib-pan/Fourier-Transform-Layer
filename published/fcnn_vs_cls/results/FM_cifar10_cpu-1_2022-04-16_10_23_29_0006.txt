Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                       10
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.3
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      4.841080 ||   0.4246 ||   0.8760 ||  0.782412 ||  0.779812 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.767186 ||   0.3221 ||   0.7883 ||  0.735673 ||  0.737689 ||      4.615761 ||      0.3474 ||   0.8443 ||  0.763400 ||  0.748150 ||    17.226424 || 
    Epoch 01    --      3.630842 ||   0.4244 ||   0.8860 ||  0.799539 ||  0.801491 ||      3.563357 ||      0.4211 ||   0.8866 ||  0.805375 ||  0.803077 ||    15.229295 || 
    Epoch 02    --      3.552046 ||   0.4480 ||   0.8960 ||  0.810183 ||  0.812109 ||      4.195761 ||      0.4158 ||   0.8849 ||  0.792689 ||  0.782423 ||    15.215710 || 
    Epoch 03    --      3.587934 ||   0.4413 ||   0.8982 ||  0.809183 ||  0.810916 ||      3.554059 ||      0.4414 ||   0.8979 ||  0.812887 ||  0.810275 ||    14.852806 || 
    Epoch 04    --      3.473333 ||   0.4568 ||   0.9072 ||  0.817197 ||  0.819168 ||      4.098314 ||      0.4113 ||   0.9059 ||  0.799282 ||  0.792617 ||    14.963346 || 
    Epoch 05    --      3.532620 ||   0.4569 ||   0.9046 ||  0.816389 ||  0.818121 ||      3.434917 ||      0.4564 ||   0.9128 ||  0.820296 ||  0.819376 ||    15.122707 || 
    Epoch 06    --      3.478642 ||   0.4676 ||   0.9106 ||  0.822529 ||  0.824308 ||      3.349196 ||      0.4668 ||   0.9197 ||  0.830948 ||  0.830831 ||    15.339150 || 
    Epoch 07    --      3.385685 ||   0.4824 ||   0.9201 ||  0.829582 ||  0.831352 ||      3.663533 ||      0.4534 ||   0.9162 ||  0.819504 ||  0.815243 ||    14.969536 || 
    Epoch 08    --      3.434779 ||   0.4813 ||   0.9165 ||  0.828008 ||  0.829747 ||      3.623852 ||      0.4763 ||   0.9112 ||  0.829984 ||  0.826376 ||    15.249205 || 
    Epoch 09    --      3.436341 ||   0.4818 ||   0.9210 ||  0.830057 ||  0.831812 ||      3.732466 ||      0.4769 ||   0.9232 ||  0.826434 ||  0.821305 ||    15.229235 || 
    Epoch 10    --      3.352253 ||   0.4932 ||   0.9219 ||  0.833116 ||  0.834990 ||      3.436767 ||      0.4906 ||   0.9184 ||  0.835084 ||  0.833714 ||    15.330195 || 
    Epoch 11    --      3.403540 ||   0.4993 ||   0.9232 ||  0.835101 ||  0.836774 ||      3.315996 ||      0.4985 ||   0.9244 ||  0.840907 ||  0.837953 ||    15.282644 || 
    Epoch 12    --      3.320720 ||   0.5026 ||   0.9280 ||  0.838261 ||  0.840095 ||      3.470213 ||      0.5095 ||   0.9214 ||  0.839980 ||  0.836369 ||    16.069049 || 
    Epoch 13    --      3.312949 ||   0.5100 ||   0.9284 ||  0.840680 ||  0.842432 ||      3.417877 ||      0.4952 ||   0.9334 ||  0.839252 ||  0.836184 ||    15.879681 || 
    Epoch 14    --      3.332350 ||   0.5088 ||   0.9287 ||  0.839353 ||  0.841044 ||      3.755260 ||      0.4945 ||   0.9180 ||  0.829374 ||  0.826763 ||    15.222632 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                         (6144, 10)|(10,)
	dense_2-units                            -                                       10
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                61450     
=================================================================
Total params: 64,522
Trainable params: 64,522
Non-trainable params: 0
_________________________________________________________________
