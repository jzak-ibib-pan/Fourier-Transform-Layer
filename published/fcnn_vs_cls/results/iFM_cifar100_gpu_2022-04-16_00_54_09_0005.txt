Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.039757 ||   0.1685 ||   0.3816 ||  0.742898 ||  0.745997 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.778551 ||   0.1524 ||   0.3673 ||  0.742706 ||  0.749125 ||      3.618960 ||      0.1786 ||   0.4120 ||  0.837063 ||  0.841435 ||    16.804590 || 
    Epoch 01    --      3.399041 ||   0.2201 ||   0.4719 ||  0.858918 ||  0.866377 ||      3.405266 ||      0.2158 ||   0.4738 ||  0.864318 ||  0.867048 ||    14.072993 || 
    Epoch 02    --      3.212444 ||   0.2521 ||   0.5202 ||  0.879119 ||  0.885629 ||      3.256119 ||      0.2488 ||   0.5092 ||  0.876620 ||  0.880230 ||    12.872442 || 
    Epoch 03    --      3.068632 ||   0.2835 ||   0.5549 ||  0.891760 ||  0.897472 ||      3.119728 ||      0.2681 ||   0.5362 ||  0.890680 ||  0.893659 ||    14.316455 || 
    Epoch 04    --      2.943513 ||   0.3059 ||   0.5824 ||  0.902893 ||  0.908143 ||      2.994727 ||      0.3026 ||   0.5742 ||  0.901418 ||  0.902763 ||    14.140860 || 
    Epoch 05    --      2.823331 ||   0.3298 ||   0.6112 ||  0.912175 ||  0.916866 ||      2.859843 ||      0.3189 ||   0.5993 ||  0.912570 ||  0.914419 ||    14.018811 || 
    Epoch 06    --      2.699097 ||   0.3586 ||   0.6395 ||  0.921272 ||  0.925363 ||      2.786733 ||      0.3332 ||   0.6165 ||  0.917592 ||  0.919657 ||    13.362812 || 
    Epoch 07    --      2.595246 ||   0.3774 ||   0.6607 ||  0.928150 ||  0.931916 ||      2.641392 ||      0.3696 ||   0.6501 ||  0.927926 ||  0.929383 ||    13.615088 || 
    Epoch 08    --      2.485667 ||   0.3986 ||   0.6852 ||  0.934790 ||  0.938305 ||      2.548676 ||      0.3854 ||   0.6727 ||  0.933804 ||  0.934690 ||    13.815135 || 
    Epoch 09    --      2.373773 ||   0.4244 ||   0.7072 ||  0.941368 ||  0.944576 ||      2.431990 ||      0.4064 ||   0.6886 ||  0.941295 ||  0.941587 ||    14.378671 || 
    Epoch 10    --      2.258424 ||   0.4492 ||   0.7282 ||  0.947659 ||  0.950579 ||      2.335763 ||      0.4284 ||   0.7132 ||  0.945470 ||  0.946376 ||    14.216894 || 
    Epoch 11    --      2.158445 ||   0.4724 ||   0.7464 ||  0.952556 ||  0.955128 ||      2.277021 ||      0.4439 ||   0.7238 ||  0.949728 ||  0.949364 ||    14.723191 || 
    Epoch 12    --      2.060139 ||   0.4963 ||   0.7656 ||  0.956775 ||  0.959228 ||      2.144912 ||      0.4702 ||   0.7472 ||  0.955504 ||  0.955272 ||    13.449676 || 
    Epoch 13    --      1.968707 ||   0.5157 ||   0.7829 ||  0.960909 ||  0.963172 ||      2.090800 ||      0.4791 ||   0.7640 ||  0.958968 ||  0.958183 ||    13.769238 || 
    Epoch 14    --      1.866439 ||   0.5414 ||   0.7998 ||  0.965148 ||  0.967090 ||      1.966551 ||      0.5107 ||   0.7845 ||  0.962785 ||  0.963361 ||    14.830323 || 
    Epoch 15    --      1.777971 ||   0.5607 ||   0.8139 ||  0.968435 ||  0.970258 ||      1.856767 ||      0.5390 ||   0.8002 ||  0.966927 ||  0.967665 ||    14.318848 || 
    Epoch 16    --      1.690645 ||   0.5814 ||   0.8297 ||  0.971694 ||  0.973436 ||      1.799182 ||      0.5484 ||   0.8135 ||  0.968859 ||  0.969242 ||    13.700848 || 
    Epoch 17    --      1.614843 ||   0.6007 ||   0.8404 ||  0.974070 ||  0.975682 ||      1.688578 ||      0.5740 ||   0.8262 ||  0.973485 ||  0.973666 ||    12.999359 || 
    Epoch 18    --      1.535649 ||   0.6202 ||   0.8519 ||  0.976795 ||  0.978208 ||      1.631557 ||      0.5851 ||   0.8378 ||  0.975165 ||  0.975158 ||    15.103776 || 
    Epoch 19    --      1.450080 ||   0.6434 ||   0.8649 ||  0.979142 ||  0.980471 ||      1.570190 ||      0.6052 ||   0.8512 ||  0.976319 ||  0.976588 ||    13.850730 || 
    Epoch 20    --      1.378709 ||   0.6583 ||   0.8772 ||  0.981437 ||  0.982707 ||      1.490803 ||      0.6238 ||   0.8612 ||  0.978685 ||  0.978908 ||    14.268498 || 
    Epoch 21    --      1.317037 ||   0.6745 ||   0.8854 ||  0.982854 ||  0.983994 ||      1.401080 ||      0.6425 ||   0.8721 ||  0.982866 ||  0.982758 ||    15.215118 || 
    Epoch 22    --      1.245226 ||   0.6927 ||   0.8944 ||  0.984894 ||  0.985953 ||      1.346509 ||      0.6635 ||   0.8809 ||  0.982918 ||  0.983012 ||    14.871275 || 
    Epoch 23    --      1.181693 ||   0.7107 ||   0.9046 ||  0.986265 ||  0.987229 ||      1.278141 ||      0.6798 ||   0.8873 ||  0.984779 ||  0.984840 ||    14.538608 || 
    Epoch 24    --      1.122383 ||   0.7246 ||   0.9109 ||  0.987986 ||  0.988866 ||      1.221932 ||      0.6909 ||   0.9003 ||  0.985236 ||  0.985703 ||    14.031003 || 
    Epoch 25    --      1.065469 ||   0.7394 ||   0.9182 ||  0.988744 ||  0.989591 ||      1.155466 ||      0.7080 ||   0.9100 ||  0.987841 ||  0.988005 ||    13.950068 || 
    Epoch 26    --      1.010682 ||   0.7543 ||   0.9262 ||  0.990223 ||  0.990979 ||      1.108241 ||      0.7206 ||   0.9152 ||  0.989280 ||  0.989143 ||    14.239713 || 
    Epoch 27    --      0.951251 ||   0.7679 ||   0.9334 ||  0.991316 ||  0.992010 ||      1.058352 ||      0.7385 ||   0.9220 ||  0.989053 ||  0.989468 ||    13.961778 || 
    Epoch 28    --      0.905573 ||   0.7821 ||   0.9376 ||  0.992088 ||  0.992753 ||      1.014857 ||      0.7398 ||   0.9290 ||  0.991191 ||  0.991328 ||    13.648676 || 
    Epoch 29    --      0.862037 ||   0.7923 ||   0.9426 ||  0.992845 ||  0.993475 ||      0.958710 ||      0.7560 ||   0.9383 ||  0.992001 ||  0.992036 ||    15.177219 || 
    Epoch 30    --      0.816767 ||   0.8056 ||   0.9485 ||  0.993557 ||  0.994126 ||      0.889878 ||      0.7786 ||   0.9415 ||  0.993207 ||  0.993281 ||    14.224632 || 
    Epoch 31    --      0.771505 ||   0.8186 ||   0.9529 ||  0.994358 ||  0.994867 ||      0.836499 ||      0.7956 ||   0.9487 ||  0.993630 ||  0.993882 ||    14.026802 || 
    Epoch 32    --      0.726642 ||   0.8297 ||   0.9574 ||  0.995070 ||  0.995530 ||      0.818663 ||      0.7939 ||   0.9526 ||  0.994213 ||  0.994179 ||    13.600721 || 
    Epoch 33    --      0.723719 ||   0.8320 ||   0.9585 ||  0.995113 ||  0.995567 ||      0.801199 ||      0.8040 ||   0.9495 ||  0.994336 ||  0.994407 ||    14.490066 || 
    Epoch 34    --      0.687939 ||   0.8388 ||   0.9613 ||  0.995497 ||  0.995931 ||      0.758844 ||      0.8130 ||   0.9567 ||  0.995151 ||  0.995177 ||    14.381839 || 
    Epoch 35    --      0.648454 ||   0.8521 ||   0.9656 ||  0.996106 ||  0.996471 ||      0.749999 ||      0.8116 ||   0.9583 ||  0.995269 ||  0.995164 ||    14.411015 || 
    Epoch 36    --      0.647216 ||   0.8503 ||   0.9659 ||  0.996137 ||  0.996517 ||      0.744150 ||      0.8092 ||   0.9587 ||  0.995635 ||  0.995643 ||    14.357368 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
