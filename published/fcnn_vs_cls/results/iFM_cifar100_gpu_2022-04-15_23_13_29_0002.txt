Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.881495 ||   0.1681 ||   0.3792 ||  0.727593 ||  0.730183 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.780086 ||   0.1532 ||   0.3721 ||  0.744507 ||  0.751034 ||      3.615808 ||      0.1786 ||   0.4104 ||  0.837845 ||  0.842195 ||    18.452743 || 
    Epoch 01    --      3.389770 ||   0.2212 ||   0.4746 ||  0.860377 ||  0.867873 ||      3.406236 ||      0.2180 ||   0.4724 ||  0.864406 ||  0.867045 ||    16.577481 || 
    Epoch 02    --      3.208972 ||   0.2523 ||   0.5208 ||  0.879601 ||  0.886122 ||      3.242971 ||      0.2538 ||   0.5140 ||  0.878205 ||  0.881847 ||    16.954901 || 
    Epoch 03    --      3.066283 ||   0.2845 ||   0.5565 ||  0.892126 ||  0.897912 ||      3.113324 ||      0.2695 ||   0.5421 ||  0.891204 ||  0.894073 ||    16.821623 || 
    Epoch 04    --      2.942217 ||   0.3074 ||   0.5803 ||  0.903243 ||  0.908456 ||      2.989887 ||      0.2999 ||   0.5726 ||  0.902024 ||  0.903420 ||    17.325209 || 
    Epoch 05    --      2.823449 ||   0.3319 ||   0.6109 ||  0.912194 ||  0.917013 ||      2.860229 ||      0.3220 ||   0.5986 ||  0.912897 ||  0.914493 ||    16.723979 || 
    Epoch 06    --      2.699365 ||   0.3546 ||   0.6392 ||  0.921500 ||  0.925622 ||      2.789618 ||      0.3320 ||   0.6167 ||  0.917662 ||  0.919527 ||    16.907960 || 
    Epoch 07    --      2.596532 ||   0.3778 ||   0.6608 ||  0.928074 ||  0.931918 ||      2.634534 ||      0.3669 ||   0.6484 ||  0.928411 ||  0.929945 ||    16.890500 || 
    Epoch 08    --      2.483503 ||   0.4000 ||   0.6834 ||  0.935246 ||  0.938760 ||      2.565304 ||      0.3807 ||   0.6674 ||  0.933198 ||  0.934105 ||    17.031965 || 
    Epoch 09    --      2.375179 ||   0.4236 ||   0.7059 ||  0.941511 ||  0.944672 ||      2.438936 ||      0.4050 ||   0.6854 ||  0.941023 ||  0.941333 ||    16.888744 || 
    Epoch 10    --      2.262414 ||   0.4492 ||   0.7255 ||  0.947806 ||  0.950724 ||      2.334122 ||      0.4273 ||   0.7153 ||  0.945718 ||  0.946712 ||    17.025958 || 
    Epoch 11    --      2.159994 ||   0.4712 ||   0.7456 ||  0.952773 ||  0.955329 ||      2.281492 ||      0.4441 ||   0.7216 ||  0.949801 ||  0.949322 ||    17.434789 || 
    Epoch 12    --      2.064339 ||   0.4948 ||   0.7653 ||  0.957206 ||  0.959658 ||      2.160133 ||      0.4668 ||   0.7440 ||  0.954946 ||  0.954873 ||    17.008787 || 
    Epoch 13    --      1.971762 ||   0.5129 ||   0.7807 ||  0.960980 ||  0.963253 ||      2.099847 ||      0.4766 ||   0.7577 ||  0.959007 ||  0.958245 ||    16.697982 || 
    Epoch 14    --      1.871027 ||   0.5413 ||   0.7976 ||  0.965057 ||  0.967052 ||      1.981422 ||      0.5037 ||   0.7791 ||  0.962835 ||  0.962957 ||    17.171429 || 
    Epoch 15    --      1.787539 ||   0.5599 ||   0.8098 ||  0.968437 ||  0.970208 ||      1.856836 ||      0.5366 ||   0.8011 ||  0.966512 ||  0.967281 ||    17.088735 || 
    Epoch 16    --      1.696667 ||   0.5825 ||   0.8261 ||  0.971768 ||  0.973442 ||      1.800163 ||      0.5472 ||   0.8121 ||  0.969406 ||  0.969763 ||    16.771810 || 
    Epoch 17    --      1.618152 ||   0.5961 ||   0.8390 ||  0.974518 ||  0.976099 ||      1.691424 ||      0.5773 ||   0.8274 ||  0.972896 ||  0.973314 ||    16.971843 || 
    Epoch 18    --      1.542945 ||   0.6181 ||   0.8511 ||  0.977133 ||  0.978611 ||      1.628859 ||      0.5877 ||   0.8366 ||  0.975465 ||  0.975309 ||    17.096766 || 
    Epoch 19    --      1.453151 ||   0.6371 ||   0.8639 ||  0.979514 ||  0.980830 ||      1.597314 ||      0.5973 ||   0.8457 ||  0.975998 ||  0.976116 ||    17.197377 || 
    Epoch 20    --      1.385416 ||   0.6553 ||   0.8751 ||  0.981469 ||  0.982764 ||      1.514821 ||      0.6127 ||   0.8558 ||  0.978475 ||  0.978457 ||    16.844132 || 
    Epoch 21    --      1.324471 ||   0.6728 ||   0.8842 ||  0.982815 ||  0.984013 ||      1.397940 ||      0.6309 ||   0.8779 ||  0.982896 ||  0.982745 ||    17.007986 || 
    Epoch 22    --      1.252649 ||   0.6895 ||   0.8942 ||  0.984862 ||  0.985906 ||      1.343450 ||      0.6571 ||   0.8828 ||  0.984058 ||  0.984081 ||    16.537872 || 
    Epoch 23    --      1.187670 ||   0.7077 ||   0.9027 ||  0.986192 ||  0.987159 ||      1.285594 ||      0.6740 ||   0.8867 ||  0.984902 ||  0.985067 ||    16.723371 || 
    Epoch 24    --      1.128704 ||   0.7196 ||   0.9111 ||  0.988098 ||  0.988974 ||      1.221074 ||      0.6909 ||   0.8970 ||  0.986198 ||  0.986612 ||    16.949526 || 
    Epoch 25    --      1.070915 ||   0.7385 ||   0.9175 ||  0.988916 ||  0.989740 ||      1.144165 ||      0.7124 ||   0.9109 ||  0.988500 ||  0.988801 ||    17.108920 || 
    Epoch 26    --      1.013032 ||   0.7511 ||   0.9259 ||  0.990322 ||  0.991091 ||      1.120515 ||      0.7159 ||   0.9132 ||  0.988854 ||  0.988709 ||    17.021943 || 
    Epoch 27    --      0.957907 ||   0.7668 ||   0.9318 ||  0.991437 ||  0.992138 ||      1.080207 ||      0.7244 ||   0.9190 ||  0.989510 ||  0.989667 ||    16.618627 || 
    Epoch 28    --      0.910906 ||   0.7796 ||   0.9370 ||  0.992189 ||  0.992884 ||      1.005221 ||      0.7437 ||   0.9304 ||  0.991614 ||  0.991746 ||    17.563573 || 
    Epoch 29    --      0.865234 ||   0.7918 ||   0.9427 ||  0.992868 ||  0.993500 ||      0.966166 ||      0.7503 ||   0.9350 ||  0.992171 ||  0.992162 ||    17.194391 || 
    Epoch 30    --      0.814958 ||   0.8051 ||   0.9487 ||  0.993771 ||  0.994328 ||      0.898604 ||      0.7773 ||   0.9390 ||  0.992689 ||  0.992782 ||    17.032631 || 
    Epoch 31    --      0.774273 ||   0.8155 ||   0.9530 ||  0.994385 ||  0.994874 ||      0.839138 ||      0.7954 ||   0.9470 ||  0.993871 ||  0.994151 ||    16.756743 || 
    Epoch 32    --      0.728078 ||   0.8297 ||   0.9567 ||  0.995095 ||  0.995570 ||      0.807506 ||      0.8009 ||   0.9537 ||  0.994150 ||  0.994312 ||    16.851653 || 
    Epoch 33    --      0.687138 ||   0.8406 ||   0.9621 ||  0.995654 ||  0.996084 ||      0.772748 ||      0.8118 ||   0.9558 ||  0.994966 ||  0.994945 ||    17.060317 || 
    Epoch 34    --      0.649254 ||   0.8516 ||   0.9660 ||  0.995965 ||  0.996328 ||      0.725139 ||      0.8204 ||   0.9597 ||  0.995455 ||  0.995532 ||    17.016563 || 
    Epoch 35    --      0.615890 ||   0.8585 ||   0.9690 ||  0.996491 ||  0.996837 ||      0.706273 ||      0.8189 ||   0.9655 ||  0.996173 ||  0.995981 ||    16.995673 || 
    Epoch 36    --      0.610325 ||   0.8598 ||   0.9693 ||  0.996525 ||  0.996867 ||      0.697782 ||      0.8294 ||   0.9653 ||  0.996236 ||  0.996285 ||    16.760177 || 
    Epoch 37    --      0.573851 ||   0.8713 ||   0.9733 ||  0.996961 ||  0.997274 ||      0.682124 ||      0.8302 ||   0.9639 ||  0.996119 ||  0.996028 ||    16.962200 || 
    Epoch 38    --      0.577367 ||   0.8726 ||   0.9721 ||  0.996888 ||  0.997205 ||      0.674602 ||      0.8315 ||   0.9649 ||  0.996342 ||  0.996371 ||    17.103905 || 
    Epoch 39    --      0.547853 ||   0.8781 ||   0.9746 ||  0.997248 ||  0.997558 ||      0.599655 ||      0.8560 ||   0.9741 ||  0.996877 ||  0.996904 ||    16.931806 || 
    Epoch 40    --      0.508934 ||   0.8868 ||   0.9780 ||  0.997729 ||  0.997985 ||      0.591816 ||      0.8601 ||   0.9729 ||  0.996963 ||  0.997008 ||    16.969923 || 
    Epoch 41    --      0.481124 ||   0.8951 ||   0.9805 ||  0.997962 ||  0.998193 ||      0.575291 ||      0.8602 ||   0.9749 ||  0.997511 ||  0.997347 ||    16.770108 || 
    Epoch 42    --      0.485419 ||   0.8949 ||   0.9801 ||  0.997928 ||  0.998149 ||      0.562185 ||      0.8676 ||   0.9747 ||  0.997544 ||  0.997600 ||    16.831483 || 
    Epoch 43    --      0.451545 ||   0.9049 ||   0.9827 ||  0.998179 ||  0.998378 ||      0.523405 ||      0.8793 ||   0.9767 ||  0.997639 ||  0.997712 ||    16.725642 || 
    Epoch 44    --      0.428117 ||   0.9105 ||   0.9838 ||  0.998392 ||  0.998590 ||      0.473253 ||      0.8993 ||   0.9833 ||  0.998240 ||  0.998267 ||    17.097558 || 
    Epoch 45    --      0.407257 ||   0.9162 ||   0.9854 ||  0.998531 ||  0.998698 ||      0.462043 ||      0.8944 ||   0.9845 ||  0.998630 ||  0.998605 ||    17.184632 || 
    Epoch 46    --      0.406066 ||   0.9163 ||   0.9859 ||  0.998503 ||  0.998666 ||      0.448572 ||      0.8991 ||   0.9849 ||  0.998426 ||  0.998502 ||    16.933947 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
