Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.572164 ||   0.1647 ||   0.3656 ||  0.710334 ||  0.712519 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.777560 ||   0.1528 ||   0.3692 ||  0.744524 ||  0.751083 ||      3.613994 ||      0.1778 ||   0.4096 ||  0.837577 ||  0.842034 ||    15.934116 || 
    Epoch 01    --      3.388659 ||   0.2218 ||   0.4742 ||  0.860237 ||  0.867704 ||      3.393714 ||      0.2175 ||   0.4777 ||  0.865701 ||  0.868237 ||    14.898196 || 
    Epoch 02    --      3.204713 ||   0.2536 ||   0.5219 ||  0.879875 ||  0.886462 ||      3.241282 ||      0.2559 ||   0.5150 ||  0.877621 ||  0.881288 ||    14.689938 || 
    Epoch 03    --      3.060349 ||   0.2860 ||   0.5575 ||  0.892416 ||  0.898155 ||      3.102813 ||      0.2742 ||   0.5413 ||  0.892063 ||  0.895089 ||    12.940659 || 
    Epoch 04    --      2.931639 ||   0.3091 ||   0.5851 ||  0.903941 ||  0.909189 ||      2.985548 ||      0.3014 ||   0.5759 ||  0.901986 ||  0.903663 ||    13.825302 || 
    Epoch 05    --      2.810736 ||   0.3316 ||   0.6129 ||  0.913097 ||  0.917910 ||      2.842067 ||      0.3257 ||   0.6031 ||  0.913829 ||  0.915789 ||    14.001249 || 
    Epoch 06    --      2.686510 ||   0.3574 ||   0.6422 ||  0.922049 ||  0.926156 ||      2.770310 ||      0.3346 ||   0.6222 ||  0.919053 ||  0.921184 ||    13.603363 || 
    Epoch 07    --      2.579984 ||   0.3786 ||   0.6642 ||  0.929332 ||  0.933162 ||      2.624350 ||      0.3698 ||   0.6526 ||  0.928890 ||  0.930470 ||    14.223547 || 
    Epoch 08    --      2.464524 ||   0.4061 ||   0.6905 ||  0.936098 ||  0.939620 ||      2.549681 ||      0.3837 ||   0.6667 ||  0.934336 ||  0.935132 ||    14.454499 || 
    Epoch 09    --      2.355566 ||   0.4302 ||   0.7108 ||  0.942341 ||  0.945537 ||      2.423561 ||      0.4056 ||   0.6915 ||  0.942039 ||  0.942315 ||    14.079313 || 
    Epoch 10    --      2.241807 ||   0.4541 ||   0.7314 ||  0.948416 ||  0.951400 ||      2.310167 ||      0.4364 ||   0.7212 ||  0.946790 ||  0.947701 ||    13.345601 || 
    Epoch 11    --      2.139341 ||   0.4738 ||   0.7501 ||  0.953763 ||  0.956377 ||      2.263106 ||      0.4505 ||   0.7258 ||  0.949947 ||  0.949878 ||    13.192876 || 
    Epoch 12    --      2.042317 ||   0.4995 ||   0.7687 ||  0.957856 ||  0.960293 ||      2.143356 ||      0.4735 ||   0.7481 ||  0.955385 ||  0.955198 ||    14.103716 || 
    Epoch 13    --      1.952830 ||   0.5191 ||   0.7838 ||  0.961561 ||  0.963902 ||      2.064369 ||      0.4845 ||   0.7673 ||  0.960477 ||  0.959986 ||    14.411370 || 
    Epoch 14    --      1.850300 ||   0.5428 ||   0.8029 ||  0.965726 ||  0.967771 ||      1.967269 ||      0.5095 ||   0.7792 ||  0.962849 ||  0.963316 ||    13.741184 || 
    Epoch 15    --      1.770582 ||   0.5631 ||   0.8156 ||  0.968781 ||  0.970681 ||      1.834328 ||      0.5407 ||   0.8072 ||  0.968496 ||  0.969071 ||    14.117235 || 
    Epoch 16    --      1.682754 ||   0.5842 ||   0.8299 ||  0.972157 ||  0.973917 ||      1.776462 ||      0.5533 ||   0.8133 ||  0.970378 ||  0.970869 ||    14.885628 || 
    Epoch 17    --      1.606153 ||   0.6023 ||   0.8426 ||  0.974824 ||  0.976456 ||      1.676661 ||      0.5791 ||   0.8304 ||  0.973610 ||  0.973960 ||    14.852783 || 
    Epoch 18    --      1.531593 ||   0.6231 ||   0.8540 ||  0.977243 ||  0.978739 ||      1.611864 ||      0.5983 ||   0.8376 ||  0.976303 ||  0.976461 ||    12.891966 || 
    Epoch 19    --      1.446837 ||   0.6411 ||   0.8650 ||  0.979593 ||  0.980945 ||      1.573770 ||      0.6055 ||   0.8458 ||  0.977070 ||  0.977265 ||    13.826570 || 
    Epoch 20    --      1.375274 ||   0.6588 ||   0.8759 ||  0.981728 ||  0.983077 ||      1.498741 ||      0.6185 ||   0.8583 ||  0.979189 ||  0.979141 ||    13.636946 || 
    Epoch 21    --      1.316769 ||   0.6748 ||   0.8845 ||  0.983042 ||  0.984222 ||      1.397883 ||      0.6362 ||   0.8777 ||  0.982788 ||  0.982771 ||    13.894981 || 
    Epoch 22    --      1.245483 ||   0.6936 ||   0.8942 ||  0.984895 ||  0.985928 ||      1.348388 ||      0.6571 ||   0.8785 ||  0.984039 ||  0.984241 ||    13.834454 || 
    Epoch 23    --      1.182715 ||   0.7087 ||   0.9044 ||  0.986464 ||  0.987424 ||      1.279699 ||      0.6745 ||   0.8846 ||  0.984988 ||  0.985376 ||    13.872325 || 
    Epoch 24    --      1.128474 ||   0.7229 ||   0.9103 ||  0.987913 ||  0.988833 ||      1.209663 ||      0.6873 ||   0.9032 ||  0.985632 ||  0.986282 ||    14.220035 || 
    Epoch 25    --      1.067674 ||   0.7388 ||   0.9179 ||  0.988864 ||  0.989720 ||      1.166517 ||      0.7018 ||   0.9080 ||  0.988049 ||  0.988269 ||    12.715228 || 
    Epoch 26    --      1.014735 ||   0.7515 ||   0.9245 ||  0.990328 ||  0.991097 ||      1.113259 ||      0.7201 ||   0.9140 ||  0.989585 ||  0.989496 ||    13.838897 || 
    Epoch 27    --      0.956255 ||   0.7687 ||   0.9328 ||  0.991256 ||  0.991962 ||      1.078967 ||      0.7236 ||   0.9204 ||  0.989875 ||  0.990060 ||    13.192865 || 
    Epoch 28    --      0.911551 ||   0.7794 ||   0.9365 ||  0.992026 ||  0.992687 ||      1.026888 ||      0.7366 ||   0.9257 ||  0.991569 ||  0.991713 ||    14.220052 || 
    Epoch 29    --      0.861966 ||   0.7924 ||   0.9436 ||  0.993129 ||  0.993730 ||      0.957723 ||      0.7651 ||   0.9304 ||  0.991699 ||  0.991944 ||    14.011795 || 
    Epoch 30    --      0.820727 ||   0.8038 ||   0.9488 ||  0.993702 ||  0.994254 ||      0.907087 ||      0.7726 ||   0.9405 ||  0.993041 ||  0.993162 ||    14.264299 || 
    Epoch 31    --      0.777678 ||   0.8170 ||   0.9524 ||  0.994348 ||  0.994838 ||      0.840221 ||      0.7964 ||   0.9497 ||  0.993730 ||  0.994100 ||    13.321604 || 
    Epoch 32    --      0.730556 ||   0.8308 ||   0.9569 ||  0.995069 ||  0.995538 ||      0.814837 ||      0.7979 ||   0.9511 ||  0.994276 ||  0.994466 ||    13.871956 || 
    Epoch 33    --      0.690719 ||   0.8394 ||   0.9618 ||  0.995595 ||  0.996023 ||      0.781546 ||      0.8065 ||   0.9560 ||  0.994844 ||  0.994780 ||    14.036327 || 
    Epoch 34    --      0.652830 ||   0.8504 ||   0.9656 ||  0.996024 ||  0.996415 ||      0.715829 ||      0.8263 ||   0.9609 ||  0.995960 ||  0.996059 ||    14.650573 || 
    Epoch 35    --      0.617668 ||   0.8594 ||   0.9679 ||  0.996532 ||  0.996879 ||      0.701036 ||      0.8276 ||   0.9653 ||  0.995972 ||  0.995923 ||    14.922917 || 
    Epoch 36    --      0.583533 ||   0.8685 ||   0.9709 ||  0.996917 ||  0.997239 ||      0.679098 ||      0.8291 ||   0.9684 ||  0.996537 ||  0.996444 ||    13.522152 || 
    Epoch 37    --      0.545369 ||   0.8781 ||   0.9754 ||  0.997258 ||  0.997548 ||      0.651020 ||      0.8408 ||   0.9666 ||  0.996487 ||  0.996453 ||    14.690948 || 
    Epoch 38    --      0.515198 ||   0.8862 ||   0.9770 ||  0.997490 ||  0.997746 ||      0.582284 ||      0.8655 ||   0.9733 ||  0.997025 ||  0.997216 ||    15.140063 || 
    Epoch 39    --      0.492817 ||   0.8936 ||   0.9790 ||  0.997691 ||  0.997939 ||      0.550661 ||      0.8766 ||   0.9768 ||  0.997862 ||  0.997842 ||    14.422217 || 
    Epoch 40    --      0.454627 ||   0.9044 ||   0.9818 ||  0.998141 ||  0.998341 ||      0.525962 ||      0.8831 ||   0.9803 ||  0.997890 ||  0.997928 ||    13.716307 || 
    Epoch 41    --      0.430915 ||   0.9093 ||   0.9847 ||  0.998483 ||  0.998655 ||      0.499658 ||      0.8879 ||   0.9816 ||  0.997838 ||  0.997856 ||    13.984878 || 
    Epoch 42    --      0.406848 ||   0.9157 ||   0.9865 ||  0.998580 ||  0.998738 ||      0.475115 ||      0.8927 ||   0.9811 ||  0.998355 ||  0.998425 ||    14.057354 || 
    Epoch 43    --      0.384552 ||   0.9214 ||   0.9870 ||  0.998693 ||  0.998843 ||      0.447671 ||      0.8998 ||   0.9844 ||  0.998543 ||  0.998529 ||    14.321048 || 
    Epoch 44    --      0.359860 ||   0.9298 ||   0.9887 ||  0.998949 ||  0.999073 ||      0.423029 ||      0.9068 ||   0.9866 ||  0.998533 ||  0.998546 ||    13.224673 || 
    Epoch 45    --      0.344646 ||   0.9316 ||   0.9903 ||  0.999051 ||  0.999165 ||      0.387995 ||      0.9185 ||   0.9905 ||  0.998885 ||  0.998883 ||    13.823011 || 
    Epoch 46    --      0.321101 ||   0.9393 ||   0.9917 ||  0.999157 ||  0.999260 ||      0.358219 ||      0.9275 ||   0.9898 ||  0.999052 ||  0.999088 ||    14.416414 || 
    Epoch 47    --      0.300334 ||   0.9459 ||   0.9925 ||  0.999289 ||  0.999377 ||      0.354431 ||      0.9271 ||   0.9919 ||  0.999027 ||  0.998976 ||    14.317441 || 
    Epoch 48    --      0.299479 ||   0.9448 ||   0.9924 ||  0.999265 ||  0.999353 ||      0.352302 ||      0.9268 ||   0.9901 ||  0.999171 ||  0.999167 ||    13.552051 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
