Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.389019 ||   0.1598 ||   0.3689 ||  0.713095 ||  0.715488 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.792001 ||   0.1520 ||   0.3658 ||  0.744048 ||  0.750485 ||      3.627034 ||      0.1762 ||   0.4076 ||  0.836048 ||  0.840749 ||    18.857875 || 
    Epoch 01    --      3.390707 ||   0.2208 ||   0.4757 ||  0.860147 ||  0.867624 ||      3.394426 ||      0.2186 ||   0.4749 ||  0.864963 ||  0.867742 ||    17.066179 || 
    Epoch 02    --      3.201042 ||   0.2562 ||   0.5221 ||  0.880421 ||  0.886881 ||      3.233923 ||      0.2566 ||   0.5179 ||  0.877773 ||  0.881730 ||    17.359282 || 
    Epoch 03    --      3.053760 ||   0.2878 ||   0.5597 ||  0.893117 ||  0.898822 ||      3.105556 ||      0.2717 ||   0.5421 ||  0.892261 ||  0.895208 ||    16.864847 || 
    Epoch 04    --      2.926834 ||   0.3106 ||   0.5871 ||  0.904420 ||  0.909633 ||      2.976374 ||      0.3088 ||   0.5795 ||  0.903166 ||  0.904621 ||    16.970701 || 
    Epoch 05    --      2.804766 ||   0.3361 ||   0.6141 ||  0.913469 ||  0.918160 ||      2.833830 ||      0.3218 ||   0.6052 ||  0.915116 ||  0.917154 ||    17.099234 || 
    Epoch 06    --      2.678419 ||   0.3621 ||   0.6442 ||  0.922706 ||  0.926798 ||      2.761306 ||      0.3391 ||   0.6231 ||  0.919517 ||  0.921640 ||    17.193021 || 
    Epoch 07    --      2.570552 ||   0.3841 ||   0.6659 ||  0.930072 ||  0.933732 ||      2.618149 ||      0.3687 ||   0.6518 ||  0.929695 ||  0.930942 ||    16.939748 || 
    Epoch 08    --      2.457220 ||   0.4052 ||   0.6886 ||  0.937070 ||  0.940512 ||      2.526568 ||      0.3911 ||   0.6728 ||  0.934756 ||  0.935747 ||    17.144690 || 
    Epoch 09    --      2.344628 ||   0.4322 ||   0.7112 ||  0.943229 ||  0.946277 ||      2.409799 ||      0.4064 ||   0.6966 ||  0.943313 ||  0.943933 ||    17.151791 || 
    Epoch 10    --      2.230936 ||   0.4559 ||   0.7329 ||  0.949440 ||  0.952255 ||      2.308613 ||      0.4339 ||   0.7213 ||  0.947328 ||  0.948489 ||    17.129712 || 
    Epoch 11    --      2.129861 ||   0.4759 ||   0.7524 ||  0.954248 ||  0.956784 ||      2.239957 ||      0.4528 ||   0.7293 ||  0.951173 ||  0.950682 ||    16.742248 || 
    Epoch 12    --      2.030010 ||   0.5014 ||   0.7707 ||  0.958777 ||  0.961115 ||      2.134944 ||      0.4744 ||   0.7480 ||  0.956473 ||  0.956315 ||    17.168706 || 
    Epoch 13    --      1.939786 ||   0.5221 ||   0.7872 ||  0.962397 ||  0.964648 ||      2.062487 ||      0.4825 ||   0.7674 ||  0.961117 ||  0.960311 ||    17.083785 || 
    Epoch 14    --      1.837109 ||   0.5469 ||   0.8043 ||  0.966660 ||  0.968528 ||      1.957561 ||      0.5092 ||   0.7821 ||  0.963712 ||  0.964066 ||    17.090665 || 
    Epoch 15    --      1.754398 ||   0.5661 ||   0.8170 ||  0.969850 ||  0.971583 ||      1.824872 ||      0.5420 ||   0.8083 ||  0.968443 ||  0.969151 ||    16.880450 || 
    Epoch 16    --      1.670322 ||   0.5859 ||   0.8317 ||  0.972920 ||  0.974561 ||      1.747201 ||      0.5590 ||   0.8238 ||  0.971728 ||  0.971963 ||    16.812919 || 
    Epoch 17    --      1.588264 ||   0.6044 ||   0.8438 ||  0.975585 ||  0.977140 ||      1.668076 ||      0.5790 ||   0.8313 ||  0.974669 ||  0.974776 ||    17.225918 || 
    Epoch 18    --      1.510918 ||   0.6244 ||   0.8566 ||  0.978014 ||  0.979418 ||      1.608951 ||      0.5884 ||   0.8409 ||  0.976497 ||  0.976473 ||    16.450860 || 
    Epoch 19    --      1.427823 ||   0.6454 ||   0.8699 ||  0.980545 ||  0.981815 ||      1.564622 ||      0.6078 ||   0.8503 ||  0.977750 ||  0.977655 ||    17.266241 || 
    Epoch 20    --      1.356858 ||   0.6628 ||   0.8808 ||  0.982443 ||  0.983683 ||      1.487333 ||      0.6209 ||   0.8620 ||  0.979280 ||  0.979265 ||    16.824505 || 
    Epoch 21    --      1.297630 ||   0.6798 ||   0.8861 ||  0.983695 ||  0.984809 ||      1.365660 ||      0.6499 ||   0.8804 ||  0.983004 ||  0.983026 ||    16.875423 || 
    Epoch 22    --      1.223209 ||   0.6972 ||   0.8984 ||  0.985709 ||  0.986714 ||      1.322023 ||      0.6645 ||   0.8821 ||  0.983600 ||  0.984021 ||    16.767152 || 
    Epoch 23    --      1.161981 ||   0.7139 ||   0.9064 ||  0.986828 ||  0.987749 ||      1.278245 ||      0.6739 ||   0.8898 ||  0.984957 ||  0.985072 ||    17.066363 || 
    Epoch 24    --      1.104480 ||   0.7277 ||   0.9144 ||  0.988511 ||  0.989374 ||      1.204178 ||      0.6954 ||   0.9031 ||  0.985945 ||  0.986224 ||    16.418898 || 
    Epoch 25    --      1.050992 ||   0.7425 ||   0.9206 ||  0.989426 ||  0.990189 ||      1.124234 ||      0.7156 ||   0.9126 ||  0.988741 ||  0.989021 ||    17.030722 || 
    Epoch 26    --      0.991798 ||   0.7576 ||   0.9287 ||  0.990603 ||  0.991299 ||      1.092551 ||      0.7237 ||   0.9179 ||  0.989913 ||  0.989960 ||    17.402318 || 
    Epoch 27    --      0.934170 ||   0.7724 ||   0.9355 ||  0.991884 ||  0.992544 ||      1.047421 ||      0.7378 ||   0.9236 ||  0.989857 ||  0.990114 ||    17.298628 || 
    Epoch 28    --      0.892198 ||   0.7846 ||   0.9388 ||  0.992444 ||  0.993081 ||      0.982674 ||      0.7522 ||   0.9316 ||  0.991886 ||  0.992100 ||    17.048761 || 
    Epoch 29    --      0.845258 ||   0.7983 ||   0.9442 ||  0.993350 ||  0.993918 ||      0.949136 ||      0.7572 ||   0.9378 ||  0.992060 ||  0.992148 ||    16.891293 || 
    Epoch 30    --      0.798493 ||   0.8112 ||   0.9514 ||  0.994012 ||  0.994527 ||      0.902566 ||      0.7711 ||   0.9384 ||  0.993040 ||  0.993099 ||    17.184679 || 
    Epoch 31    --      0.756873 ||   0.8221 ||   0.9544 ||  0.994805 ||  0.995260 ||      0.826599 ||      0.7974 ||   0.9507 ||  0.993781 ||  0.993954 ||    16.888336 || 
    Epoch 32    --      0.715320 ||   0.8326 ||   0.9584 ||  0.995202 ||  0.995634 ||      0.789643 ||      0.8050 ||   0.9525 ||  0.994639 ||  0.994664 ||    17.145631 || 
    Epoch 33    --      0.671539 ||   0.8440 ||   0.9633 ||  0.995953 ||  0.996367 ||      0.745890 ||      0.8197 ||   0.9578 ||  0.995131 ||  0.995149 ||    17.091116 || 
    Epoch 34    --      0.634328 ||   0.8555 ||   0.9669 ||  0.996326 ||  0.996684 ||      0.711453 ||      0.8243 ||   0.9603 ||  0.996029 ||  0.996048 ||    16.812705 || 
    Epoch 35    --      0.600923 ||   0.8635 ||   0.9708 ||  0.996688 ||  0.997021 ||      0.701428 ||      0.8213 ||   0.9629 ||  0.996622 ||  0.996441 ||    16.632101 || 
    Epoch 36    --      0.597518 ||   0.8645 ||   0.9699 ||  0.996732 ||  0.997052 ||      0.683160 ||      0.8338 ||   0.9649 ||  0.996492 ||  0.996452 ||    17.049647 || 
    Epoch 37    --      0.565725 ||   0.8727 ||   0.9735 ||  0.997081 ||  0.997368 ||      0.674617 ||      0.8369 ||   0.9630 ||  0.996285 ||  0.996191 ||    16.838703 || 
    Epoch 38    --      0.528751 ||   0.8828 ||   0.9750 ||  0.997580 ||  0.997833 ||      0.633493 ||      0.8463 ||   0.9676 ||  0.996821 ||  0.996816 ||    17.040359 || 
    Epoch 39    --      0.504658 ||   0.8914 ||   0.9772 ||  0.997692 ||  0.997942 ||      0.565828 ||      0.8654 ||   0.9776 ||  0.997758 ||  0.997802 ||    16.546463 || 
    Epoch 40    --      0.471965 ||   0.8997 ||   0.9806 ||  0.998061 ||  0.998277 ||      0.544798 ||      0.8719 ||   0.9776 ||  0.997790 ||  0.997768 ||    17.135713 || 
    Epoch 41    --      0.444055 ||   0.9050 ||   0.9830 ||  0.998393 ||  0.998582 ||      0.513710 ||      0.8829 ||   0.9783 ||  0.998061 ||  0.998053 ||    16.670705 || 
    Epoch 42    --      0.418052 ||   0.9130 ||   0.9853 ||  0.998521 ||  0.998688 ||      0.499760 ||      0.8845 ||   0.9805 ||  0.998260 ||  0.998292 ||    16.510993 || 
    Epoch 43    --      0.394006 ||   0.9198 ||   0.9871 ||  0.998634 ||  0.998778 ||      0.454855 ||      0.8982 ||   0.9823 ||  0.998522 ||  0.998548 ||    17.247822 || 
    Epoch 44    --      0.369313 ||   0.9263 ||   0.9882 ||  0.998890 ||  0.999026 ||      0.439758 ||      0.9014 ||   0.9848 ||  0.998731 ||  0.998708 ||    17.134553 || 
    Epoch 45    --      0.351341 ||   0.9319 ||   0.9894 ||  0.999034 ||  0.999152 ||      0.388437 ||      0.9224 ||   0.9884 ||  0.998870 ||  0.998884 ||    16.726893 || 
    Epoch 46    --      0.328966 ||   0.9376 ||   0.9903 ||  0.999090 ||  0.999195 ||      0.376985 ||      0.9226 ||   0.9894 ||  0.999127 ||  0.999119 ||    17.099290 || 
    Epoch 47    --      0.327714 ||   0.9383 ||   0.9908 ||  0.999129 ||  0.999238 ||      0.403138 ||      0.9086 ||   0.9876 ||  0.998894 ||  0.998865 ||    17.023205 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
