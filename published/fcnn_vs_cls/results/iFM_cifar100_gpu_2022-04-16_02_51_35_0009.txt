Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      5.213565 ||   0.1771 ||   0.3926 ||  0.770691 ||  0.775239 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.790054 ||   0.1530 ||   0.3668 ||  0.745682 ||  0.752296 ||      3.615988 ||      0.1771 ||   0.4130 ||  0.837659 ||  0.841960 ||    15.836195 || 
    Epoch 01    --      3.391059 ||   0.2224 ||   0.4757 ||  0.859789 ||  0.867326 ||      3.395988 ||      0.2192 ||   0.4709 ||  0.865152 ||  0.867601 ||    14.925366 || 
    Epoch 02    --      3.204011 ||   0.2531 ||   0.5225 ||  0.879772 ||  0.886309 ||      3.238943 ||      0.2536 ||   0.5167 ||  0.878487 ||  0.882029 ||    13.907446 || 
    Epoch 03    --      3.059392 ||   0.2848 ||   0.5605 ||  0.892415 ||  0.898272 ||      3.105440 ||      0.2725 ||   0.5405 ||  0.891902 ||  0.895034 ||    13.725138 || 
    Epoch 04    --      2.933111 ||   0.3085 ||   0.5853 ||  0.903644 ||  0.908998 ||      2.982952 ||      0.3022 ||   0.5781 ||  0.902393 ||  0.903572 ||    14.290214 || 
    Epoch 05    --      2.812932 ||   0.3327 ||   0.6132 ||  0.912546 ||  0.917453 ||      2.847409 ||      0.3227 ||   0.5987 ||  0.913370 ||  0.915292 ||    13.274357 || 
    Epoch 06    --      2.690202 ||   0.3575 ||   0.6414 ||  0.921558 ||  0.925763 ||      2.770020 ||      0.3388 ||   0.6239 ||  0.918011 ||  0.920268 ||    12.897227 || 
    Epoch 07    --      2.585219 ||   0.3802 ||   0.6615 ||  0.928668 ||  0.932444 ||      2.637320 ||      0.3645 ||   0.6484 ||  0.927561 ||  0.928949 ||    14.025245 || 
    Epoch 08    --      2.471809 ||   0.4040 ||   0.6855 ||  0.935687 ||  0.939243 ||      2.543527 ||      0.3894 ||   0.6687 ||  0.932844 ||  0.934131 ||    14.594781 || 
    Epoch 09    --      2.361733 ||   0.4272 ||   0.7088 ||  0.941514 ||  0.944778 ||      2.430276 ||      0.4015 ||   0.6928 ||  0.941039 ||  0.941472 ||    13.960752 || 
    Epoch 10    --      2.249584 ||   0.4531 ||   0.7296 ||  0.947761 ||  0.950746 ||      2.313774 ||      0.4385 ||   0.7194 ||  0.946102 ||  0.947483 ||    14.356685 || 
    Epoch 11    --      2.145367 ||   0.4756 ||   0.7492 ||  0.952987 ||  0.955633 ||      2.261658 ||      0.4471 ||   0.7285 ||  0.949841 ||  0.949869 ||    13.988814 || 
    Epoch 12    --      2.048818 ||   0.4974 ||   0.7672 ||  0.957495 ||  0.959910 ||      2.138787 ||      0.4754 ||   0.7469 ||  0.955069 ||  0.955295 ||    15.035764 || 
    Epoch 13    --      1.955295 ||   0.5227 ||   0.7824 ||  0.961174 ||  0.963456 ||      2.071206 ||      0.4840 ||   0.7657 ||  0.960231 ||  0.959692 ||    13.890931 || 
    Epoch 14    --      1.853070 ||   0.5479 ||   0.8006 ||  0.965309 ||  0.967322 ||      1.965044 ||      0.5106 ||   0.7862 ||  0.963634 ||  0.963608 ||    14.165222 || 
    Epoch 15    --      1.767994 ||   0.5652 ||   0.8153 ||  0.968950 ||  0.970761 ||      1.847113 ||      0.5392 ||   0.8039 ||  0.967920 ||  0.968836 ||    14.448394 || 
    Epoch 16    --      1.678335 ||   0.5885 ||   0.8288 ||  0.972068 ||  0.973804 ||      1.781248 ||      0.5497 ||   0.8149 ||  0.970394 ||  0.970620 ||    14.446623 || 
    Epoch 17    --      1.603523 ||   0.6036 ||   0.8414 ||  0.974802 ||  0.976368 ||      1.685704 ||      0.5745 ||   0.8262 ||  0.974174 ||  0.974315 ||    13.355915 || 
    Epoch 18    --      1.521733 ||   0.6240 ||   0.8555 ||  0.977772 ||  0.979203 ||      1.628862 ||      0.5894 ||   0.8380 ||  0.975154 ||  0.975361 ||    13.520729 || 
    Epoch 19    --      1.437889 ||   0.6438 ||   0.8674 ||  0.979929 ||  0.981292 ||      1.575005 ||      0.6054 ||   0.8479 ||  0.976348 ||  0.976279 ||    14.935540 || 
    Epoch 20    --      1.368200 ||   0.6626 ||   0.8775 ||  0.981706 ||  0.982986 ||      1.489953 ||      0.6218 ||   0.8617 ||  0.979364 ||  0.979278 ||    14.382716 || 
    Epoch 21    --      1.303458 ||   0.6793 ||   0.8867 ||  0.983192 ||  0.984334 ||      1.394095 ||      0.6402 ||   0.8743 ||  0.982819 ||  0.982816 ||    14.589739 || 
    Epoch 22    --      1.232374 ||   0.6958 ||   0.8962 ||  0.985456 ||  0.986510 ||      1.335576 ||      0.6630 ||   0.8802 ||  0.983213 ||  0.983380 ||    14.130717 || 
    Epoch 23    --      1.177182 ||   0.7117 ||   0.9039 ||  0.986319 ||  0.987269 ||      1.248307 ||      0.6859 ||   0.8931 ||  0.985776 ||  0.986038 ||    14.221678 || 
    Epoch 24    --      1.116896 ||   0.7275 ||   0.9097 ||  0.987979 ||  0.988876 ||      1.189512 ||      0.6997 ||   0.9057 ||  0.987175 ||  0.987539 ||    14.113021 || 
    Epoch 25    --      1.056770 ||   0.7418 ||   0.9195 ||  0.989384 ||  0.990196 ||      1.145927 ||      0.7083 ||   0.9079 ||  0.988224 ||  0.988445 ||    14.242941 || 
    Epoch 26    --      1.000965 ||   0.7565 ||   0.9262 ||  0.990525 ||  0.991261 ||      1.097334 ||      0.7217 ||   0.9122 ||  0.989581 ||  0.989757 ||    14.301499 || 
    Epoch 27    --      0.943808 ||   0.7722 ||   0.9331 ||  0.991421 ||  0.992112 ||      1.077198 ||      0.7227 ||   0.9217 ||  0.989448 ||  0.989594 ||    14.376492 || 
    Epoch 28    --      0.945890 ||   0.7703 ||   0.9327 ||  0.991510 ||  0.992225 ||      1.082060 ||      0.7211 ||   0.9204 ||  0.989892 ||  0.989874 ||    13.862772 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
