Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      8.118927 ||   0.1624 ||   0.3671 ||  0.699236 ||  0.701259 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.784536 ||   0.1524 ||   0.3693 ||  0.742907 ||  0.749304 ||      3.618988 ||      0.1773 ||   0.4105 ||  0.837926 ||  0.842386 ||   100.608068 || 
    Epoch 01    --      3.386982 ||   0.2225 ||   0.4763 ||  0.860584 ||  0.868024 ||      3.392655 ||      0.2191 ||   0.4708 ||  0.865624 ||  0.868420 ||    99.173378 || 
    Epoch 02    --      3.203417 ||   0.2547 ||   0.5214 ||  0.880250 ||  0.886770 ||      3.236199 ||      0.2531 ||   0.5142 ||  0.877969 ||  0.882006 ||   100.592510 || 
    Epoch 03    --      3.060082 ||   0.2857 ||   0.5578 ||  0.892539 ||  0.898322 ||      3.105367 ||      0.2739 ||   0.5412 ||  0.892005 ||  0.895076 ||    99.389372 || 
    Epoch 04    --      2.932465 ||   0.3084 ||   0.5849 ||  0.903946 ||  0.909258 ||      2.986804 ||      0.3043 ||   0.5762 ||  0.901949 ||  0.903390 ||    99.967487 || 
    Epoch 05    --      2.815348 ||   0.3323 ||   0.6129 ||  0.912523 ||  0.917400 ||      2.844938 ||      0.3266 ||   0.6035 ||  0.913843 ||  0.915952 ||    99.817835 || 
    Epoch 06    --      2.692905 ||   0.3566 ||   0.6424 ||  0.921420 ||  0.925607 ||      2.772865 ||      0.3357 ||   0.6174 ||  0.918398 ||  0.920501 ||   100.264440 || 
    Epoch 07    --      2.590185 ||   0.3796 ||   0.6630 ||  0.928404 ||  0.932276 ||      2.628468 ||      0.3688 ||   0.6533 ||  0.928401 ||  0.929949 ||   100.436321 || 
    Epoch 08    --      2.477645 ||   0.4035 ||   0.6844 ||  0.935381 ||  0.938960 ||      2.556549 ||      0.3867 ||   0.6689 ||  0.933144 ||  0.934088 ||   100.512378 || 
    Epoch 09    --      2.370856 ||   0.4257 ||   0.7075 ||  0.941401 ||  0.944592 ||      2.445146 ||      0.4004 ||   0.6862 ||  0.941274 ||  0.941491 ||   100.670675 || 
    Epoch 10    --      2.261145 ||   0.4496 ||   0.7268 ||  0.947394 ||  0.950318 ||      2.325769 ||      0.4382 ||   0.7143 ||  0.945540 ||  0.946853 ||   101.123844 || 
    Epoch 11    --      2.159097 ||   0.4726 ||   0.7467 ||  0.952522 ||  0.955100 ||      2.268986 ||      0.4444 ||   0.7266 ||  0.949649 ||  0.949571 ||   100.719824 || 
    Epoch 12    --      2.063273 ||   0.4948 ||   0.7656 ||  0.956987 ||  0.959433 ||      2.156061 ||      0.4717 ||   0.7414 ||  0.955143 ||  0.954988 ||   100.389467 || 
    Epoch 13    --      1.972729 ||   0.5153 ||   0.7769 ||  0.961056 ||  0.963355 ||      2.087759 ||      0.4787 ||   0.7575 ||  0.959993 ||  0.959423 ||   100.536473 || 
    Epoch 14    --      1.870744 ||   0.5410 ||   0.7958 ||  0.965300 ||  0.967290 ||      1.983248 ||      0.5023 ||   0.7825 ||  0.961554 ||  0.961821 ||   100.655135 || 
    Epoch 15    --      1.788948 ||   0.5599 ||   0.8110 ||  0.968212 ||  0.970105 ||      1.858165 ||      0.5375 ||   0.8021 ||  0.967475 ||  0.968277 ||   100.655132 || 
    Epoch 16    --      1.697796 ||   0.5791 ||   0.8282 ||  0.971645 ||  0.973364 ||      1.801662 ||      0.5496 ||   0.8128 ||  0.968940 ||  0.969382 ||   101.295788 || 
    Epoch 17    --      1.624635 ||   0.5996 ||   0.8389 ||  0.974046 ||  0.975652 ||      1.694371 ||      0.5770 ||   0.8279 ||  0.973103 ||  0.973358 ||   100.801363 || 
    Epoch 18    --      1.546816 ||   0.6158 ||   0.8519 ||  0.976888 ||  0.978361 ||      1.636976 ||      0.5891 ||   0.8332 ||  0.975013 ||  0.975124 ||   101.014539 || 
    Epoch 19    --      1.462613 ||   0.6396 ||   0.8616 ||  0.979199 ||  0.980541 ||      1.575324 ||      0.6006 ||   0.8485 ||  0.976198 ||  0.976339 ||   102.238033 || 
    Epoch 20    --      1.388625 ||   0.6554 ||   0.8735 ||  0.981203 ||  0.982516 ||      1.512700 ||      0.6201 ||   0.8605 ||  0.978236 ||  0.978163 ||   102.702049 || 
    Epoch 21    --      1.325937 ||   0.6701 ||   0.8832 ||  0.982768 ||  0.983906 ||      1.423877 ||      0.6301 ||   0.8732 ||  0.982255 ||  0.982082 ||   103.440053 || 
    Epoch 22    --      1.255116 ||   0.6914 ||   0.8922 ||  0.984900 ||  0.985940 ||      1.351659 ||      0.6548 ||   0.8762 ||  0.983205 ||  0.983516 ||   104.509621 || 
    Epoch 23    --      1.190822 ||   0.7090 ||   0.9019 ||  0.986006 ||  0.986975 ||      1.310498 ||      0.6641 ||   0.8867 ||  0.984385 ||  0.984460 ||   102.686467 || 
    Epoch 24    --      1.137855 ||   0.7193 ||   0.9099 ||  0.987830 ||  0.988699 ||      1.224975 ||      0.6951 ||   0.8973 ||  0.985335 ||  0.985613 ||   103.920819 || 
    Epoch 25    --      1.078538 ||   0.7368 ||   0.9176 ||  0.988531 ||  0.989336 ||      1.159345 ||      0.7057 ||   0.9089 ||  0.988167 ||  0.988377 ||   103.952097 || 
    Epoch 26    --      1.019746 ||   0.7508 ||   0.9241 ||  0.990139 ||  0.990867 ||      1.120468 ||      0.7160 ||   0.9157 ||  0.988426 ||  0.988409 ||   103.670864 || 
    Epoch 27    --      0.964107 ||   0.7666 ||   0.9316 ||  0.991156 ||  0.991827 ||      1.079892 ||      0.7305 ||   0.9175 ||  0.989003 ||  0.989170 ||   103.998961 || 
    Epoch 28    --      0.921040 ||   0.7781 ||   0.9355 ||  0.991982 ||  0.992644 ||      1.008889 ||      0.7435 ||   0.9315 ||  0.990999 ||  0.991068 ||   104.473541 || 
    Epoch 29    --      0.872482 ||   0.7900 ||   0.9409 ||  0.992769 ||  0.993366 ||      0.972191 ||      0.7558 ||   0.9316 ||  0.991749 ||  0.991731 ||   103.999004 || 
    Epoch 30    --      0.827245 ||   0.8023 ||   0.9469 ||  0.993489 ||  0.994022 ||      0.899987 ||      0.7805 ||   0.9424 ||  0.992664 ||  0.992822 ||   103.999907 || 
    Epoch 31    --      0.779943 ||   0.8157 ||   0.9516 ||  0.994239 ||  0.994732 ||      0.832586 ||      0.7986 ||   0.9493 ||  0.993736 ||  0.993971 ||   105.889627 || 
    Epoch 32    --      0.734883 ||   0.8284 ||   0.9565 ||  0.995052 ||  0.995502 ||      0.818179 ||      0.7980 ||   0.9513 ||  0.993767 ||  0.993834 ||   104.568784 || 
    Epoch 33    --      0.736408 ||   0.8252 ||   0.9567 ||  0.994978 ||  0.995436 ||      0.810883 ||      0.8011 ||   0.9482 ||  0.994440 ||  0.994460 ||   103.983381 || 
    Epoch 34    --      0.696698 ||   0.8392 ||   0.9605 ||  0.995335 ||  0.995760 ||      0.763701 ||      0.8114 ||   0.9582 ||  0.995149 ||  0.995167 ||   103.749031 || 
    Epoch 35    --      0.658523 ||   0.8470 ||   0.9657 ||  0.996087 ||  0.996439 ||      0.738400 ||      0.8148 ||   0.9606 ||  0.995665 ||  0.995618 ||   103.967805 || 
    Epoch 36    --      0.618295 ||   0.8589 ||   0.9679 ||  0.996421 ||  0.996753 ||      0.713129 ||      0.8256 ||   0.9629 ||  0.995729 ||  0.995623 ||   104.103278 || 
    Epoch 37    --      0.586736 ||   0.8663 ||   0.9715 ||  0.996806 ||  0.997108 ||      0.688002 ||      0.8304 ||   0.9645 ||  0.995891 ||  0.995780 ||   104.499063 || 
    Epoch 38    --      0.546976 ||   0.8800 ||   0.9755 ||  0.997303 ||  0.997559 ||      0.650469 ||      0.8432 ||   0.9675 ||  0.996550 ||  0.996538 ||   104.202186 || 
    Epoch 39    --      0.525118 ||   0.8832 ||   0.9759 ||  0.997435 ||  0.997700 ||      0.588005 ||      0.8594 ||   0.9755 ||  0.997344 ||  0.997352 ||   104.311524 || 
    Epoch 40    --      0.487912 ||   0.8967 ||   0.9796 ||  0.997783 ||  0.998020 ||      0.568257 ||      0.8684 ||   0.9760 ||  0.997448 ||  0.997450 ||   108.118641 || 
    Epoch 41    --      0.459271 ||   0.9027 ||   0.9822 ||  0.998191 ||  0.998383 ||      0.547067 ||      0.8688 ||   0.9753 ||  0.997569 ||  0.997526 ||   103.217855 || 
    Epoch 42    --      0.463456 ||   0.9015 ||   0.9819 ||  0.998104 ||  0.998296 ||      0.539712 ||      0.8754 ||   0.9782 ||  0.997583 ||  0.997582 ||   100.905338 || 
    Epoch 43    --      0.434294 ||   0.9093 ||   0.9844 ||  0.998315 ||  0.998501 ||      0.500672 ||      0.8859 ||   0.9812 ||  0.998159 ||  0.998145 ||   101.123873 || 
    Epoch 44    --      0.410866 ||   0.9162 ||   0.9856 ||  0.998486 ||  0.998659 ||      0.472007 ||      0.8927 ||   0.9835 ||  0.998462 ||  0.998435 ||   100.952244 || 
    Epoch 45    --      0.387415 ||   0.9218 ||   0.9875 ||  0.998766 ||  0.998904 ||      0.429675 ||      0.9104 ||   0.9854 ||  0.998401 ||  0.998456 ||   101.389749 || 
    Epoch 46    --      0.364583 ||   0.9268 ||   0.9884 ||  0.998825 ||  0.998954 ||      0.403366 ||      0.9168 ||   0.9874 ||  0.998744 ||  0.998740 ||   101.327220 || 
    Epoch 47    --      0.341245 ||   0.9350 ||   0.9901 ||  0.999001 ||  0.999114 ||      0.422526 ||      0.9053 ||   0.9872 ||  0.998505 ||  0.998422 ||   101.080538 || 
    Epoch 48    --      0.341063 ||   0.9346 ||   0.9900 ||  0.998982 ||  0.999095 ||      0.395676 ||      0.9185 ||   0.9876 ||  0.998650 ||  0.998658 ||   100.827240 || 
    Epoch 49    --      0.319848 ||   0.9411 ||   0.9913 ||  0.999102 ||  0.999199 ||      0.371807 ||      0.9243 ||   0.9879 ||  0.998956 ||  0.998975 ||   101.076335 || 
    Epoch 50    --      0.303629 ||   0.9441 ||   0.9919 ||  0.999208 ||  0.999297 ||      0.341938 ||      0.9298 ||   0.9920 ||  0.999239 ||  0.999226 ||   101.077566 || 
    Epoch 51    --      0.281758 ||   0.9495 ||   0.9929 ||  0.999295 ||  0.999371 ||      0.329138 ||      0.9362 ||   0.9918 ||  0.999314 ||  0.999303 ||   101.381037 || 
    Epoch 52    --      0.268580 ||   0.9525 ||   0.9934 ||  0.999433 ||  0.999507 ||      0.318044 ||      0.9365 ||   0.9939 ||  0.999392 ||  0.999360 ||   100.889747 || 
    Epoch 53    --      0.268419 ||   0.9528 ||   0.9934 ||  0.999444 ||  0.999517 ||      0.308580 ||      0.9415 ||   0.9935 ||  0.999221 ||  0.999237 ||   100.933233 || 
    Epoch 54    --      0.251291 ||   0.9576 ||   0.9947 ||  0.999529 ||  0.999584 ||      0.301706 ||      0.9403 ||   0.9937 ||  0.999388 ||  0.999346 ||   101.341384 || 
    Epoch 55    --      0.250568 ||   0.9570 ||   0.9947 ||  0.999484 ||  0.999543 ||      0.287026 ||      0.9454 ||   0.9938 ||  0.999567 ||  0.999594 ||   101.061645 || 
    Epoch 56    --      0.235750 ||   0.9614 ||   0.9953 ||  0.999593 ||  0.999646 ||      0.278390 ||      0.9454 ||   0.9939 ||  0.999442 ||  0.999505 ||   100.858521 || 
    Epoch 57    --      0.239056 ||   0.9589 ||   0.9955 ||  0.999545 ||  0.999598 ||      0.262683 ||      0.9543 ||   0.9946 ||  0.999750 ||  0.999758 ||   101.249151 || 
    Epoch 58    --      0.220259 ||   0.9638 ||   0.9960 ||  0.999658 ||  0.999700 ||      0.269902 ||      0.9483 ||   0.9944 ||  0.999557 ||  0.999527 ||   101.030437 || 
    Epoch 59    --      0.222554 ||   0.9637 ||   0.9962 ||  0.999688 ||  0.999734 ||      0.251995 ||      0.9552 ||   0.9952 ||  0.999631 ||  0.999636 ||   100.999189 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
