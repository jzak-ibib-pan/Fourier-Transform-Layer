Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     41.220123 ||   0.1517 ||   0.3803 ||  0.594707 ||  0.594763 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.391399 ||   0.0952 ||   0.2530 ||  0.621889 ||  0.623281 ||     12.277250 ||      0.1350 ||   0.3351 ||  0.646972 ||  0.644197 ||    17.862463 || 
    Epoch 01    --     10.102531 ||   0.2127 ||   0.4602 ||  0.703521 ||  0.704272 ||     11.453564 ||      0.1787 ||   0.4242 ||  0.679151 ||  0.678782 ||    16.630424 || 
    Epoch 02    --      9.531532 ||   0.2599 ||   0.5302 ||  0.725941 ||  0.726515 ||     10.589344 ||      0.2405 ||   0.5126 ||  0.709815 ||  0.708872 ||    16.571188 || 
    Epoch 03    --      8.865920 ||   0.3078 ||   0.5913 ||  0.750194 ||  0.750735 ||     10.556138 ||      0.2743 ||   0.5429 ||  0.721454 ||  0.720596 ||    16.196059 || 
    Epoch 04    --      8.221010 ||   0.3572 ||   0.6493 ||  0.773059 ||  0.773572 ||      9.889650 ||      0.3136 ||   0.6060 ||  0.747375 ||  0.746700 ||    16.525756 || 
    Epoch 05    --      7.608161 ||   0.3966 ||   0.6953 ||  0.791564 ||  0.791944 ||      8.545447 ||      0.3651 ||   0.6666 ||  0.769969 ||  0.769674 ||    16.566032 || 
    Epoch 06    --      7.047845 ||   0.4388 ||   0.7314 ||  0.809667 ||  0.809906 ||      8.394417 ||      0.3891 ||   0.6990 ||  0.779178 ||  0.779870 ||    16.654581 || 
    Epoch 07    --      6.628637 ||   0.4744 ||   0.7611 ||  0.822189 ||  0.822472 ||      7.767428 ||      0.4239 ||   0.7273 ||  0.797467 ||  0.796189 ||    16.188146 || 
    Epoch 08    --      6.123474 ||   0.5052 ||   0.7939 ||  0.835947 ||  0.836143 ||      7.323149 ||      0.4613 ||   0.7558 ||  0.812162 ||  0.811783 ||    16.429872 || 
    Epoch 09    --      5.676017 ||   0.5352 ||   0.8210 ||  0.848459 ||  0.848697 ||      6.523193 ||      0.5042 ||   0.7933 ||  0.831862 ||  0.831048 ||    16.538468 || 
    Epoch 10    --      5.312830 ||   0.5582 ||   0.8399 ||  0.859531 ||  0.859817 ||      6.302969 ||      0.5182 ||   0.8188 ||  0.837889 ||  0.838507 ||    16.785636 || 
    Epoch 11    --      4.984992 ||   0.5860 ||   0.8585 ||  0.868144 ||  0.868419 ||      6.208842 ||      0.5274 ||   0.8278 ||  0.839800 ||  0.839354 ||    16.546070 || 
    Epoch 12    --      4.658801 ||   0.6073 ||   0.8742 ||  0.876656 ||  0.876875 ||      6.806069 ||      0.5206 ||   0.8249 ||  0.833458 ||  0.831536 ||    16.499400 || 
    Epoch 13    --      4.685039 ||   0.6035 ||   0.8736 ||  0.876286 ||  0.876485 ||      5.334154 ||      0.5730 ||   0.8584 ||  0.861746 ||  0.861474 ||    16.496635 || 
    Epoch 14    --      4.404534 ||   0.6286 ||   0.8873 ||  0.883769 ||  0.883924 ||      5.503521 ||      0.5758 ||   0.8584 ||  0.861734 ||  0.861398 ||    16.342565 || 
    Epoch 15    --      4.154951 ||   0.6496 ||   0.8995 ||  0.890906 ||  0.891065 ||      4.672338 ||      0.6157 ||   0.8846 ||  0.879540 ||  0.879263 ||    16.418714 || 
    Epoch 16    --      3.872545 ||   0.6662 ||   0.9098 ||  0.897534 ||  0.897697 ||      4.757808 ||      0.6236 ||   0.8885 ||  0.877868 ||  0.877837 ||    16.669448 || 
    Epoch 17    --      3.750488 ||   0.6817 ||   0.9169 ||  0.901738 ||  0.901971 ||      5.052667 ||      0.6238 ||   0.8857 ||  0.876354 ||  0.875757 ||    16.553700 || 
    Epoch 18    --      3.779701 ||   0.6783 ||   0.9179 ||  0.901543 ||  0.901777 ||      4.625190 ||      0.6339 ||   0.9074 ||  0.881982 ||  0.881733 ||    16.673093 || 
    Epoch 19    --      3.484841 ||   0.6964 ||   0.9289 ||  0.907564 ||  0.907732 ||      4.396477 ||      0.6512 ||   0.9074 ||  0.887501 ||  0.887228 ||    16.566154 || 
    Epoch 20    --      3.289413 ||   0.7108 ||   0.9350 ||  0.913180 ||  0.913281 ||      4.220708 ||      0.6626 ||   0.9202 ||  0.892963 ||  0.892583 ||    16.794021 || 
    Epoch 21    --      3.201955 ||   0.7209 ||   0.9398 ||  0.916587 ||  0.916709 ||      3.889700 ||      0.6791 ||   0.9254 ||  0.899870 ||  0.899623 ||    16.395879 || 
    Epoch 22    --      3.003261 ||   0.7334 ||   0.9464 ||  0.920730 ||  0.920836 ||      3.722295 ||      0.7004 ||   0.9296 ||  0.907162 ||  0.906967 ||    16.417146 || 
    Epoch 23    --      2.862490 ||   0.7480 ||   0.9513 ||  0.924649 ||  0.924778 ||      3.172408 ||      0.7253 ||   0.9472 ||  0.917498 ||  0.917682 ||    16.365861 || 
    Epoch 24    --      2.733627 ||   0.7590 ||   0.9564 ||  0.929067 ||  0.929101 ||      3.167126 ||      0.7318 ||   0.9510 ||  0.919234 ||  0.919019 ||    16.497501 || 
    Epoch 25    --      2.608131 ||   0.7664 ||   0.9612 ||  0.930857 ||  0.930973 ||      3.111195 ||      0.7310 ||   0.9504 ||  0.919283 ||  0.918456 ||    16.272117 || 
    Epoch 26    --      2.589825 ||   0.7661 ||   0.9608 ||  0.932624 ||  0.932806 ||      3.409715 ||      0.7272 ||   0.9500 ||  0.915073 ||  0.913919 ||    16.646204 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
