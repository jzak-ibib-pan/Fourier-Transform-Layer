Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.566904 ||   0.1645 ||   0.3658 ||  0.707944 ||  0.710179 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.779966 ||   0.1504 ||   0.3656 ||  0.745240 ||  0.751597 ||      3.614159 ||      0.1759 ||   0.4119 ||  0.837180 ||  0.841614 ||    18.222072 || 
    Epoch 01    --      3.395581 ||   0.2198 ||   0.4724 ||  0.859283 ||  0.866882 ||      3.408268 ||      0.2177 ||   0.4679 ||  0.863874 ||  0.866618 ||    17.154306 || 
    Epoch 02    --      3.209924 ||   0.2521 ||   0.5193 ||  0.879333 ||  0.885929 ||      3.245360 ||      0.2558 ||   0.5135 ||  0.876868 ||  0.880804 ||    16.862075 || 
    Epoch 03    --      3.063348 ||   0.2836 ||   0.5578 ||  0.891988 ||  0.897823 ||      3.108967 ||      0.2716 ||   0.5429 ||  0.891546 ||  0.894402 ||    17.077799 || 
    Epoch 04    --      2.935267 ||   0.3083 ||   0.5855 ||  0.903354 ||  0.908713 ||      2.979817 ||      0.3036 ||   0.5790 ||  0.902457 ||  0.904107 ||    17.159732 || 
    Epoch 05    --      2.812914 ||   0.3306 ||   0.6138 ||  0.912869 ||  0.917708 ||      2.848074 ||      0.3223 ||   0.6036 ||  0.913073 ||  0.914963 ||    16.915722 || 
    Epoch 06    --      2.686570 ||   0.3568 ||   0.6423 ||  0.921790 ||  0.925904 ||      2.770967 ||      0.3400 ||   0.6225 ||  0.918277 ||  0.920232 ||    17.202528 || 
    Epoch 07    --      2.578720 ||   0.3822 ||   0.6624 ||  0.928913 ||  0.932748 ||      2.628154 ||      0.3650 ||   0.6507 ||  0.928525 ||  0.930217 ||    16.579226 || 
    Epoch 08    --      2.465338 ||   0.4022 ||   0.6889 ||  0.935904 ||  0.939478 ||      2.540629 ||      0.3862 ||   0.6685 ||  0.933448 ||  0.934755 ||    17.257710 || 
    Epoch 09    --      2.355227 ||   0.4309 ||   0.7094 ||  0.941955 ||  0.945234 ||      2.430286 ||      0.4074 ||   0.6912 ||  0.940980 ||  0.941373 ||    17.101382 || 
    Epoch 10    --      2.245635 ||   0.4506 ||   0.7305 ||  0.948167 ||  0.951129 ||      2.309915 ||      0.4374 ||   0.7184 ||  0.946488 ||  0.947749 ||    17.143004 || 
    Epoch 11    --      2.142110 ||   0.4770 ||   0.7490 ||  0.953216 ||  0.955819 ||      2.258287 ||      0.4427 ||   0.7252 ||  0.949808 ||  0.949948 ||    17.095260 || 
    Epoch 12    --      2.044436 ||   0.4973 ||   0.7666 ||  0.957829 ||  0.960362 ||      2.144635 ||      0.4721 ||   0.7465 ||  0.954513 ||  0.954563 ||    16.710932 || 
    Epoch 13    --      1.951787 ||   0.5212 ||   0.7828 ||  0.961358 ||  0.963791 ||      2.077819 ||      0.4811 ||   0.7670 ||  0.958873 ||  0.958380 ||    16.911601 || 
    Epoch 14    --      1.854211 ||   0.5444 ||   0.7996 ||  0.965396 ||  0.967405 ||      1.961714 ||      0.5107 ||   0.7856 ||  0.962418 ||  0.962710 ||    16.829940 || 
    Epoch 15    --      1.767139 ||   0.5626 ||   0.8152 ||  0.968801 ||  0.970710 ||      1.846544 ||      0.5408 ||   0.8055 ||  0.966704 ||  0.967489 ||    17.400022 || 
    Epoch 16    --      1.680722 ||   0.5843 ||   0.8294 ||  0.971769 ||  0.973513 ||      1.792676 ||      0.5489 ||   0.8134 ||  0.970009 ||  0.970432 ||    16.928671 || 
    Epoch 17    --      1.601914 ||   0.6037 ||   0.8425 ||  0.974594 ||  0.976242 ||      1.685800 ||      0.5780 ||   0.8268 ||  0.972924 ||  0.973306 ||    16.688550 || 
    Epoch 18    --      1.527810 ||   0.6209 ||   0.8541 ||  0.977162 ||  0.978629 ||      1.623192 ||      0.5882 ||   0.8371 ||  0.975205 ||  0.975260 ||    17.191671 || 
    Epoch 19    --      1.444109 ||   0.6416 ||   0.8656 ||  0.979337 ||  0.980693 ||      1.578689 ||      0.6024 ||   0.8482 ||  0.976541 ||  0.976306 ||    16.838097 || 
    Epoch 20    --      1.374594 ||   0.6576 ||   0.8769 ||  0.981614 ||  0.982928 ||      1.477226 ||      0.6318 ||   0.8608 ||  0.979129 ||  0.979458 ||    17.073213 || 
    Epoch 21    --      1.311725 ||   0.6780 ||   0.8845 ||  0.982939 ||  0.984127 ||      1.384034 ||      0.6457 ||   0.8778 ||  0.983468 ||  0.983313 ||    17.183192 || 
    Epoch 22    --      1.237860 ||   0.6935 ||   0.8970 ||  0.985166 ||  0.986198 ||      1.354920 ||      0.6551 ||   0.8779 ||  0.982693 ||  0.983080 ||    17.024785 || 
    Epoch 23    --      1.179563 ||   0.7119 ||   0.9037 ||  0.986145 ||  0.987130 ||      1.269088 ||      0.6791 ||   0.8920 ||  0.985338 ||  0.985537 ||    17.016301 || 
    Epoch 24    --      1.120444 ||   0.7219 ||   0.9119 ||  0.988184 ||  0.989097 ||      1.212132 ||      0.6973 ||   0.8990 ||  0.985296 ||  0.985874 ||    16.761583 || 
    Epoch 25    --      1.064721 ||   0.7392 ||   0.9190 ||  0.988931 ||  0.989763 ||      1.145498 ||      0.7066 ||   0.9079 ||  0.988610 ||  0.988927 ||    17.202827 || 
    Epoch 26    --      1.006074 ||   0.7538 ||   0.9256 ||  0.990410 ||  0.991175 ||      1.132272 ||      0.7161 ||   0.9108 ||  0.988630 ||  0.988491 ||    17.207277 || 
    Epoch 27    --      0.950170 ||   0.7686 ||   0.9329 ||  0.991379 ||  0.992081 ||      1.092643 ||      0.7229 ||   0.9161 ||  0.988925 ||  0.989086 ||    16.918528 || 
    Epoch 28    --      0.907205 ||   0.7821 ||   0.9364 ||  0.992114 ||  0.992790 ||      1.004527 ||      0.7447 ||   0.9292 ||  0.990881 ||  0.991121 ||    17.111330 || 
    Epoch 29    --      0.862925 ||   0.7927 ||   0.9424 ||  0.992807 ||  0.993458 ||      0.946879 ||      0.7636 ||   0.9363 ||  0.992158 ||  0.992155 ||    17.111058 || 
    Epoch 30    --      0.815867 ||   0.8035 ||   0.9495 ||  0.993567 ||  0.994137 ||      0.906908 ||      0.7711 ||   0.9381 ||  0.992922 ||  0.992997 ||    16.567964 || 
    Epoch 31    --      0.775895 ||   0.8150 ||   0.9526 ||  0.994280 ||  0.994786 ||      0.835358 ||      0.7964 ||   0.9496 ||  0.993816 ||  0.994162 ||    17.354988 || 
    Epoch 32    --      0.726983 ||   0.8298 ||   0.9577 ||  0.994964 ||  0.995425 ||      0.812040 ||      0.7977 ||   0.9500 ||  0.994380 ||  0.994567 ||    17.068127 || 
    Epoch 33    --      0.690034 ||   0.8381 ||   0.9624 ||  0.995612 ||  0.996057 ||      0.782706 ||      0.8101 ||   0.9533 ||  0.994467 ||  0.994493 ||    16.813739 || 
    Epoch 34    --      0.651418 ||   0.8508 ||   0.9652 ||  0.996063 ||  0.996447 ||      0.728694 ||      0.8245 ||   0.9579 ||  0.995563 ||  0.995623 ||    16.742380 || 
    Epoch 35    --      0.618241 ||   0.8602 ||   0.9682 ||  0.996371 ||  0.996724 ||      0.697943 ||      0.8274 ||   0.9639 ||  0.996226 ||  0.996175 ||    17.417345 || 
    Epoch 36    --      0.581123 ||   0.8686 ||   0.9717 ||  0.996809 ||  0.997130 ||      0.646181 ||      0.8431 ||   0.9675 ||  0.996821 ||  0.996843 ||    17.290712 || 
    Epoch 37    --      0.546149 ||   0.8787 ||   0.9747 ||  0.997382 ||  0.997675 ||      0.669254 ||      0.8356 ||   0.9663 ||  0.995994 ||  0.995961 ||    17.003403 || 
    Epoch 38    --      0.544457 ||   0.8777 ||   0.9758 ||  0.997421 ||  0.997709 ||      0.643001 ||      0.8463 ||   0.9673 ||  0.996095 ||  0.996162 ||    17.049357 || 
    Epoch 39    --      0.522158 ||   0.8856 ||   0.9765 ||  0.997467 ||  0.997750 ||      0.578972 ||      0.8645 ||   0.9727 ||  0.997091 ||  0.997203 ||    17.296781 || 
    Epoch 40    --      0.486021 ||   0.8949 ||   0.9793 ||  0.997756 ||  0.998010 ||      0.560781 ||      0.8687 ||   0.9772 ||  0.997431 ||  0.997503 ||    16.930020 || 
    Epoch 41    --      0.459592 ||   0.8997 ||   0.9817 ||  0.998159 ||  0.998356 ||      0.534550 ||      0.8780 ||   0.9770 ||  0.997218 ||  0.997204 ||    16.719675 || 
    Epoch 42    --      0.433311 ||   0.9093 ||   0.9836 ||  0.998215 ||  0.998410 ||      0.505910 ||      0.8848 ||   0.9798 ||  0.997947 ||  0.997974 ||    17.092088 || 
    Epoch 43    --      0.410142 ||   0.9151 ||   0.9853 ||  0.998425 ||  0.998596 ||      0.479622 ||      0.8908 ||   0.9807 ||  0.998193 ||  0.998269 ||    17.380057 || 
    Epoch 44    --      0.386261 ||   0.9219 ||   0.9869 ||  0.998666 ||  0.998831 ||      0.438253 ||      0.9036 ||   0.9854 ||  0.998565 ||  0.998556 ||    16.808072 || 
    Epoch 45    --      0.365138 ||   0.9276 ||   0.9887 ||  0.998792 ||  0.998928 ||      0.411778 ||      0.9120 ||   0.9877 ||  0.999070 ||  0.999071 ||    16.786052 || 
    Epoch 46    --      0.344960 ||   0.9317 ||   0.9894 ||  0.999024 ||  0.999144 ||      0.393004 ||      0.9130 ||   0.9897 ||  0.998796 ||  0.998795 ||    17.154005 || 
    Epoch 47    --      0.340227 ||   0.9335 ||   0.9900 ||  0.999085 ||  0.999207 ||      0.387050 ||      0.9166 ||   0.9875 ||  0.998771 ||  0.998784 ||    16.893615 || 
    Epoch 48    --      0.322821 ||   0.9385 ||   0.9911 ||  0.999152 ||  0.999262 ||      0.386935 ||      0.9168 ||   0.9885 ||  0.998846 ||  0.998832 ||    16.684408 || 
    Epoch 49    --      0.321392 ||   0.9379 ||   0.9914 ||  0.999104 ||  0.999208 ||      0.374942 ||      0.9222 ||   0.9882 ||  0.998895 ||  0.998914 ||    16.715146 || 
    Epoch 50    --      0.305084 ||   0.9426 ||   0.9922 ||  0.999174 ||  0.999266 ||      0.358834 ||      0.9208 ||   0.9918 ||  0.999207 ||  0.999176 ||    17.105588 || 
    Epoch 51    --      0.302004 ||   0.9434 ||   0.9922 ||  0.999287 ||  0.999372 ||      0.366787 ||      0.9213 ||   0.9888 ||  0.999080 ||  0.999052 ||    17.008458 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
