Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                       10
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -           ['conv2d', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.3
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      2.182289 ||   0.2608 ||   0.8400 ||  0.753657 ||  0.760819 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      2.119473 ||   0.2900 ||   0.7818 ||  0.746186 ||  0.753420 ||      2.131293 ||      0.1903 ||   0.7023 ||  0.697267 ||  0.680168 ||    29.874691 || 
    Epoch 01    --      2.067855 ||   0.2388 ||   0.7626 ||  0.708435 ||  0.718886 ||      2.067415 ||      0.2724 ||   0.7920 ||  0.747580 ||  0.735608 ||    29.046552 || 
    Epoch 02    --      2.011029 ||   0.2566 ||   0.7996 ||  0.732346 ||  0.741531 ||      2.031609 ||      0.2631 ||   0.8122 ||  0.746773 ||  0.744431 ||    29.452789 || 
    Epoch 03    --      1.990493 ||   0.2592 ||   0.8059 ||  0.739726 ||  0.747769 ||      1.937952 ||      0.2781 ||   0.8288 ||  0.758926 ||  0.765707 ||    29.421574 || 
    Epoch 04    --      1.980471 ||   0.2631 ||   0.8110 ||  0.743770 ||  0.751156 ||      1.928210 ||      0.2752 ||   0.8297 ||  0.761410 ||  0.767860 ||    30.296559 || 
    Epoch 05    --      1.980139 ||   0.2639 ||   0.8087 ||  0.743080 ||  0.750900 ||      1.959617 ||      0.2564 ||   0.8166 ||  0.749895 ||  0.754590 ||    29.437229 || 
    Epoch 06    --      1.966878 ||   0.2655 ||   0.8143 ||  0.746706 ||  0.754367 ||      1.970763 ||      0.2506 ||   0.8076 ||  0.746476 ||  0.748276 ||    29.687226 || 
    Epoch 07    --      1.967777 ||   0.2663 ||   0.8166 ||  0.747959 ||  0.755115 ||      2.069919 ||      0.2160 ||   0.7461 ||  0.728689 ||  0.706612 ||    29.546574 || 
    Epoch 08    --      1.965807 ||   0.2672 ||   0.8153 ||  0.748409 ||  0.755484 ||      1.969627 ||      0.2731 ||   0.8291 ||  0.763006 ||  0.762483 ||    29.577852 || 
    Epoch 09    --      1.966190 ||   0.2691 ||   0.8181 ||  0.748868 ||  0.755812 ||      1.956973 ||      0.2856 ||   0.8256 ||  0.770609 ||  0.767880 ||    29.780948 || 
    Epoch 10    --      1.955137 ||   0.2684 ||   0.8199 ||  0.750652 ||  0.758164 ||      1.987635 ||      0.2869 ||   0.8246 ||  0.763338 ||  0.762297 ||    29.593436 || 
    Epoch 11    --      1.960840 ||   0.2653 ||   0.8211 ||  0.749734 ||  0.757264 ||      2.102566 ||      0.1966 ||   0.7348 ||  0.719682 ||  0.692949 ||    29.405976 || 
    Epoch 12    --      1.951541 ||   0.2712 ||   0.8211 ||  0.753173 ||  0.759994 ||      2.012160 ||      0.2269 ||   0.7843 ||  0.748960 ||  0.730247 ||    29.650313 || 
    Epoch 13    --      1.950279 ||   0.2721 ||   0.8199 ||  0.753299 ||  0.760167 ||      1.894266 ||      0.2845 ||   0.8429 ||  0.768054 ||  0.775563 ||    29.734101 || 
    Epoch 14    --      1.945562 ||   0.2702 ||   0.8240 ||  0.754731 ||  0.761715 ||      1.916050 ||      0.2734 ||   0.8305 ||  0.765453 ||  0.767595 ||    29.593496 || 
    Epoch 15    --      1.946007 ||   0.2713 ||   0.8210 ||  0.755139 ||  0.761178 ||      1.923711 ||      0.2751 ||   0.8336 ||  0.761382 ||  0.765659 ||    29.468495 || 
    Epoch 16    --      1.944977 ||   0.2709 ||   0.8222 ||  0.755440 ||  0.762029 ||      1.893943 ||      0.2840 ||   0.8428 ||  0.767102 ||  0.774314 ||    29.624727 || 
    Epoch 17    --      1.939209 ||   0.2729 ||   0.8249 ||  0.757001 ||  0.764059 ||      1.902320 ||      0.2799 ||   0.8422 ||  0.765825 ||  0.774914 ||    29.765345 || 
    Epoch 18    --      1.934387 ||   0.2722 ||   0.8271 ||  0.758121 ||  0.765147 ||      1.974696 ||      0.2438 ||   0.8003 ||  0.750998 ||  0.744021 ||    29.624744 || 
    Epoch 19    --      1.936437 ||   0.2734 ||   0.8273 ||  0.757665 ||  0.764352 ||      1.920342 ||      0.2631 ||   0.8279 ||  0.761017 ||  0.762485 ||    29.452872 || 
    Epoch 20    --      1.931171 ||   0.2748 ||   0.8278 ||  0.758907 ||  0.765497 ||      1.899438 ||      0.2730 ||   0.8420 ||  0.765142 ||  0.771939 ||    29.687221 || 
    Epoch 21    --      1.934351 ||   0.2737 ||   0.8262 ||  0.757799 ||  0.763977 ||      2.109139 ||      0.1928 ||   0.7242 ||  0.724132 ||  0.692129 ||    29.812243 || 
    Epoch 22    --      1.928888 ||   0.2724 ||   0.8302 ||  0.760059 ||  0.766292 ||      1.925227 ||      0.2629 ||   0.8273 ||  0.760872 ||  0.763241 ||    29.577873 || 
    Epoch 23    --      1.924915 ||   0.2746 ||   0.8314 ||  0.761736 ||  0.767579 ||      2.042488 ||      0.2209 ||   0.7635 ||  0.737755 ||  0.717671 ||    29.421625 || 
    Epoch 24    --      1.923468 ||   0.2762 ||   0.8310 ||  0.761272 ||  0.768087 ||      1.937036 ||      0.2484 ||   0.8161 ||  0.758135 ||  0.755506 ||    30.296607 || 
    Epoch 25    --      1.917727 ||   0.2742 ||   0.8322 ||  0.762200 ||  0.769449 ||      1.906211 ||      0.2875 ||   0.8497 ||  0.777017 ||  0.779736 ||    31.952871 || 
    Epoch 26    --      1.924646 ||   0.2743 ||   0.8303 ||  0.760854 ||  0.767593 ||      1.877087 ||      0.2788 ||   0.8486 ||  0.773502 ||  0.780367 ||    29.656013 || 
    Epoch 27    --      1.918920 ||   0.2754 ||   0.8327 ||  0.761934 ||  0.768329 ||      1.878168 ||      0.2815 ||   0.8411 ||  0.769621 ||  0.775102 ||    29.405990 || 
    Epoch 28    --      1.921433 ||   0.2739 ||   0.8293 ||  0.761209 ||  0.767417 ||      1.899080 ||      0.2654 ||   0.8358 ||  0.767164 ||  0.770136 ||    29.534503 || 
    Epoch 29    --      1.916597 ||   0.2754 ||   0.8338 ||  0.761990 ||  0.768980 ||      1.971797 ||      0.2902 ||   0.8411 ||  0.773401 ||  0.770833 ||    30.015378 || 
    Epoch 30    --      1.911866 ||   0.2763 ||   0.8359 ||  0.764980 ||  0.771459 ||      1.895929 ||      0.2837 ||   0.8444 ||  0.771361 ||  0.776100 ||    29.593539 || 
    Epoch 31    --      1.906559 ||   0.2763 ||   0.8370 ||  0.765206 ||  0.771907 ||      1.884622 ||      0.2753 ||   0.8426 ||  0.769131 ||  0.774971 ||    29.421665 || 
    Epoch 32    --      1.907681 ||   0.2770 ||   0.8346 ||  0.764938 ||  0.771293 ||      1.902636 ||      0.2733 ||   0.8397 ||  0.767644 ||  0.770782 ||    29.593539 || 
    Epoch 33    --      1.904503 ||   0.2781 ||   0.8373 ||  0.765816 ||  0.772684 ||      1.897106 ||      0.2685 ||   0.8334 ||  0.768458 ||  0.769396 ||    29.687287 || 
    Epoch 34    --      1.899929 ||   0.2789 ||   0.8381 ||  0.767577 ||  0.774121 ||      1.945030 ||      0.2880 ||   0.8528 ||  0.773758 ||  0.778493 ||    29.655989 || 
    Epoch 35    --      1.901842 ||   0.2806 ||   0.8367 ||  0.766729 ||  0.772915 ||      1.850488 ||      0.2852 ||   0.8578 ||  0.782520 ||  0.787842 ||    29.468541 || 
    Epoch 36    --      1.898620 ||   0.2799 ||   0.8366 ||  0.767271 ||  0.774199 ||      1.861687 ||      0.2767 ||   0.8466 ||  0.777790 ||  0.780989 ||    29.609162 || 
    Epoch 37    --      1.897933 ||   0.2799 ||   0.8382 ||  0.768246 ||  0.774835 ||      1.969802 ||      0.2444 ||   0.7861 ||  0.751314 ||  0.740309 ||    29.671672 || 
    Epoch 38    --      1.895344 ||   0.2801 ||   0.8384 ||  0.767575 ||  0.774253 ||      1.903004 ||      0.2984 ||   0.8434 ||  0.779344 ||  0.780328 ||    29.531052 || 
    Epoch 39    --      1.892294 ||   0.2819 ||   0.8396 ||  0.769260 ||  0.775032 ||      1.867854 ||      0.2755 ||   0.8489 ||  0.775202 ||  0.778235 ||    29.406047 || 
    Epoch 40    --      1.892151 ||   0.2820 ||   0.8389 ||  0.769251 ||  0.775701 ||      1.856583 ||      0.2895 ||   0.8559 ||  0.781415 ||  0.788163 ||    29.593547 || 
    Epoch 41    --      1.896208 ||   0.2828 ||   0.8369 ||  0.767892 ||  0.774415 ||      1.843515 ||      0.2938 ||   0.8580 ||  0.781142 ||  0.789012 ||    29.624799 || 
    Epoch 42    --      1.888605 ||   0.2848 ||   0.8391 ||  0.770815 ||  0.776548 ||      1.868578 ||      0.2877 ||   0.8526 ||  0.780656 ||  0.786062 ||    29.327925 || 
    Epoch 43    --      1.887432 ||   0.2831 ||   0.8406 ||  0.770786 ||  0.776621 ||      1.861600 ||      0.3038 ||   0.8582 ||  0.782804 ||  0.789805 ||    29.327929 || 
    Epoch 44    --      1.890403 ||   0.2833 ||   0.8378 ||  0.769832 ||  0.776487 ||      1.842813 ||      0.2894 ||   0.8540 ||  0.781613 ||  0.788440 ||    30.031044 || 
    Epoch 45    --      1.888256 ||   0.2858 ||   0.8368 ||  0.769293 ||  0.776100 ||      1.898614 ||      0.2967 ||   0.8447 ||  0.779454 ||  0.779984 ||    29.343549 || 
Layers list:
	conv2d                                   -                    (17, 17, 3, 20)|(20,)
	conv2d-filters                           -                                       20
	conv2d-kernel_size                       -                                       17
	conv2d-strides                           -                                   (1, 1)
	conv2d-padding                           -                                    valid
	conv2d-data_format                       -                                     None
	conv2d-dilation_rate                     -                                   (1, 1)
	conv2d-groups                            -                                        1
	conv2d-activation                        -                                     relu
	conv2d-use_bias                          -                                     True
	conv2d-kernel_initializer                -                                he_normal
	conv2d-bias_initializer                  -                                    zeros
	conv2d-kernel_regularizer                -                                     None
	conv2d-bias_regularizer                  -                                     None
	conv2d-activity_regularizer              -                                     None
	conv2d-kernel_constraint                 -                                     None
	conv2d-bias_constraint                   -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                         (5120, 10)|(10,)
	dense_2-units                            -                                       10
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 16, 16, 20)        17360     
_________________________________________________________________
flatten_2 (Flatten)          (None, 5120)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                51210     
=================================================================
Total params: 68,570
Trainable params: 68,570
Non-trainable params: 0
_________________________________________________________________
