Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      5.997164 ||   0.1673 ||   0.3831 ||  0.745031 ||  0.748442 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.781202 ||   0.1524 ||   0.3682 ||  0.744685 ||  0.751075 ||      3.627074 ||      0.1768 ||   0.4091 ||  0.836372 ||  0.840902 ||    18.321669 || 
    Epoch 01    --      3.390152 ||   0.2212 ||   0.4753 ||  0.860089 ||  0.867461 ||      3.393487 ||      0.2202 ||   0.4754 ||  0.865086 ||  0.867927 ||    17.013714 || 
    Epoch 02    --      3.204018 ||   0.2542 ||   0.5215 ||  0.880057 ||  0.886470 ||      3.235997 ||      0.2588 ||   0.5179 ||  0.878056 ||  0.882035 ||    16.915107 || 
    Epoch 03    --      3.058135 ||   0.2874 ||   0.5588 ||  0.892528 ||  0.898223 ||      3.101446 ||      0.2727 ||   0.5456 ||  0.892143 ||  0.895278 ||    17.063758 || 
    Epoch 04    --      2.928709 ||   0.3072 ||   0.5886 ||  0.904068 ||  0.909371 ||      2.983245 ||      0.3052 ||   0.5752 ||  0.902350 ||  0.903882 ||    16.953366 || 
    Epoch 05    --      2.807670 ||   0.3332 ||   0.6160 ||  0.913106 ||  0.917928 ||      2.845487 ||      0.3262 ||   0.6060 ||  0.913655 ||  0.915888 ||    17.055939 || 
    Epoch 06    --      2.683921 ||   0.3602 ||   0.6438 ||  0.922233 ||  0.926286 ||      2.772744 ||      0.3391 ||   0.6188 ||  0.918310 ||  0.920530 ||    17.080483 || 
    Epoch 07    --      2.580937 ||   0.3824 ||   0.6630 ||  0.928841 ||  0.932662 ||      2.625659 ||      0.3716 ||   0.6535 ||  0.928660 ||  0.930116 ||    16.755950 || 
    Epoch 08    --      2.466906 ||   0.4042 ||   0.6867 ||  0.936077 ||  0.939529 ||      2.543919 ||      0.3887 ||   0.6715 ||  0.933017 ||  0.934410 ||    16.788958 || 
    Epoch 09    --      2.359128 ||   0.4282 ||   0.7089 ||  0.942026 ||  0.945208 ||      2.421648 ||      0.4047 ||   0.6935 ||  0.942185 ||  0.942448 ||    17.223425 || 
    Epoch 10    --      2.245003 ||   0.4550 ||   0.7293 ||  0.948137 ||  0.951119 ||      2.325768 ||      0.4360 ||   0.7153 ||  0.945875 ||  0.946817 ||    16.860235 || 
    Epoch 11    --      2.141815 ||   0.4769 ||   0.7476 ||  0.953304 ||  0.955897 ||      2.261121 ||      0.4497 ||   0.7278 ||  0.950227 ||  0.949995 ||    16.431863 || 
    Epoch 12    --      2.045307 ||   0.4996 ||   0.7668 ||  0.957784 ||  0.960209 ||      2.143164 ||      0.4704 ||   0.7451 ||  0.955353 ||  0.955387 ||    17.054562 || 
    Epoch 13    --      1.949214 ||   0.5207 ||   0.7821 ||  0.961970 ||  0.964226 ||      2.086762 ||      0.4762 ||   0.7622 ||  0.959549 ||  0.958839 ||    17.015931 || 
    Epoch 14    --      1.850176 ||   0.5455 ||   0.8000 ||  0.965874 ||  0.967823 ||      1.959190 ||      0.5100 ||   0.7823 ||  0.963521 ||  0.964181 ||    16.848767 || 
    Epoch 15    --      1.767626 ||   0.5652 ||   0.8131 ||  0.969042 ||  0.970857 ||      1.834022 ||      0.5439 ||   0.8004 ||  0.968277 ||  0.969077 ||    17.104491 || 
    Epoch 16    --      1.681026 ||   0.5841 ||   0.8292 ||  0.972254 ||  0.973927 ||      1.769515 ||      0.5566 ||   0.8144 ||  0.971016 ||  0.971330 ||    17.419228 || 
    Epoch 17    --      1.601533 ||   0.6021 ||   0.8411 ||  0.974788 ||  0.976378 ||      1.672595 ||      0.5820 ||   0.8254 ||  0.974145 ||  0.974442 ||    17.098608 || 
    Epoch 18    --      1.522435 ||   0.6237 ||   0.8532 ||  0.977670 ||  0.979087 ||      1.607897 ||      0.5906 ||   0.8371 ||  0.975824 ||  0.976051 ||    16.914627 || 
    Epoch 19    --      1.439919 ||   0.6421 ||   0.8652 ||  0.980132 ||  0.981451 ||      1.569752 ||      0.6029 ||   0.8483 ||  0.976723 ||  0.976576 ||    17.049654 || 
    Epoch 20    --      1.366326 ||   0.6614 ||   0.8770 ||  0.981970 ||  0.983248 ||      1.489745 ||      0.6287 ||   0.8587 ||  0.978924 ||  0.979052 ||    17.206504 || 
    Epoch 21    --      1.304150 ||   0.6782 ||   0.8857 ||  0.983333 ||  0.984443 ||      1.392412 ||      0.6418 ||   0.8756 ||  0.982747 ||  0.982691 ||    16.934258 || 
    Epoch 22    --      1.234511 ||   0.6938 ||   0.8960 ||  0.985381 ||  0.986408 ||      1.319080 ||      0.6664 ||   0.8842 ||  0.984047 ||  0.984402 ||    16.644897 || 
    Epoch 23    --      1.169696 ||   0.7132 ||   0.9052 ||  0.986654 ||  0.987610 ||      1.278771 ||      0.6756 ||   0.8880 ||  0.984858 ||  0.985088 ||    17.465572 || 
    Epoch 24    --      1.117310 ||   0.7254 ||   0.9122 ||  0.988160 ||  0.989024 ||      1.200110 ||      0.6956 ||   0.9027 ||  0.986359 ||  0.986858 ||    17.287592 || 
    Epoch 25    --      1.055322 ||   0.7412 ||   0.9215 ||  0.989211 ||  0.990027 ||      1.153967 ||      0.7057 ||   0.9073 ||  0.987498 ||  0.987787 ||    16.860912 || 
    Epoch 26    --      1.000829 ||   0.7562 ||   0.9257 ||  0.990446 ||  0.991196 ||      1.108506 ||      0.7157 ||   0.9142 ||  0.989165 ||  0.989000 ||    16.623881 || 
    Epoch 27    --      0.941583 ||   0.7716 ||   0.9338 ||  0.991458 ||  0.992158 ||      1.058574 ||      0.7310 ||   0.9173 ||  0.990007 ||  0.990291 ||    17.161337 || 
    Epoch 28    --      0.898668 ||   0.7828 ||   0.9393 ||  0.992293 ||  0.992940 ||      1.010917 ||      0.7416 ||   0.9286 ||  0.991339 ||  0.991381 ||    16.624413 || 
    Epoch 29    --      0.851028 ||   0.7961 ||   0.9449 ||  0.993167 ||  0.993795 ||      0.973889 ||      0.7497 ||   0.9333 ||  0.991349 ||  0.991196 ||    16.796389 || 
    Epoch 30    --      0.807415 ||   0.8085 ||   0.9497 ||  0.993719 ||  0.994255 ||      0.888715 ||      0.7800 ||   0.9401 ||  0.993308 ||  0.993436 ||    17.124822 || 
    Epoch 31    --      0.762966 ||   0.8196 ||   0.9529 ||  0.994640 ||  0.995110 ||      0.830965 ||      0.7969 ||   0.9473 ||  0.993941 ||  0.994266 ||    16.814640 || 
    Epoch 32    --      0.721020 ||   0.8318 ||   0.9575 ||  0.995167 ||  0.995643 ||      0.797769 ||      0.8031 ||   0.9545 ||  0.994271 ||  0.994376 ||    16.656195 || 
    Epoch 33    --      0.675341 ||   0.8441 ||   0.9633 ||  0.995788 ||  0.996222 ||      0.740924 ||      0.8254 ||   0.9554 ||  0.994892 ||  0.995056 ||    17.035556 || 
    Epoch 34    --      0.643419 ||   0.8525 ||   0.9663 ||  0.995917 ||  0.996307 ||      0.710686 ||      0.8249 ||   0.9614 ||  0.996198 ||  0.996246 ||    17.861872 || 
    Epoch 35    --      0.641616 ||   0.8523 ||   0.9661 ||  0.996070 ||  0.996456 ||      0.727659 ||      0.8207 ||   0.9607 ||  0.995614 ||  0.995575 ||    16.845699 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
