Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.112907 ||   0.1638 ||   0.3659 ||  0.718371 ||  0.720830 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.791271 ||   0.1515 ||   0.3671 ||  0.743172 ||  0.749557 ||      3.619780 ||      0.1760 ||   0.4101 ||  0.836992 ||  0.841740 ||    15.410696 || 
    Epoch 01    --      3.389954 ||   0.2215 ||   0.4752 ||  0.860395 ||  0.867827 ||      3.390124 ||      0.2202 ||   0.4748 ||  0.865368 ||  0.868310 ||    14.745471 || 
    Epoch 02    --      3.203749 ||   0.2541 ||   0.5207 ||  0.880293 ||  0.886814 ||      3.237141 ||      0.2541 ||   0.5167 ||  0.878629 ||  0.882484 ||    13.697213 || 
    Epoch 03    --      3.062720 ||   0.2832 ||   0.5560 ||  0.892505 ||  0.898265 ||      3.103722 ||      0.2707 ||   0.5413 ||  0.892111 ||  0.895366 ||    14.047532 || 
    Epoch 04    --      2.937385 ||   0.3067 ||   0.5839 ||  0.903576 ||  0.908981 ||      2.990918 ||      0.2988 ||   0.5724 ||  0.902253 ||  0.903665 ||    13.741055 || 
    Epoch 05    --      2.822316 ||   0.3318 ||   0.6113 ||  0.912231 ||  0.917123 ||      2.850275 ||      0.3207 ||   0.6001 ||  0.913078 ||  0.915593 ||    14.245277 || 
    Epoch 06    --      2.699599 ||   0.3532 ||   0.6399 ||  0.921539 ||  0.925777 ||      2.787392 ||      0.3326 ||   0.6165 ||  0.917896 ||  0.920067 ||    12.652908 || 
    Epoch 07    --      2.596518 ||   0.3745 ||   0.6613 ||  0.928183 ||  0.932146 ||      2.643874 ||      0.3685 ||   0.6485 ||  0.927693 ||  0.928979 ||    13.247334 || 
    Epoch 08    --      2.488356 ||   0.4001 ||   0.6809 ||  0.934927 ||  0.938599 ||      2.546875 ||      0.3862 ||   0.6678 ||  0.933500 ||  0.934769 ||    12.897242 || 
    Epoch 09    --      2.377648 ||   0.4237 ||   0.7066 ||  0.941215 ||  0.944499 ||      2.444329 ||      0.3986 ||   0.6864 ||  0.941082 ||  0.941721 ||    14.486282 || 
    Epoch 10    --      2.266168 ||   0.4490 ||   0.7259 ||  0.947245 ||  0.950305 ||      2.329925 ||      0.4325 ||   0.7187 ||  0.945778 ||  0.947189 ||    13.650579 || 
    Epoch 11    --      2.166748 ||   0.4681 ||   0.7440 ||  0.952612 ||  0.955263 ||      2.271905 ||      0.4407 ||   0.7223 ||  0.950308 ||  0.950130 ||    13.853436 || 
    Epoch 12    --      2.071869 ||   0.4954 ||   0.7641 ||  0.956730 ||  0.959274 ||      2.151298 ||      0.4667 ||   0.7486 ||  0.955226 ||  0.955081 ||    14.257881 || 
    Epoch 13    --      1.976089 ||   0.5133 ||   0.7805 ||  0.960778 ||  0.963147 ||      2.094479 ||      0.4765 ||   0.7630 ||  0.958636 ||  0.958090 ||    13.778774 || 
    Epoch 14    --      1.877468 ||   0.5372 ||   0.7989 ||  0.964746 ||  0.966790 ||      1.977431 ||      0.5110 ||   0.7774 ||  0.962551 ||  0.963160 ||    13.968628 || 
    Epoch 15    --      1.790472 ||   0.5588 ||   0.8117 ||  0.968344 ||  0.970228 ||      1.864741 ||      0.5317 ||   0.8044 ||  0.966956 ||  0.967696 ||    14.159197 || 
    Epoch 16    --      1.704903 ||   0.5776 ||   0.8259 ||  0.971137 ||  0.972893 ||      1.798691 ||      0.5479 ||   0.8113 ||  0.969958 ||  0.970217 ||    13.780856 || 
    Epoch 17    --      1.625605 ||   0.5967 ||   0.8384 ||  0.974020 ||  0.975706 ||      1.706305 ||      0.5724 ||   0.8219 ||  0.972671 ||  0.973034 ||    12.718270 || 
    Epoch 18    --      1.545129 ||   0.6177 ||   0.8517 ||  0.976902 ||  0.978436 ||      1.647437 ||      0.5881 ||   0.8360 ||  0.974563 ||  0.974668 ||    14.363493 || 
    Epoch 19    --      1.462798 ||   0.6379 ||   0.8648 ||  0.979155 ||  0.980525 ||      1.599028 ||      0.5952 ||   0.8422 ||  0.976056 ||  0.976165 ||    14.520183 || 
    Epoch 20    --      1.390307 ||   0.6541 ||   0.8742 ||  0.981105 ||  0.982492 ||      1.516408 ||      0.6186 ||   0.8522 ||  0.978665 ||  0.978787 ||    13.968898 || 
    Epoch 21    --      1.329276 ||   0.6736 ||   0.8835 ||  0.982522 ||  0.983756 ||      1.386922 ||      0.6479 ||   0.8746 ||  0.983041 ||  0.983095 ||    13.228754 || 
    Epoch 22    --      1.256078 ||   0.6913 ||   0.8939 ||  0.984856 ||  0.985949 ||      1.354564 ||      0.6569 ||   0.8777 ||  0.982843 ||  0.983051 ||    15.109329 || 
    Epoch 23    --      1.193853 ||   0.7071 ||   0.9019 ||  0.985899 ||  0.986913 ||      1.295173 ||      0.6753 ||   0.8860 ||  0.984414 ||  0.984619 ||    14.105506 || 
    Epoch 24    --      1.138756 ||   0.7187 ||   0.9071 ||  0.987881 ||  0.988842 ||      1.222891 ||      0.6960 ||   0.8987 ||  0.985296 ||  0.985673 ||    12.903204 || 
    Epoch 25    --      1.076438 ||   0.7364 ||   0.9172 ||  0.988558 ||  0.989395 ||      1.168265 ||      0.7047 ||   0.9068 ||  0.987938 ||  0.988245 ||    14.144690 || 
    Epoch 26    --      1.018501 ||   0.7519 ||   0.9242 ||  0.990125 ||  0.990902 ||      1.151570 ||      0.7072 ||   0.9106 ||  0.988116 ||  0.987945 ||    14.098593 || 
    Epoch 27    --      0.965526 ||   0.7646 ||   0.9311 ||  0.991098 ||  0.991828 ||      1.083418 ||      0.7266 ||   0.9190 ||  0.988996 ||  0.989241 ||    13.466741 || 
    Epoch 28    --      0.918256 ||   0.7769 ||   0.9363 ||  0.991879 ||  0.992549 ||      1.028900 ||      0.7357 ||   0.9266 ||  0.990849 ||  0.991025 ||    13.564117 || 
    Epoch 29    --      0.872315 ||   0.7897 ||   0.9423 ||  0.992743 ||  0.993361 ||      0.969363 ||      0.7603 ||   0.9306 ||  0.991382 ||  0.991552 ||    13.952564 || 
    Epoch 30    --      0.827109 ||   0.8040 ||   0.9475 ||  0.993511 ||  0.994081 ||      0.901890 ||      0.7752 ||   0.9405 ||  0.992807 ||  0.992943 ||    13.669933 || 
    Epoch 31    --      0.781172 ||   0.8167 ||   0.9516 ||  0.994113 ||  0.994611 ||      0.845937 ||      0.7951 ||   0.9472 ||  0.993361 ||  0.993635 ||    13.758302 || 
    Epoch 32    --      0.735845 ||   0.8277 ||   0.9572 ||  0.994856 ||  0.995328 ||      0.821443 ||      0.7932 ||   0.9516 ||  0.994227 ||  0.994341 ||    13.163110 || 
    Epoch 33    --      0.735517 ||   0.8257 ||   0.9576 ||  0.995002 ||  0.995488 ||      0.816969 ||      0.7995 ||   0.9508 ||  0.994153 ||  0.994217 ||    14.038876 || 
    Epoch 34    --      0.697822 ||   0.8364 ||   0.9606 ||  0.995239 ||  0.995672 ||      0.770241 ||      0.8092 ||   0.9539 ||  0.995157 ||  0.995147 ||    14.679787 || 
    Epoch 35    --      0.659097 ||   0.8468 ||   0.9645 ||  0.995919 ||  0.996313 ||      0.757532 ||      0.8064 ||   0.9585 ||  0.995359 ||  0.995308 ||    14.327166 || 
    Epoch 36    --      0.656888 ||   0.8486 ||   0.9647 ||  0.995872 ||  0.996256 ||      0.758367 ||      0.8126 ||   0.9563 ||  0.995602 ||  0.995639 ||    14.099246 || 
    Epoch 37    --      0.619920 ||   0.8575 ||   0.9689 ||  0.996452 ||  0.996813 ||      0.732282 ||      0.8176 ||   0.9580 ||  0.995449 ||  0.995434 ||    14.244597 || 
    Epoch 38    --      0.582058 ||   0.8705 ||   0.9721 ||  0.996902 ||  0.997209 ||      0.697990 ||      0.8269 ||   0.9614 ||  0.995650 ||  0.995725 ||    14.705755 || 
    Epoch 39    --      0.556358 ||   0.8769 ||   0.9730 ||  0.997128 ||  0.997460 ||      0.642072 ||      0.8415 ||   0.9698 ||  0.996759 ||  0.996692 ||    14.584300 || 
    Epoch 40    --      0.520294 ||   0.8873 ||   0.9762 ||  0.997602 ||  0.997883 ||      0.589150 ||      0.8621 ||   0.9741 ||  0.996934 ||  0.997049 ||    14.432930 || 
    Epoch 41    --      0.492639 ||   0.8927 ||   0.9792 ||  0.997907 ||  0.998151 ||      0.564706 ||      0.8645 ||   0.9745 ||  0.997358 ||  0.997346 ||    14.326681 || 
    Epoch 42    --      0.462154 ||   0.9010 ||   0.9819 ||  0.998076 ||  0.998291 ||      0.545635 ||      0.8736 ||   0.9751 ||  0.997716 ||  0.997796 ||    14.243982 || 
    Epoch 43    --      0.436966 ||   0.9090 ||   0.9836 ||  0.998372 ||  0.998564 ||      0.511638 ||      0.8803 ||   0.9770 ||  0.997863 ||  0.997923 ||    13.557556 || 
    Epoch 44    --      0.411530 ||   0.9169 ||   0.9849 ||  0.998482 ||  0.998671 ||      0.462959 ||      0.9001 ||   0.9821 ||  0.998391 ||  0.998443 ||    14.085176 || 
    Epoch 45    --      0.391965 ||   0.9200 ||   0.9861 ||  0.998730 ||  0.998896 ||      0.449207 ||      0.8991 ||   0.9855 ||  0.998904 ||  0.998904 ||    14.036622 || 
    Epoch 46    --      0.390775 ||   0.9210 ||   0.9857 ||  0.998700 ||  0.998863 ||      0.450353 ||      0.8989 ||   0.9852 ||  0.998519 ||  0.998474 ||    13.805919 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
