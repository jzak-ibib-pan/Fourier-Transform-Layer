Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.572276 ||   0.1614 ||   0.3728 ||  0.731036 ||  0.733774 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.787510 ||   0.1522 ||   0.3687 ||  0.742416 ||  0.748800 ||      3.620395 ||      0.1764 ||   0.4105 ||  0.836764 ||  0.841341 ||    98.232633 || 
    Epoch 01    --      3.395828 ||   0.2203 ||   0.4719 ||  0.859470 ||  0.866908 ||      3.399943 ||      0.2161 ||   0.4726 ||  0.864758 ||  0.867458 ||    97.279490 || 
    Epoch 02    --      3.209916 ||   0.2526 ||   0.5184 ||  0.879480 ||  0.885985 ||      3.245026 ||      0.2520 ||   0.5125 ||  0.877524 ||  0.881230 ||    97.123317 || 
    Epoch 03    --      3.065260 ||   0.2829 ||   0.5565 ||  0.892120 ||  0.897837 ||      3.113708 ||      0.2720 ||   0.5408 ||  0.891361 ||  0.894256 ||    97.098421 || 
    Epoch 04    --      2.937946 ||   0.3050 ||   0.5825 ||  0.903610 ||  0.908869 ||      2.997172 ||      0.2975 ||   0.5741 ||  0.901615 ||  0.903055 ||    97.125920 || 
    Epoch 05    --      2.818084 ||   0.3304 ||   0.6115 ||  0.912525 ||  0.917350 ||      2.862736 ||      0.3197 ||   0.5995 ||  0.912979 ||  0.914703 ||    96.835219 || 
    Epoch 06    --      2.695309 ||   0.3555 ||   0.6399 ||  0.921460 ||  0.925515 ||      2.789924 ||      0.3353 ||   0.6183 ||  0.917054 ||  0.919052 ||    96.764022 || 
    Epoch 07    --      2.591980 ||   0.3774 ||   0.6618 ||  0.928366 ||  0.932126 ||      2.646473 ||      0.3681 ||   0.6520 ||  0.927504 ||  0.928769 ||    97.279621 || 
    Epoch 08    --      2.481446 ||   0.3998 ||   0.6857 ||  0.935143 ||  0.938652 ||      2.553736 ||      0.3877 ||   0.6683 ||  0.933335 ||  0.934437 ||    97.295267 || 
    Epoch 09    --      2.371821 ||   0.4258 ||   0.7071 ||  0.941608 ||  0.944714 ||      2.448956 ||      0.4014 ||   0.6883 ||  0.940099 ||  0.940406 ||    96.826503 || 
    Epoch 10    --      2.261650 ||   0.4507 ||   0.7275 ||  0.947113 ||  0.950036 ||      2.336900 ||      0.4326 ||   0.7202 ||  0.945515 ||  0.946601 ||    96.685904 || 
    Epoch 11    --      2.159947 ||   0.4718 ||   0.7471 ||  0.952403 ||  0.954929 ||      2.275372 ||      0.4432 ||   0.7232 ||  0.949697 ||  0.949687 ||    96.600823 || 
    Epoch 12    --      2.068238 ||   0.4958 ||   0.7638 ||  0.956330 ||  0.958819 ||      2.154475 ||      0.4651 ||   0.7425 ||  0.955869 ||  0.955642 ||    96.826579 || 
    Epoch 13    --      1.974304 ||   0.5154 ||   0.7795 ||  0.960835 ||  0.963117 ||      2.111660 ||      0.4746 ||   0.7575 ||  0.957800 ||  0.956941 ||    97.609989 || 
    Epoch 14    --      1.874955 ||   0.5397 ||   0.7964 ||  0.964760 ||  0.966752 ||      1.981793 ||      0.5028 ||   0.7804 ||  0.962481 ||  0.962615 ||    97.014056 || 
    Epoch 15    --      1.794093 ||   0.5571 ||   0.8113 ||  0.968079 ||  0.969914 ||      1.853894 ||      0.5444 ||   0.8042 ||  0.967077 ||  0.967756 ||    96.751112 || 
    Epoch 16    --      1.703244 ||   0.5824 ||   0.8264 ||  0.971222 ||  0.972901 ||      1.804937 ||      0.5503 ||   0.8133 ||  0.968823 ||  0.969151 ||    97.498446 || 
    Epoch 17    --      1.629332 ||   0.5967 ||   0.8385 ||  0.973748 ||  0.975321 ||      1.695104 ||      0.5748 ||   0.8276 ||  0.973217 ||  0.973494 ||    96.722922 || 
    Epoch 18    --      1.549852 ||   0.6153 ||   0.8513 ||  0.976427 ||  0.977880 ||      1.626014 ||      0.5980 ||   0.8359 ||  0.975425 ||  0.975591 ||    97.045348 || 
    Epoch 19    --      1.467482 ||   0.6385 ||   0.8623 ||  0.978863 ||  0.980191 ||      1.582376 ||      0.5993 ||   0.8467 ||  0.976480 ||  0.976506 ||    97.180297 || 
    Epoch 20    --      1.394326 ||   0.6529 ||   0.8755 ||  0.981027 ||  0.982350 ||      1.530214 ||      0.6167 ||   0.8510 ||  0.977414 ||  0.977134 ||    97.061024 || 
    Epoch 21    --      1.332244 ||   0.6705 ||   0.8822 ||  0.982330 ||  0.983502 ||      1.413195 ||      0.6377 ||   0.8727 ||  0.982329 ||  0.982111 ||    96.985301 || 
    Epoch 22    --      1.263880 ||   0.6871 ||   0.8911 ||  0.984461 ||  0.985512 ||      1.359603 ||      0.6492 ||   0.8787 ||  0.983012 ||  0.983155 ||    98.225682 || 
    Epoch 23    --      1.199737 ||   0.7054 ||   0.9024 ||  0.985985 ||  0.986952 ||      1.284434 ||      0.6755 ||   0.8868 ||  0.984342 ||  0.984693 ||    98.086673 || 
    Epoch 24    --      1.145064 ||   0.7186 ||   0.9088 ||  0.987244 ||  0.988171 ||      1.228855 ||      0.6852 ||   0.8984 ||  0.985884 ||  0.986312 ||    98.058863 || 
    Epoch 25    --      1.082944 ||   0.7341 ||   0.9155 ||  0.988486 ||  0.989315 ||      1.183731 ||      0.6958 ||   0.9055 ||  0.987331 ||  0.987379 ||    98.154779 || 
    Epoch 26    --      1.028137 ||   0.7461 ||   0.9236 ||  0.989877 ||  0.990654 ||      1.116602 ||      0.7161 ||   0.9124 ||  0.988830 ||  0.988878 ||    98.208140 || 
    Epoch 27    --      0.969742 ||   0.7631 ||   0.9297 ||  0.990986 ||  0.991721 ||      1.077963 ||      0.7301 ||   0.9186 ||  0.989495 ||  0.989727 ||    97.922563 || 
    Epoch 28    --      0.924374 ||   0.7749 ||   0.9352 ||  0.991833 ||  0.992504 ||      1.046947 ||      0.7336 ||   0.9221 ||  0.990477 ||  0.990531 ||    97.998595 || 
    Epoch 29    --      0.878633 ||   0.7866 ||   0.9409 ||  0.992598 ||  0.993242 ||      0.983339 ||      0.7496 ||   0.9277 ||  0.991857 ||  0.991790 ||    98.109652 || 
    Epoch 30    --      0.832056 ||   0.8006 ||   0.9461 ||  0.993327 ||  0.993905 ||      0.908195 ||      0.7756 ||   0.9420 ||  0.992843 ||  0.992905 ||    98.076726 || 
    Epoch 31    --      0.788423 ||   0.8136 ||   0.9507 ||  0.994166 ||  0.994676 ||      0.858733 ||      0.7889 ||   0.9450 ||  0.993528 ||  0.993718 ||    98.020731 || 
    Epoch 32    --      0.745124 ||   0.8219 ||   0.9552 ||  0.994768 ||  0.995243 ||      0.818321 ||      0.7981 ||   0.9508 ||  0.994007 ||  0.994060 ||    97.975412 || 
    Epoch 33    --      0.702719 ||   0.8361 ||   0.9597 ||  0.995424 ||  0.995870 ||      0.787199 ||      0.8062 ||   0.9545 ||  0.994563 ||  0.994508 ||    98.132200 || 
    Epoch 34    --      0.664740 ||   0.8461 ||   0.9639 ||  0.995834 ||  0.996227 ||      0.747390 ||      0.8098 ||   0.9567 ||  0.995350 ||  0.995324 ||    97.936145 || 
    Epoch 35    --      0.627541 ||   0.8549 ||   0.9675 ||  0.996351 ||  0.996698 ||      0.729800 ||      0.8119 ||   0.9596 ||  0.995963 ||  0.995782 ||    98.092394 || 
    Epoch 36    --      0.591372 ||   0.8663 ||   0.9703 ||  0.996896 ||  0.997229 ||      0.690761 ||      0.8320 ||   0.9636 ||  0.995849 ||  0.995824 ||    98.014222 || 
    Epoch 37    --      0.557830 ||   0.8756 ||   0.9739 ||  0.997162 ||  0.997461 ||      0.687048 ||      0.8270 ||   0.9634 ||  0.995817 ||  0.995692 ||    97.998675 || 
    Epoch 38    --      0.560299 ||   0.8752 ||   0.9726 ||  0.996990 ||  0.997289 ||      0.659484 ||      0.8379 ||   0.9650 ||  0.996298 ||  0.996251 ||    98.326796 || 
    Epoch 39    --      0.532578 ||   0.8817 ||   0.9756 ||  0.997334 ||  0.997603 ||      0.592810 ||      0.8586 ||   0.9723 ||  0.997336 ||  0.997432 ||    98.076762 || 
    Epoch 40    --      0.495594 ||   0.8916 ||   0.9777 ||  0.997886 ||  0.998127 ||      0.594618 ||      0.8580 ||   0.9717 ||  0.997024 ||  0.996989 ||    98.059608 || 
    Epoch 41    --      0.496890 ||   0.8920 ||   0.9790 ||  0.997758 ||  0.997972 ||      0.584976 ||      0.8532 ||   0.9737 ||  0.997288 ||  0.997160 ||    98.139319 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
