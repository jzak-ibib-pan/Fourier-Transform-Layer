Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     46.517231 ||   0.1671 ||   0.3955 ||  0.598815 ||  0.598890 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.342128 ||   0.0984 ||   0.2569 ||  0.624029 ||  0.625352 ||     12.111659 ||      0.1378 ||   0.3424 ||  0.648467 ||  0.646055 ||    85.138881 || 
    Epoch 01    --     10.094090 ||   0.2149 ||   0.4680 ||  0.702818 ||  0.703534 ||     11.291683 ||      0.1889 ||   0.4316 ||  0.683869 ||  0.682387 ||    85.201369 || 
    Epoch 02    --      9.457349 ||   0.2665 ||   0.5358 ||  0.728393 ||  0.729003 ||     10.622539 ||      0.2412 ||   0.5161 ||  0.712397 ||  0.712009 ||    86.029547 || 
    Epoch 03    --      8.830816 ||   0.3160 ||   0.5986 ||  0.753199 ||  0.753695 ||     10.064797 ||      0.2889 ||   0.5612 ||  0.730823 ||  0.731664 ||    85.401121 || 
    Epoch 04    --      8.144711 ||   0.3623 ||   0.6532 ||  0.774775 ||  0.775322 ||      9.315947 ||      0.3240 ||   0.6172 ||  0.756868 ||  0.755474 ||    85.732727 || 
    Epoch 05    --      7.588518 ||   0.4000 ||   0.6972 ||  0.793502 ||  0.793919 ||      8.578960 ||      0.3722 ||   0.6691 ||  0.773529 ||  0.773071 ||    89.388909 || 
    Epoch 06    --      7.039804 ||   0.4426 ||   0.7366 ||  0.810638 ||  0.810922 ||      7.789815 ||      0.4102 ||   0.7106 ||  0.791635 ||  0.792232 ||    87.317958 || 
    Epoch 07    --      6.549388 ||   0.4750 ||   0.7670 ||  0.823551 ||  0.823820 ||      7.975649 ||      0.4232 ||   0.7321 ||  0.796116 ||  0.795252 ||    87.397239 || 
    Epoch 08    --      6.033668 ||   0.5099 ||   0.7983 ||  0.838568 ||  0.838770 ||      6.927573 ||      0.4757 ||   0.7741 ||  0.821656 ||  0.820907 ||    87.911024 || 
    Epoch 09    --      5.658908 ||   0.5390 ||   0.8220 ||  0.850040 ||  0.850320 ||      6.806334 ||      0.4946 ||   0.7884 ||  0.824671 ||  0.823905 ||    91.326510 || 
    Epoch 10    --      5.259735 ||   0.5675 ||   0.8444 ||  0.860158 ||  0.860409 ||      5.977706 ||      0.5309 ||   0.8248 ||  0.844692 ||  0.845571 ||    87.598864 || 
    Epoch 11    --      4.998899 ||   0.5842 ||   0.8608 ||  0.868078 ||  0.868377 ||      5.600535 ||      0.5519 ||   0.8395 ||  0.853410 ||  0.853535 ||    86.435961 || 
    Epoch 12    --      4.664604 ||   0.6094 ||   0.8755 ||  0.876141 ||  0.876346 ||      5.545234 ||      0.5697 ||   0.8536 ||  0.860989 ||  0.860039 ||    87.795383 || 
    Epoch 13    --      4.286895 ||   0.6347 ||   0.8928 ||  0.887274 ||  0.887472 ||      5.305641 ||      0.5930 ||   0.8701 ||  0.865561 ||  0.865161 ||    87.451668 || 
    Epoch 14    --      4.130256 ||   0.6517 ||   0.9014 ||  0.891038 ||  0.891170 ||      4.754467 ||      0.6165 ||   0.8892 ||  0.877111 ||  0.876953 ||    87.437713 || 
    Epoch 15    --      3.872155 ||   0.6694 ||   0.9136 ||  0.898676 ||  0.898830 ||      4.736193 ||      0.6262 ||   0.8931 ||  0.879323 ||  0.879188 ||    87.225150 || 
    Epoch 16    --      3.672569 ||   0.6864 ||   0.9217 ||  0.904907 ||  0.905053 ||      5.198102 ||      0.6036 ||   0.8960 ||  0.868666 ||  0.868015 ||    87.607953 || 
    Epoch 17    --      3.682567 ||   0.6856 ||   0.9234 ||  0.904348 ||  0.904557 ||      4.244766 ||      0.6556 ||   0.9088 ||  0.891411 ||  0.890844 ||    87.389233 || 
    Epoch 18    --      3.491209 ||   0.6976 ||   0.9288 ||  0.908796 ||  0.909039 ||      4.563469 ||      0.6492 ||   0.9056 ||  0.884524 ||  0.884947 ||    87.889198 || 
    Epoch 19    --      3.494471 ||   0.6986 ||   0.9285 ||  0.907925 ||  0.908123 ||      3.916612 ||      0.6676 ||   0.9222 ||  0.896371 ||  0.896098 ||    87.279877 || 
    Epoch 20    --      3.341701 ||   0.7139 ||   0.9357 ||  0.912756 ||  0.912892 ||      3.979993 ||      0.6603 ||   0.9237 ||  0.894946 ||  0.894727 ||    87.545500 || 
    Epoch 21    --      3.371376 ||   0.7107 ||   0.9342 ||  0.911926 ||  0.912059 ||      4.001865 ||      0.6770 ||   0.9213 ||  0.897531 ||  0.897753 ||    87.545502 || 
    Epoch 22    --      3.087467 ||   0.7274 ||   0.9423 ||  0.918852 ||  0.918957 ||      3.954913 ||      0.6798 ||   0.9295 ||  0.897588 ||  0.897529 ||    87.576784 || 
    Epoch 23    --      2.980070 ||   0.7407 ||   0.9491 ||  0.922480 ||  0.922631 ||      3.703725 ||      0.7008 ||   0.9349 ||  0.904132 ||  0.904745 ||    87.326851 || 
    Epoch 24    --      2.858719 ||   0.7508 ||   0.9537 ||  0.925737 ||  0.925766 ||      3.527545 ||      0.7185 ||   0.9380 ||  0.911238 ||  0.911407 ||    87.592469 || 
    Epoch 25    --      2.725783 ||   0.7611 ||   0.9577 ||  0.928986 ||  0.929115 ||      3.433891 ||      0.7201 ||   0.9447 ||  0.915340 ||  0.914562 ||    88.029938 || 
    Epoch 26    --      2.573236 ||   0.7712 ||   0.9611 ||  0.933296 ||  0.933472 ||      3.359847 ||      0.7261 ||   0.9518 ||  0.914443 ||  0.914010 ||    87.358141 || 
    Epoch 27    --      2.518485 ||   0.7775 ||   0.9654 ||  0.935393 ||  0.935491 ||      2.992356 ||      0.7477 ||   0.9552 ||  0.924412 ||  0.924358 ||    87.717517 || 
    Epoch 28    --      2.419004 ||   0.7871 ||   0.9686 ||  0.936728 ||  0.936838 ||      3.375627 ||      0.7283 ||   0.9505 ||  0.914855 ||  0.913675 ||    87.724889 || 
    Epoch 29    --      2.423951 ||   0.7868 ||   0.9668 ||  0.937624 ||  0.937588 ||      2.983072 ||      0.7510 ||   0.9575 ||  0.924481 ||  0.924436 ||    87.451885 || 
    Epoch 30    --      2.265841 ||   0.7979 ||   0.9717 ||  0.941846 ||  0.941914 ||      3.023459 ||      0.7515 ||   0.9611 ||  0.923339 ||  0.923241 ||    87.748787 || 
    Epoch 31    --      2.278212 ||   0.7972 ||   0.9708 ||  0.941827 ||  0.941921 ||      2.682316 ||      0.7694 ||   0.9667 ||  0.929503 ||  0.929857 ||    87.639438 || 
    Epoch 32    --      2.205728 ||   0.8023 ||   0.9725 ||  0.943625 ||  0.943690 ||      2.249739 ||      0.7966 ||   0.9719 ||  0.940422 ||  0.940621 ||    88.233184 || 
    Epoch 33    --      2.093769 ||   0.8127 ||   0.9764 ||  0.946059 ||  0.946163 ||      2.746279 ||      0.7816 ||   0.9621 ||  0.933517 ||  0.932721 ||    87.467584 || 
    Epoch 34    --      2.110841 ||   0.8110 ||   0.9748 ||  0.945538 ||  0.945638 ||      2.754448 ||      0.7723 ||   0.9660 ||  0.932042 ||  0.930503 ||    87.792364 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
