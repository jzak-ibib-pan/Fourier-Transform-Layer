Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.015608 ||   0.1699 ||   0.3847 ||  0.746180 ||  0.749770 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.779181 ||   0.1528 ||   0.3695 ||  0.744041 ||  0.750543 ||      3.617523 ||      0.1769 ||   0.4130 ||  0.836913 ||  0.841525 ||    99.248675 || 
    Epoch 01    --      3.389369 ||   0.2219 ||   0.4760 ||  0.860145 ||  0.867684 ||      3.395396 ||      0.2210 ||   0.4724 ||  0.865248 ||  0.867990 ||    98.233007 || 
    Epoch 02    --      3.205366 ||   0.2525 ||   0.5211 ||  0.879910 ||  0.886491 ||      3.240159 ||      0.2568 ||   0.5106 ||  0.877881 ||  0.881903 ||    97.951835 || 
    Epoch 03    --      3.062858 ||   0.2835 ||   0.5577 ||  0.892239 ||  0.898103 ||      3.104808 ||      0.2699 ||   0.5421 ||  0.892050 ||  0.894952 ||    97.995138 || 
    Epoch 04    --      2.933037 ||   0.3086 ||   0.5867 ||  0.903617 ||  0.908924 ||      2.992279 ||      0.3020 ||   0.5735 ||  0.902241 ||  0.903515 ||    98.097107 || 
    Epoch 05    --      2.811045 ||   0.3334 ||   0.6145 ||  0.912996 ||  0.917841 ||      2.847762 ||      0.3228 ||   0.6024 ||  0.913560 ||  0.915674 ||    98.217494 || 
    Epoch 06    --      2.684899 ||   0.3559 ||   0.6425 ||  0.922158 ||  0.926311 ||      2.764693 ||      0.3364 ||   0.6245 ||  0.919384 ||  0.921665 ||    98.100021 || 
    Epoch 07    --      2.574810 ||   0.3819 ||   0.6654 ||  0.929650 ||  0.933480 ||      2.618963 ||      0.3708 ||   0.6542 ||  0.929264 ||  0.930789 ||    98.389360 || 
    Epoch 08    --      2.459275 ||   0.4063 ||   0.6893 ||  0.936209 ||  0.939743 ||      2.523350 ||      0.3890 ||   0.6768 ||  0.935218 ||  0.936457 ||    98.014361 || 
    Epoch 09    --      2.344893 ||   0.4313 ||   0.7102 ||  0.942864 ||  0.946073 ||      2.420449 ||      0.4101 ||   0.6915 ||  0.941713 ||  0.942065 ||    97.814913 || 
    Epoch 10    --      2.230540 ||   0.4579 ||   0.7324 ||  0.948947 ||  0.951854 ||      2.302700 ||      0.4361 ||   0.7215 ||  0.947455 ||  0.948703 ||    98.310568 || 
    Epoch 11    --      2.125338 ||   0.4788 ||   0.7534 ||  0.954335 ||  0.956843 ||      2.246428 ||      0.4519 ||   0.7279 ||  0.950562 ||  0.950465 ||    98.326864 || 
    Epoch 12    --      2.028931 ||   0.5020 ||   0.7703 ||  0.958586 ||  0.960927 ||      2.122346 ||      0.4794 ||   0.7523 ||  0.956266 ||  0.956359 ||    97.936246 || 
    Epoch 13    --      1.934660 ||   0.5248 ||   0.7850 ||  0.962427 ||  0.964673 ||      2.059746 ||      0.4874 ||   0.7672 ||  0.961190 ||  0.960603 ||    98.346014 || 
    Epoch 14    --      1.835525 ||   0.5465 ||   0.8049 ||  0.966403 ||  0.968360 ||      1.933165 ||      0.5151 ||   0.7907 ||  0.964565 ||  0.964832 ||    97.905012 || 
    Epoch 15    --      1.750718 ||   0.5687 ||   0.8195 ||  0.969478 ||  0.971265 ||      1.822782 ||      0.5455 ||   0.8080 ||  0.969418 ||  0.969923 ||    98.280046 || 
    Epoch 16    --      1.663134 ||   0.5908 ||   0.8328 ||  0.972905 ||  0.974572 ||      1.768891 ||      0.5519 ||   0.8194 ||  0.971230 ||  0.971277 ||    98.030032 || 
    Epoch 17    --      1.583383 ||   0.6047 ||   0.8439 ||  0.975638 ||  0.977150 ||      1.668219 ||      0.5817 ||   0.8304 ||  0.973897 ||  0.974268 ||    98.670647 || 
    Epoch 18    --      1.506202 ||   0.6269 ||   0.8574 ||  0.978030 ||  0.979414 ||      1.599471 ||      0.5957 ||   0.8426 ||  0.976037 ||  0.976127 ||    98.686287 || 
    Epoch 19    --      1.423889 ||   0.6476 ||   0.8685 ||  0.980395 ||  0.981626 ||      1.549446 ||      0.6071 ||   0.8527 ||  0.977751 ||  0.977737 ||    98.985528 || 
    Epoch 20    --      1.349758 ||   0.6658 ||   0.8796 ||  0.982368 ||  0.983607 ||      1.470541 ||      0.6283 ||   0.8612 ||  0.979338 ||  0.979414 ||    99.967538 || 
    Epoch 21    --      1.289677 ||   0.6825 ||   0.8864 ||  0.983760 ||  0.984897 ||      1.375292 ||      0.6429 ||   0.8823 ||  0.983634 ||  0.983398 ||    99.974277 || 
    Epoch 22    --      1.222102 ||   0.6962 ||   0.8991 ||  0.985759 ||  0.986715 ||      1.304121 ||      0.6688 ||   0.8870 ||  0.983842 ||  0.984084 ||   101.268415 || 
    Epoch 23    --      1.156945 ||   0.7158 ||   0.9076 ||  0.987001 ||  0.987923 ||      1.246046 ||      0.6827 ||   0.8929 ||  0.986151 ||  0.986381 ||   101.467533 || 
    Epoch 24    --      1.100794 ||   0.7289 ||   0.9141 ||  0.988648 ||  0.989512 ||      1.194615 ||      0.6931 ||   0.9034 ||  0.986649 ||  0.986883 ||   101.030072 || 
    Epoch 25    --      1.043781 ||   0.7444 ||   0.9194 ||  0.989563 ||  0.990318 ||      1.142338 ||      0.7100 ||   0.9127 ||  0.987855 ||  0.988127 ||   102.139455 || 
    Epoch 26    --      0.987554 ||   0.7573 ||   0.9280 ||  0.990875 ||  0.991583 ||      1.092566 ||      0.7218 ||   0.9191 ||  0.989560 ||  0.989383 ||   102.081109 || 
    Epoch 27    --      0.931576 ||   0.7722 ||   0.9352 ||  0.991723 ||  0.992351 ||      1.032618 ||      0.7418 ||   0.9239 ||  0.989944 ||  0.990135 ||   101.155238 || 
    Epoch 28    --      0.884711 ||   0.7865 ||   0.9409 ||  0.992455 ||  0.993078 ||      1.004419 ||      0.7433 ||   0.9280 ||  0.991705 ||  0.991570 ||   101.295701 || 
    Epoch 29    --      0.843587 ||   0.7988 ||   0.9451 ||  0.993277 ||  0.993826 ||      0.926195 ||      0.7660 ||   0.9409 ||  0.992965 ||  0.993038 ||   101.405082 || 
    Epoch 30    --      0.794490 ||   0.8110 ||   0.9518 ||  0.993989 ||  0.994514 ||      0.872431 ||      0.7774 ||   0.9493 ||  0.993278 ||  0.993274 ||   101.092583 || 
    Epoch 31    --      0.751227 ||   0.8229 ||   0.9564 ||  0.994763 ||  0.995190 ||      0.818166 ||      0.8008 ||   0.9507 ||  0.994199 ||  0.994466 ||   101.592590 || 
    Epoch 32    --      0.707442 ||   0.8347 ||   0.9595 ||  0.995362 ||  0.995781 ||      0.797733 ||      0.8030 ||   0.9533 ||  0.994665 ||  0.994617 ||    97.795749 || 
    Epoch 33    --      0.665338 ||   0.8464 ||   0.9645 ||  0.995962 ||  0.996342 ||      0.753608 ||      0.8159 ||   0.9563 ||  0.995016 ||  0.995023 ||    98.233282 || 
    Epoch 34    --      0.630104 ||   0.8576 ||   0.9679 ||  0.996173 ||  0.996514 ||      0.725525 ||      0.8145 ||   0.9597 ||  0.995901 ||  0.995822 ||    98.217625 || 
    Epoch 35    --      0.631050 ||   0.8562 ||   0.9675 ||  0.996222 ||  0.996559 ||      0.727012 ||      0.8154 ||   0.9630 ||  0.995789 ||  0.995742 ||    97.758134 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
