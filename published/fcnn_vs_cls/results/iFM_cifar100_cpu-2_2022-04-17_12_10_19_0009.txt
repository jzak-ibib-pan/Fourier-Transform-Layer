Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.296461 ||   0.1616 ||   0.3736 ||  0.715793 ||  0.718109 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.775681 ||   0.1534 ||   0.3723 ||  0.744267 ||  0.750865 ||      3.621192 ||      0.1770 ||   0.4119 ||  0.836491 ||  0.840719 ||   100.145090 || 
    Epoch 01    --      3.397727 ||   0.2183 ||   0.4746 ||  0.859167 ||  0.866612 ||      3.407818 ||      0.2164 ||   0.4724 ||  0.864434 ||  0.866763 ||    98.209945 || 
    Epoch 02    --      3.215345 ||   0.2523 ||   0.5195 ||  0.879003 ||  0.885485 ||      3.252114 ||      0.2546 ||   0.5129 ||  0.876371 ||  0.880111 ||    98.108447 || 
    Epoch 03    --      3.075846 ||   0.2826 ||   0.5545 ||  0.891170 ||  0.896919 ||      3.114334 ||      0.2729 ||   0.5422 ||  0.890796 ||  0.894011 ||    98.280353 || 
    Epoch 04    --      2.950263 ||   0.3040 ||   0.5833 ||  0.902373 ||  0.907779 ||      2.999663 ||      0.3013 ||   0.5721 ||  0.901624 ||  0.902766 ||    98.124082 || 
    Epoch 05    --      2.831977 ||   0.3275 ||   0.6096 ||  0.911449 ||  0.916254 ||      2.864311 ||      0.3203 ||   0.5959 ||  0.911694 ||  0.913927 ||    98.342856 || 
    Epoch 06    --      2.711318 ||   0.3524 ||   0.6389 ||  0.920540 ||  0.924667 ||      2.792439 ||      0.3345 ||   0.6172 ||  0.917486 ||  0.919687 ||    98.601200 || 
    Epoch 07    --      2.606917 ||   0.3760 ||   0.6570 ||  0.927564 ||  0.931409 ||      2.651744 ||      0.3612 ||   0.6486 ||  0.926704 ||  0.928210 ||    98.670957 || 
    Epoch 08    --      2.496555 ||   0.3970 ||   0.6826 ||  0.934353 ||  0.937917 ||      2.570705 ||      0.3827 ||   0.6647 ||  0.932099 ||  0.933135 ||    98.545986 || 
    Epoch 09    --      2.387604 ||   0.4223 ||   0.7026 ||  0.940723 ||  0.943954 ||      2.453876 ||      0.4010 ||   0.6883 ||  0.939753 ||  0.940070 ||    98.747080 || 
    Epoch 10    --      2.275894 ||   0.4469 ||   0.7250 ||  0.946554 ||  0.949576 ||      2.345322 ||      0.4248 ||   0.7158 ||  0.944941 ||  0.945871 ||    98.560637 || 
    Epoch 11    --      2.175805 ||   0.4662 ||   0.7418 ||  0.951895 ||  0.954545 ||      2.297707 ||      0.4404 ||   0.7216 ||  0.948827 ||  0.948343 ||    98.342837 || 
    Epoch 12    --      2.078121 ||   0.4907 ||   0.7617 ||  0.956700 ||  0.959164 ||      2.178052 ||      0.4610 ||   0.7403 ||  0.954455 ||  0.954205 ||    98.112408 || 
    Epoch 13    --      1.985642 ||   0.5114 ||   0.7783 ||  0.960446 ||  0.962824 ||      2.116551 ||      0.4671 ||   0.7557 ||  0.958942 ||  0.958042 ||    98.407583 || 
    Epoch 14    --      1.888842 ||   0.5351 ||   0.7953 ||  0.964415 ||  0.966452 ||      1.973709 ||      0.5099 ||   0.7798 ||  0.963324 ||  0.963601 ||    98.084792 || 
    Epoch 15    --      1.803424 ||   0.5544 ||   0.8091 ||  0.967743 ||  0.969704 ||      1.864530 ||      0.5358 ||   0.8006 ||  0.967854 ||  0.968422 ||    98.139730 || 
    Epoch 16    --      1.711212 ||   0.5772 ||   0.8248 ||  0.971399 ||  0.973103 ||      1.803577 ||      0.5528 ||   0.8126 ||  0.968703 ||  0.969308 ||    97.967858 || 
    Epoch 17    --      1.633483 ||   0.5955 ||   0.8361 ||  0.973963 ||  0.975633 ||      1.715825 ||      0.5735 ||   0.8205 ||  0.972656 ||  0.972912 ||    97.921995 || 
    Epoch 18    --      1.554983 ||   0.6132 ||   0.8510 ||  0.976536 ||  0.978030 ||      1.653403 ||      0.5819 ||   0.8326 ||  0.974782 ||  0.974808 ||    97.905365 || 
    Epoch 19    --      1.470245 ||   0.6334 ||   0.8629 ||  0.979096 ||  0.980445 ||      1.615561 ||      0.5922 ||   0.8407 ||  0.975524 ||  0.975299 ||    98.610683 || 
    Epoch 20    --      1.398516 ||   0.6511 ||   0.8726 ||  0.981192 ||  0.982530 ||      1.518077 ||      0.6175 ||   0.8560 ||  0.978750 ||  0.978704 ||    98.098560 || 
    Epoch 21    --      1.336900 ||   0.6677 ||   0.8816 ||  0.982632 ||  0.983842 ||      1.421354 ||      0.6351 ||   0.8719 ||  0.982319 ||  0.982049 ||    98.139759 || 
    Epoch 22    --      1.265725 ||   0.6860 ||   0.8926 ||  0.984615 ||  0.985664 ||      1.355290 ||      0.6517 ||   0.8816 ||  0.983236 ||  0.983498 ||    98.155405 || 
    Epoch 23    --      1.203011 ||   0.7035 ||   0.9014 ||  0.985981 ||  0.986991 ||      1.297318 ||      0.6740 ||   0.8840 ||  0.984802 ||  0.984915 ||    98.717871 || 
    Epoch 24    --      1.146950 ||   0.7186 ||   0.9095 ||  0.987409 ||  0.988324 ||      1.230805 ||      0.6842 ||   0.9010 ||  0.986089 ||  0.986428 ||    98.702240 || 
    Epoch 25    --      1.083906 ||   0.7344 ||   0.9154 ||  0.988779 ||  0.989627 ||      1.174962 ||      0.7042 ||   0.9019 ||  0.987306 ||  0.987680 ||   101.608472 || 
    Epoch 26    --      1.027898 ||   0.7489 ||   0.9220 ||  0.990109 ||  0.990867 ||      1.136458 ||      0.7065 ||   0.9150 ||  0.989097 ||  0.988940 ||   101.486368 || 
    Epoch 27    --      0.972323 ||   0.7634 ||   0.9299 ||  0.990956 ||  0.991693 ||      1.075273 ||      0.7306 ||   0.9152 ||  0.989723 ||  0.990022 ||   101.438197 || 
    Epoch 28    --      0.922825 ||   0.7753 ||   0.9363 ||  0.991862 ||  0.992559 ||      1.031839 ||      0.7306 ||   0.9272 ||  0.991207 ||  0.991256 ||   101.348728 || 
    Epoch 29    --      0.923914 ||   0.7754 ||   0.9368 ||  0.992041 ||  0.992699 ||      1.048674 ||      0.7323 ||   0.9244 ||  0.990320 ||  0.990338 ||   101.217893 || 
    Epoch 30    --      0.875263 ||   0.7890 ||   0.9421 ||  0.992658 ||  0.993296 ||      0.958201 ||      0.7612 ||   0.9323 ||  0.991649 ||  0.991840 ||   101.624147 || 
    Epoch 31    --      0.830549 ||   0.8013 ||   0.9473 ||  0.993509 ||  0.994041 ||      0.901147 ||      0.7744 ||   0.9406 ||  0.993225 ||  0.993445 ||   100.436654 || 
    Epoch 32    --      0.783154 ||   0.8144 ||   0.9510 ||  0.994123 ||  0.994660 ||      0.867143 ||      0.7816 ||   0.9440 ||  0.993741 ||  0.993912 ||   101.850317 || 
    Epoch 33    --      0.737113 ||   0.8257 ||   0.9563 ||  0.995017 ||  0.995493 ||      0.825650 ||      0.7978 ||   0.9466 ||  0.993976 ||  0.994104 ||   101.391545 || 
    Epoch 34    --      0.702263 ||   0.8356 ||   0.9595 ||  0.995286 ||  0.995740 ||      0.765795 ||      0.8149 ||   0.9551 ||  0.995327 ||  0.995435 ||   101.755506 || 
    Epoch 35    --      0.663038 ||   0.8463 ||   0.9638 ||  0.996029 ||  0.996413 ||      0.765804 ||      0.8094 ||   0.9562 ||  0.995184 ||  0.994985 ||   101.127503 || 
    Epoch 36    --      0.662514 ||   0.8477 ||   0.9631 ||  0.995897 ||  0.996298 ||      0.743439 ||      0.8187 ||   0.9596 ||  0.995137 ||  0.995163 ||   101.452277 || 
    Epoch 37    --      0.622594 ||   0.8573 ||   0.9673 ||  0.996389 ||  0.996754 ||      0.738590 ||      0.8200 ||   0.9568 ||  0.995028 ||  0.994930 ||   101.796000 || 
    Epoch 38    --      0.586169 ||   0.8679 ||   0.9713 ||  0.996727 ||  0.997056 ||      0.688872 ||      0.8299 ||   0.9622 ||  0.995879 ||  0.995900 ||   101.483495 || 
    Epoch 39    --      0.561958 ||   0.8745 ||   0.9726 ||  0.996944 ||  0.997266 ||      0.618317 ||      0.8520 ||   0.9723 ||  0.997192 ||  0.997242 ||   101.593829 || 
    Epoch 40    --      0.523379 ||   0.8849 ||   0.9766 ||  0.997482 ||  0.997760 ||      0.609187 ||      0.8535 ||   0.9718 ||  0.997022 ||  0.997032 ||   101.381968 || 
    Epoch 41    --      0.495123 ||   0.8906 ||   0.9788 ||  0.997830 ||  0.998057 ||      0.577276 ||      0.8625 ||   0.9743 ||  0.997237 ||  0.997134 ||   101.631299 || 
    Epoch 42    --      0.465974 ||   0.9009 ||   0.9818 ||  0.998005 ||  0.998236 ||      0.544806 ||      0.8720 ||   0.9756 ||  0.997824 ||  0.997859 ||   101.124143 || 
    Epoch 43    --      0.441273 ||   0.9063 ||   0.9834 ||  0.998223 ||  0.998416 ||      0.515682 ||      0.8785 ||   0.9791 ||  0.997702 ||  0.997739 ||   101.811629 || 
    Epoch 44    --      0.414083 ||   0.9136 ||   0.9850 ||  0.998432 ||  0.998610 ||      0.483767 ||      0.8924 ||   0.9810 ||  0.998004 ||  0.997976 ||   101.333903 || 
    Epoch 45    --      0.393655 ||   0.9207 ||   0.9862 ||  0.998557 ||  0.998714 ||      0.465032 ||      0.8943 ||   0.9855 ||  0.998486 ||  0.998488 ||   101.612455 || 
    Epoch 46    --      0.369750 ||   0.9272 ||   0.9877 ||  0.998711 ||  0.998853 ||      0.414118 ||      0.9114 ||   0.9867 ||  0.998651 ||  0.998654 ||   101.359703 || 
    Epoch 47    --      0.346577 ||   0.9329 ||   0.9893 ||  0.999003 ||  0.999136 ||      0.417664 ||      0.9062 ||   0.9865 ||  0.998788 ||  0.998775 ||   101.467921 || 
    Epoch 48    --      0.347354 ||   0.9317 ||   0.9891 ||  0.998940 ||  0.999068 ||      0.416370 ||      0.9077 ||   0.9869 ||  0.998605 ||  0.998520 ||   101.608523 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
