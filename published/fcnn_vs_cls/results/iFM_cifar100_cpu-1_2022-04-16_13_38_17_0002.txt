Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      5.856764 ||   0.1657 ||   0.3787 ||  0.747738 ||  0.751310 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.778737 ||   0.1536 ||   0.3688 ||  0.743812 ||  0.750360 ||      3.621695 ||      0.1774 ||   0.4101 ||  0.836690 ||  0.840815 ||    18.660044 || 
    Epoch 01    --      3.404294 ||   0.2181 ||   0.4705 ||  0.858365 ||  0.865841 ||      3.411586 ||      0.2142 ||   0.4701 ||  0.863268 ||  0.865847 ||    17.350754 || 
    Epoch 02    --      3.220106 ||   0.2506 ||   0.5179 ||  0.878162 ||  0.884709 ||      3.256979 ||      0.2523 ||   0.5140 ||  0.875942 ||  0.879793 ||    17.233156 || 
    Epoch 03    --      3.077416 ||   0.2831 ||   0.5543 ||  0.890857 ||  0.896614 ||      3.120411 ||      0.2693 ||   0.5400 ||  0.890637 ||  0.893526 ||    17.143926 || 
    Epoch 04    --      2.953631 ||   0.3049 ||   0.5818 ||  0.902108 ||  0.907442 ||      3.005838 ||      0.2984 ||   0.5671 ||  0.901070 ||  0.902333 ||    17.107201 || 
    Epoch 05    --      2.835519 ||   0.3299 ||   0.6088 ||  0.911178 ||  0.915955 ||      2.874755 ||      0.3179 ||   0.5954 ||  0.911698 ||  0.913456 ||    17.087037 || 
    Epoch 06    --      2.715775 ||   0.3519 ||   0.6368 ||  0.920115 ||  0.924276 ||      2.802250 ||      0.3303 ||   0.6154 ||  0.916844 ||  0.918719 ||    16.833150 || 
    Epoch 07    --      2.612054 ||   0.3745 ||   0.6580 ||  0.927167 ||  0.930955 ||      2.664406 ||      0.3596 ||   0.6464 ||  0.926677 ||  0.927828 ||    16.586334 || 
    Epoch 08    --      2.500909 ||   0.3979 ||   0.6804 ||  0.934105 ||  0.937712 ||      2.581025 ||      0.3819 ||   0.6630 ||  0.931391 ||  0.932253 ||    17.224278 || 
    Epoch 09    --      2.392893 ||   0.4219 ||   0.7006 ||  0.940383 ||  0.943560 ||      2.453590 ||      0.4021 ||   0.6919 ||  0.940171 ||  0.940793 ||    17.173724 || 
    Epoch 10    --      2.280020 ||   0.4442 ||   0.7237 ||  0.946512 ||  0.949534 ||      2.362391 ||      0.4303 ||   0.7088 ||  0.943361 ||  0.944619 ||    16.748883 || 
    Epoch 11    --      2.178489 ||   0.4697 ||   0.7416 ||  0.951586 ||  0.954262 ||      2.298793 ||      0.4414 ||   0.7167 ||  0.948434 ||  0.947941 ||    16.781134 || 
    Epoch 12    --      2.084786 ||   0.4892 ||   0.7597 ||  0.955802 ||  0.958288 ||      2.173450 ||      0.4659 ||   0.7425 ||  0.953879 ||  0.953752 ||    17.393897 || 
    Epoch 13    --      1.993069 ||   0.5104 ||   0.7753 ||  0.959667 ||  0.962066 ||      2.094889 ||      0.4745 ||   0.7595 ||  0.958632 ||  0.957983 ||    16.554942 || 
    Epoch 14    --      1.891472 ||   0.5355 ||   0.7958 ||  0.964109 ||  0.966164 ||      1.992953 ||      0.5025 ||   0.7803 ||  0.961529 ||  0.961752 ||    16.860307 || 
    Epoch 15    --      1.802881 ||   0.5552 ||   0.8092 ||  0.967566 ||  0.969455 ||      1.884652 ||      0.5333 ||   0.7957 ||  0.966630 ||  0.967168 ||    16.928205 || 
    Epoch 16    --      1.716187 ||   0.5772 ||   0.8239 ||  0.970694 ||  0.972487 ||      1.812546 ||      0.5455 ||   0.8084 ||  0.968971 ||  0.969330 ||    16.841135 || 
    Epoch 17    --      1.637537 ||   0.5926 ||   0.8357 ||  0.973809 ||  0.975497 ||      1.709640 ||      0.5691 ||   0.8195 ||  0.972397 ||  0.972537 ||    16.469826 || 
    Epoch 18    --      1.559881 ||   0.6131 ||   0.8490 ||  0.976268 ||  0.977772 ||      1.634658 ||      0.5890 ||   0.8369 ||  0.974856 ||  0.975127 ||    16.895145 || 
    Epoch 19    --      1.475380 ||   0.6346 ||   0.8613 ||  0.978860 ||  0.980242 ||      1.587639 ||      0.6018 ||   0.8448 ||  0.976395 ||  0.976668 ||    16.496692 || 
    Epoch 20    --      1.403670 ||   0.6519 ||   0.8727 ||  0.980944 ||  0.982287 ||      1.519077 ||      0.6135 ||   0.8540 ||  0.978414 ||  0.978403 ||    16.724523 || 
    Epoch 21    --      1.336020 ||   0.6726 ||   0.8817 ||  0.982440 ||  0.983639 ||      1.412939 ||      0.6321 ||   0.8682 ||  0.981952 ||  0.982172 ||    16.965298 || 
    Epoch 22    --      1.264342 ||   0.6882 ||   0.8916 ||  0.984465 ||  0.985560 ||      1.367520 ||      0.6508 ||   0.8783 ||  0.982756 ||  0.983131 ||    16.947155 || 
    Epoch 23    --      1.202274 ||   0.7043 ||   0.9018 ||  0.986056 ||  0.987057 ||      1.305736 ||      0.6709 ||   0.8843 ||  0.983637 ||  0.983993 ||    16.796628 || 
    Epoch 24    --      1.146184 ||   0.7172 ||   0.9079 ||  0.987635 ||  0.988598 ||      1.233455 ||      0.6831 ||   0.9019 ||  0.985645 ||  0.985972 ||    17.160550 || 
    Epoch 25    --      1.082709 ||   0.7350 ||   0.9154 ||  0.988642 ||  0.989490 ||      1.189042 ||      0.6989 ||   0.9031 ||  0.987345 ||  0.987511 ||    16.962979 || 
    Epoch 26    --      1.027734 ||   0.7477 ||   0.9240 ||  0.989901 ||  0.990686 ||      1.145980 ||      0.7051 ||   0.9092 ||  0.988531 ||  0.988438 ||    16.414541 || 
    Epoch 27    --      0.971971 ||   0.7644 ||   0.9306 ||  0.990946 ||  0.991692 ||      1.079676 ||      0.7293 ||   0.9180 ||  0.989305 ||  0.989613 ||    17.230134 || 
    Epoch 28    --      0.921052 ||   0.7768 ||   0.9367 ||  0.991778 ||  0.992501 ||      1.047734 ||      0.7323 ||   0.9262 ||  0.990315 ||  0.990391 ||    16.727119 || 
    Epoch 29    --      0.879660 ||   0.7865 ||   0.9420 ||  0.992553 ||  0.993197 ||      0.979716 ||      0.7523 ||   0.9341 ||  0.991528 ||  0.991582 ||    16.803788 || 
    Epoch 30    --      0.828636 ||   0.8010 ||   0.9480 ||  0.993435 ||  0.994015 ||      0.933832 ||      0.7622 ||   0.9384 ||  0.992295 ||  0.992334 ||    16.561751 || 
    Epoch 31    --      0.788322 ||   0.8119 ||   0.9517 ||  0.993979 ||  0.994509 ||      0.836590 ||      0.7953 ||   0.9469 ||  0.994014 ||  0.994221 ||    16.964892 || 
    Epoch 32    --      0.742361 ||   0.8266 ||   0.9562 ||  0.994799 ||  0.995283 ||      0.813594 ||      0.7953 ||   0.9508 ||  0.994370 ||  0.994580 ||    16.884139 || 
    Epoch 33    --      0.741322 ||   0.8232 ||   0.9573 ||  0.994936 ||  0.995413 ||      0.814957 ||      0.8034 ||   0.9517 ||  0.993839 ||  0.993968 ||    16.653963 || 
    Epoch 34    --      0.699957 ||   0.8377 ||   0.9600 ||  0.995307 ||  0.995760 ||      0.784272 ||      0.8018 ||   0.9550 ||  0.995140 ||  0.995119 ||    16.992928 || 
    Epoch 35    --      0.698602 ||   0.8353 ||   0.9604 ||  0.995347 ||  0.995773 ||      0.813134 ||      0.7877 ||   0.9542 ||  0.994878 ||  0.994790 ||    16.953638 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
