Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     39.661461 ||   0.1669 ||   0.3923 ||  0.602112 ||  0.602155 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.343452 ||   0.0978 ||   0.2577 ||  0.624151 ||  0.625642 ||     11.718019 ||      0.1419 ||   0.3500 ||  0.657214 ||  0.655674 ||    86.905384 || 
    Epoch 01    --     10.077794 ||   0.2172 ||   0.4675 ||  0.704363 ||  0.705112 ||     11.289859 ||      0.1863 ||   0.4358 ||  0.682811 ||  0.682204 ||    84.405385 || 
    Epoch 02    --      9.407158 ||   0.2690 ||   0.5390 ||  0.730026 ||  0.730672 ||     10.694406 ||      0.2408 ||   0.5116 ||  0.709256 ||  0.709195 ||    84.655407 || 
    Epoch 03    --      8.795057 ||   0.3153 ||   0.6012 ||  0.753491 ||  0.754013 ||      9.849020 ||      0.2945 ||   0.5727 ||  0.733081 ||  0.732372 ||    84.811629 || 
    Epoch 04    --      8.089630 ||   0.3657 ||   0.6557 ||  0.776812 ||  0.777363 ||      9.382689 ||      0.3292 ||   0.6195 ||  0.752428 ||  0.751455 ||    84.952281 || 
    Epoch 05    --      7.585818 ||   0.4022 ||   0.6974 ||  0.792296 ||  0.792742 ||      8.436538 ||      0.3803 ||   0.6737 ||  0.775879 ||  0.775554 ||    84.401077 || 
    Epoch 06    --      6.950980 ||   0.4460 ||   0.7399 ||  0.812293 ||  0.812555 ||      8.082018 ||      0.4034 ||   0.7022 ||  0.785898 ||  0.786405 ||    85.405370 || 
    Epoch 07    --      6.547776 ||   0.4757 ||   0.7667 ||  0.823674 ||  0.823933 ||      7.302415 ||      0.4433 ||   0.7418 ||  0.807680 ||  0.807493 ||    84.796048 || 
    Epoch 08    --      6.030545 ||   0.5102 ||   0.8024 ||  0.839649 ||  0.839843 ||      7.383307 ||      0.4635 ||   0.7554 ||  0.814234 ||  0.812577 ||    84.577278 || 
    Epoch 09    --      5.668808 ||   0.5405 ||   0.8227 ||  0.849504 ||  0.849741 ||      6.519676 ||      0.4999 ||   0.7981 ||  0.832422 ||  0.830991 ||    85.139795 || 
    Epoch 10    --      5.254151 ||   0.5687 ||   0.8423 ||  0.860906 ||  0.861149 ||      6.292223 ||      0.5120 ||   0.8180 ||  0.839856 ||  0.838431 ||    84.881187 || 
    Epoch 11    --      4.947203 ||   0.5871 ||   0.8604 ||  0.868517 ||  0.868794 ||      5.464264 ||      0.5664 ||   0.8441 ||  0.856904 ||  0.857160 ||    84.842929 || 
    Epoch 12    --      4.534702 ||   0.6144 ||   0.8798 ||  0.879880 ||  0.880098 ||      5.995244 ||      0.5579 ||   0.8467 ||  0.850953 ||  0.849216 ||    84.755837 || 
    Epoch 13    --      4.652529 ||   0.6093 ||   0.8761 ||  0.876852 ||  0.877066 ||      5.883340 ||      0.5719 ||   0.8441 ||  0.854940 ||  0.854064 ||    85.311641 || 
    Epoch 14    --      4.328195 ||   0.6363 ||   0.8906 ||  0.886755 ||  0.886901 ||      5.599154 ||      0.5756 ||   0.8610 ||  0.860051 ||  0.859102 ||    84.967894 || 
    Epoch 15    --      4.069252 ||   0.6506 ||   0.9039 ||  0.892003 ||  0.892170 ||      4.647909 ||      0.6260 ||   0.8937 ||  0.879710 ||  0.879523 ||    84.796032 || 
    Epoch 16    --      3.852225 ||   0.6687 ||   0.9140 ||  0.899439 ||  0.899588 ||      4.481527 ||      0.6416 ||   0.8991 ||  0.884951 ||  0.885942 ||    84.971324 || 
    Epoch 17    --      3.676566 ||   0.6881 ||   0.9228 ||  0.904099 ||  0.904315 ||      4.086631 ||      0.6603 ||   0.9132 ||  0.894041 ||  0.893801 ||    85.061661 || 
    Epoch 18    --      3.483430 ||   0.7007 ||   0.9294 ||  0.908313 ||  0.908546 ||      4.016482 ||      0.6670 ||   0.9130 ||  0.894861 ||  0.894987 ||    85.382451 || 
    Epoch 19    --      3.245138 ||   0.7159 ||   0.9385 ||  0.915462 ||  0.915633 ||      4.162888 ||      0.6581 ||   0.9178 ||  0.893536 ||  0.893167 ||    85.483530 || 
    Epoch 20    --      3.247719 ||   0.7113 ||   0.9377 ||  0.914284 ||  0.914400 ||      3.776771 ||      0.6817 ||   0.9268 ||  0.902648 ||  0.902349 ||    86.342883 || 
    Epoch 21    --      3.150217 ||   0.7259 ||   0.9436 ||  0.917694 ||  0.917810 ||      3.938511 ||      0.6839 ||   0.9318 ||  0.899419 ||  0.899798 ||    86.436688 || 
    Epoch 22    --      2.955574 ||   0.7402 ||   0.9503 ||  0.922832 ||  0.922940 ||      3.472623 ||      0.7101 ||   0.9429 ||  0.910294 ||  0.909958 ||    85.330998 || 
    Epoch 23    --      2.814899 ||   0.7510 ||   0.9547 ||  0.926668 ||  0.926802 ||      3.198053 ||      0.7297 ||   0.9443 ||  0.916424 ||  0.917049 ||    85.858525 || 
    Epoch 24    --      2.724075 ||   0.7581 ||   0.9585 ||  0.929961 ||  0.929984 ||      3.403836 ||      0.7222 ||   0.9484 ||  0.912530 ||  0.913218 ||    86.173177 || 
    Epoch 25    --      2.718319 ||   0.7604 ||   0.9580 ||  0.928706 ||  0.928837 ||      3.355056 ||      0.7265 ||   0.9458 ||  0.915945 ||  0.914274 ||    87.233528 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
