Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.460731 ||   0.1631 ||   0.3668 ||  0.710273 ||  0.712763 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.768955 ||   0.1532 ||   0.3701 ||  0.747252 ||  0.753558 ||      3.614866 ||      0.1758 ||   0.4094 ||  0.837385 ||  0.841980 ||    18.411557 || 
    Epoch 01    --      3.390126 ||   0.2211 ||   0.4742 ||  0.859961 ||  0.867426 ||      3.395745 ||      0.2188 ||   0.4749 ||  0.865408 ||  0.868147 ||    16.810425 || 
    Epoch 02    --      3.206011 ||   0.2551 ||   0.5201 ||  0.879594 ||  0.886214 ||      3.239388 ||      0.2557 ||   0.5153 ||  0.877854 ||  0.881705 ||    16.788586 || 
    Epoch 03    --      3.063072 ||   0.2849 ||   0.5584 ||  0.892336 ||  0.898213 ||      3.112062 ||      0.2692 ||   0.5410 ||  0.891475 ||  0.894425 ||    17.170835 || 
    Epoch 04    --      2.935743 ||   0.3080 ||   0.5830 ||  0.903662 ||  0.908956 ||      2.992165 ||      0.3026 ||   0.5703 ||  0.902349 ||  0.903654 ||    16.815935 || 
    Epoch 05    --      2.814782 ||   0.3325 ||   0.6133 ||  0.912778 ||  0.917564 ||      2.858679 ||      0.3186 ||   0.6000 ||  0.912689 ||  0.914918 ||    16.555435 || 
    Epoch 06    --      2.691797 ||   0.3561 ||   0.6414 ||  0.921621 ||  0.925746 ||      2.778300 ||      0.3342 ||   0.6196 ||  0.918716 ||  0.920708 ||    16.563529 || 
    Epoch 07    --      2.585154 ||   0.3803 ||   0.6638 ||  0.928953 ||  0.932752 ||      2.637273 ||      0.3656 ||   0.6516 ||  0.928753 ||  0.929942 ||    16.838484 || 
    Epoch 08    --      2.472135 ||   0.4044 ||   0.6884 ||  0.935655 ||  0.939249 ||      2.541162 ||      0.3879 ||   0.6692 ||  0.933781 ||  0.935042 ||    16.588940 || 
    Epoch 09    --      2.361954 ||   0.4288 ||   0.7092 ||  0.941846 ||  0.945114 ||      2.432846 ||      0.4042 ||   0.6891 ||  0.942598 ||  0.942648 ||    16.612344 || 
    Epoch 10    --      2.249321 ||   0.4504 ||   0.7276 ||  0.948228 ||  0.951193 ||      2.316694 ||      0.4315 ||   0.7212 ||  0.946912 ||  0.948005 ||    17.134552 || 
    Epoch 11    --      2.147621 ||   0.4734 ||   0.7491 ||  0.953457 ||  0.956120 ||      2.257971 ||      0.4449 ||   0.7277 ||  0.950040 ||  0.949955 ||    16.862546 || 
    Epoch 12    --      2.050021 ||   0.4972 ||   0.7694 ||  0.957579 ||  0.960045 ||      2.146771 ||      0.4705 ||   0.7452 ||  0.955691 ||  0.955438 ||    16.947456 || 
    Epoch 13    --      1.957903 ||   0.5192 ||   0.7826 ||  0.961711 ||  0.963999 ||      2.079254 ||      0.4811 ||   0.7651 ||  0.959407 ||  0.958749 ||    16.865583 || 
    Epoch 14    --      1.859797 ||   0.5430 ||   0.8003 ||  0.965564 ||  0.967586 ||      1.964608 ||      0.5095 ||   0.7810 ||  0.963573 ||  0.963865 ||    16.923982 || 
    Epoch 15    --      1.774863 ||   0.5621 ||   0.8138 ||  0.968888 ||  0.970799 ||      1.856241 ||      0.5342 ||   0.8007 ||  0.967736 ||  0.968356 ||    16.982987 || 
    Epoch 16    --      1.685103 ||   0.5812 ||   0.8288 ||  0.972278 ||  0.973981 ||      1.805123 ||      0.5504 ||   0.8086 ||  0.968802 ||  0.969186 ||    17.011018 || 
    Epoch 17    --      1.611678 ||   0.5988 ||   0.8397 ||  0.974529 ||  0.976174 ||      1.701445 ||      0.5711 ||   0.8243 ||  0.973158 ||  0.973441 ||    17.206735 || 
    Epoch 18    --      1.540897 ||   0.6172 ||   0.8518 ||  0.976656 ||  0.978157 ||      1.617836 ||      0.5884 ||   0.8377 ||  0.976075 ||  0.976336 ||    17.132955 || 
    Epoch 19    --      1.456747 ||   0.6371 ||   0.8638 ||  0.979338 ||  0.980739 ||      1.581152 ||      0.6064 ||   0.8440 ||  0.976566 ||  0.976481 ||    18.062046 || 
    Epoch 20    --      1.382774 ||   0.6571 ||   0.8752 ||  0.981631 ||  0.982951 ||      1.507401 ||      0.6186 ||   0.8531 ||  0.978114 ||  0.978176 ||    14.459974 || 
    Epoch 21    --      1.321255 ||   0.6746 ||   0.8833 ||  0.982829 ||  0.984035 ||      1.409785 ||      0.6397 ||   0.8692 ||  0.982157 ||  0.982129 ||    15.355653 || 
    Epoch 22    --      1.253616 ||   0.6917 ||   0.8928 ||  0.984831 ||  0.985898 ||      1.354440 ||      0.6557 ||   0.8803 ||  0.982623 ||  0.982880 ||    13.927283 || 
    Epoch 23    --      1.194883 ||   0.7076 ||   0.9026 ||  0.985983 ||  0.986984 ||      1.295395 ||      0.6686 ||   0.8882 ||  0.984546 ||  0.984836 ||    14.837642 || 
    Epoch 24    --      1.137202 ||   0.7182 ||   0.9095 ||  0.987735 ||  0.988664 ||      1.228146 ||      0.6911 ||   0.8983 ||  0.985594 ||  0.985980 ||    13.415040 || 
    Epoch 25    --      1.080166 ||   0.7350 ||   0.9172 ||  0.988618 ||  0.989454 ||      1.163601 ||      0.7025 ||   0.9080 ||  0.988122 ||  0.988501 ||    13.680889 || 
    Epoch 26    --      1.024230 ||   0.7479 ||   0.9238 ||  0.990155 ||  0.990953 ||      1.131378 ||      0.7080 ||   0.9109 ||  0.989058 ||  0.988972 ||    13.276728 || 
    Epoch 27    --      0.967683 ||   0.7648 ||   0.9301 ||  0.991103 ||  0.991831 ||      1.084581 ||      0.7292 ||   0.9156 ||  0.989377 ||  0.989628 ||    13.475100 || 
    Epoch 28    --      0.921324 ||   0.7753 ||   0.9351 ||  0.991989 ||  0.992687 ||      1.044322 ||      0.7321 ||   0.9253 ||  0.990262 ||  0.990471 ||    14.123266 || 
    Epoch 29    --      0.878451 ||   0.7888 ||   0.9402 ||  0.992782 ||  0.993445 ||      0.981193 ||      0.7500 ||   0.9342 ||  0.991471 ||  0.991309 ||    13.952925 || 
    Epoch 30    --      0.833562 ||   0.8016 ||   0.9447 ||  0.993411 ||  0.993987 ||      0.917156 ||      0.7654 ||   0.9392 ||  0.993013 ||  0.993199 ||    12.820371 || 
    Epoch 31    --      0.788604 ||   0.8124 ||   0.9507 ||  0.994247 ||  0.994773 ||      0.852534 ||      0.7930 ||   0.9450 ||  0.993157 ||  0.993505 ||    12.463054 || 
    Epoch 32    --      0.747384 ||   0.8228 ||   0.9556 ||  0.994709 ||  0.995188 ||      0.827260 ||      0.7956 ||   0.9466 ||  0.994378 ||  0.994472 ||    13.055693 || 
    Epoch 33    --      0.703529 ||   0.8327 ||   0.9606 ||  0.995390 ||  0.995846 ||      0.790547 ||      0.8114 ||   0.9520 ||  0.994589 ||  0.994623 ||    13.279285 || 
    Epoch 34    --      0.666668 ||   0.8451 ||   0.9631 ||  0.995785 ||  0.996202 ||      0.751798 ||      0.8152 ||   0.9558 ||  0.995560 ||  0.995524 ||    13.725146 || 
    Epoch 35    --      0.634408 ||   0.8529 ||   0.9673 ||  0.996262 ||  0.996638 ||      0.718081 ||      0.8200 ||   0.9624 ||  0.995918 ||  0.995869 ||    13.414414 || 
    Epoch 36    --      0.592988 ||   0.8646 ||   0.9705 ||  0.996710 ||  0.997026 ||      0.692236 ||      0.8332 ||   0.9612 ||  0.996032 ||  0.996056 ||    14.101340 || 
    Epoch 37    --      0.560985 ||   0.8759 ||   0.9736 ||  0.997167 ||  0.997484 ||      0.671044 ||      0.8325 ||   0.9645 ||  0.996394 ||  0.996255 ||    13.818002 || 
    Epoch 38    --      0.560826 ||   0.8752 ||   0.9728 ||  0.997083 ||  0.997388 ||      0.641382 ||      0.8500 ||   0.9677 ||  0.996398 ||  0.996419 ||    14.504812 || 
    Epoch 39    --      0.534711 ||   0.8815 ||   0.9761 ||  0.997329 ||  0.997625 ||      0.590806 ||      0.8619 ||   0.9720 ||  0.997318 ||  0.997420 ||    13.779580 || 
    Epoch 40    --      0.501869 ||   0.8919 ||   0.9776 ||  0.997738 ||  0.998011 ||      0.575447 ||      0.8637 ||   0.9741 ||  0.997503 ||  0.997533 ||    13.588144 || 
    Epoch 41    --      0.474645 ||   0.8974 ||   0.9806 ||  0.998049 ||  0.998278 ||      0.546438 ||      0.8697 ||   0.9766 ||  0.997723 ||  0.997689 ||    13.947742 || 
    Epoch 42    --      0.443501 ||   0.9068 ||   0.9832 ||  0.998210 ||  0.998417 ||      0.532143 ||      0.8761 ||   0.9769 ||  0.997617 ||  0.997672 ||    14.437677 || 
    Epoch 43    --      0.422408 ||   0.9135 ||   0.9844 ||  0.998392 ||  0.998576 ||      0.477696 ||      0.8944 ||   0.9811 ||  0.998275 ||  0.998358 ||    14.772386 || 
    Epoch 44    --      0.396765 ||   0.9186 ||   0.9866 ||  0.998712 ||  0.998876 ||      0.454864 ||      0.9008 ||   0.9824 ||  0.998442 ||  0.998473 ||    14.995494 || 
    Epoch 45    --      0.375515 ||   0.9243 ||   0.9877 ||  0.998856 ||  0.998998 ||      0.424635 ||      0.9080 ||   0.9854 ||  0.998286 ||  0.998326 ||    14.130744 || 
    Epoch 46    --      0.355078 ||   0.9312 ||   0.9884 ||  0.998912 ||  0.999047 ||      0.387314 ||      0.9227 ||   0.9867 ||  0.998783 ||  0.998835 ||    14.603668 || 
    Epoch 47    --      0.334559 ||   0.9344 ||   0.9901 ||  0.999093 ||  0.999215 ||      0.376675 ||      0.9210 ||   0.9886 ||  0.998972 ||  0.999014 ||    13.966048 || 
    Epoch 48    --      0.330566 ||   0.9362 ||   0.9901 ||  0.999162 ||  0.999282 ||      0.389427 ||      0.9152 ||   0.9887 ||  0.998743 ||  0.998778 ||    14.520186 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
