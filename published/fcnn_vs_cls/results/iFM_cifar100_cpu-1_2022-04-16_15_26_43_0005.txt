Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      6.278426 ||   0.1692 ||   0.3819 ||  0.739311 ||  0.742544 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.786149 ||   0.1544 ||   0.3727 ||  0.745353 ||  0.751783 ||      3.626368 ||      0.1747 ||   0.4106 ||  0.836427 ||  0.841185 ||    18.209574 || 
    Epoch 01    --      3.391320 ||   0.2204 ||   0.4751 ||  0.860118 ||  0.867546 ||      3.397231 ||      0.2177 ||   0.4730 ||  0.865574 ||  0.868264 ||    16.911155 || 
    Epoch 02    --      3.201703 ||   0.2544 ||   0.5213 ||  0.880421 ||  0.886757 ||      3.241808 ||      0.2540 ||   0.5136 ||  0.877715 ||  0.881594 ||    17.137824 || 
    Epoch 03    --      3.057401 ||   0.2853 ||   0.5590 ||  0.892799 ||  0.898471 ||      3.102361 ||      0.2765 ||   0.5411 ||  0.891945 ||  0.894722 ||    17.151358 || 
    Epoch 04    --      2.931864 ||   0.3088 ||   0.5850 ||  0.904108 ||  0.909272 ||      2.983083 ||      0.3039 ||   0.5749 ||  0.902724 ||  0.903769 ||    16.854861 || 
    Epoch 05    --      2.811791 ||   0.3313 ||   0.6135 ||  0.913022 ||  0.917711 ||      2.846937 ||      0.3242 ||   0.6043 ||  0.913416 ||  0.915218 ||    17.024473 || 
    Epoch 06    --      2.689807 ||   0.3570 ||   0.6409 ||  0.921925 ||  0.925926 ||      2.775057 ||      0.3382 ||   0.6232 ||  0.918003 ||  0.920065 ||    16.797573 || 
    Epoch 07    --      2.587921 ||   0.3797 ||   0.6626 ||  0.928594 ||  0.932370 ||      2.636750 ||      0.3665 ||   0.6514 ||  0.928351 ||  0.929875 ||    16.746509 || 
    Epoch 08    --      2.479332 ||   0.3999 ||   0.6831 ||  0.935351 ||  0.938877 ||      2.553403 ||      0.3844 ||   0.6660 ||  0.932749 ||  0.933805 ||    16.811017 || 
    Epoch 09    --      2.369122 ||   0.4263 ||   0.7060 ||  0.941590 ||  0.944791 ||      2.449609 ||      0.4012 ||   0.6867 ||  0.940507 ||  0.941103 ||    17.151622 || 
    Epoch 10    --      2.263571 ||   0.4517 ||   0.7250 ||  0.947131 ||  0.950033 ||      2.322105 ||      0.4330 ||   0.7166 ||  0.946571 ||  0.947712 ||    16.974011 || 
    Epoch 11    --      2.161424 ||   0.4705 ||   0.7463 ||  0.952622 ||  0.955189 ||      2.282867 ||      0.4421 ||   0.7195 ||  0.948560 ||  0.948447 ||    16.806947 || 
    Epoch 12    --      2.066742 ||   0.4942 ||   0.7614 ||  0.956901 ||  0.959374 ||      2.153779 ||      0.4720 ||   0.7421 ||  0.954555 ||  0.954600 ||    16.935319 || 
    Epoch 13    --      1.974603 ||   0.5152 ||   0.7793 ||  0.960754 ||  0.963122 ||      2.088264 ||      0.4835 ||   0.7601 ||  0.959477 ||  0.958887 ||    16.925279 || 
    Epoch 14    --      1.874764 ||   0.5388 ||   0.7964 ||  0.964879 ||  0.966927 ||      1.991270 ||      0.5034 ||   0.7751 ||  0.961622 ||  0.961842 ||    17.200625 || 
    Epoch 15    --      1.792479 ||   0.5583 ||   0.8102 ||  0.968071 ||  0.969943 ||      1.866593 ||      0.5344 ||   0.7998 ||  0.966891 ||  0.967520 ||    16.780203 || 
    Epoch 16    --      1.701849 ||   0.5805 ||   0.8263 ||  0.971467 ||  0.973216 ||      1.814398 ||      0.5431 ||   0.8072 ||  0.968657 ||  0.969034 ||    16.598654 || 
    Epoch 17    --      1.626144 ||   0.5988 ||   0.8371 ||  0.974028 ||  0.975705 ||      1.702009 ||      0.5747 ||   0.8212 ||  0.972863 ||  0.973227 ||    16.905611 || 
    Epoch 18    --      1.548867 ||   0.6149 ||   0.8511 ||  0.976599 ||  0.978128 ||      1.649840 ||      0.5868 ||   0.8343 ||  0.974932 ||  0.975123 ||    16.800208 || 
    Epoch 19    --      1.468114 ||   0.6375 ||   0.8618 ||  0.978880 ||  0.980293 ||      1.607040 ||      0.5967 ||   0.8433 ||  0.975929 ||  0.975751 ||    17.020038 || 
    Epoch 20    --      1.395801 ||   0.6547 ||   0.8735 ||  0.981108 ||  0.982469 ||      1.500931 ||      0.6224 ||   0.8572 ||  0.978416 ||  0.978542 ||    17.369694 || 
    Epoch 21    --      1.332489 ||   0.6704 ||   0.8823 ||  0.982457 ||  0.983650 ||      1.425108 ||      0.6309 ||   0.8710 ||  0.982304 ||  0.982127 ||    17.289402 || 
    Epoch 22    --      1.262093 ||   0.6877 ||   0.8928 ||  0.984580 ||  0.985668 ||      1.364450 ||      0.6542 ||   0.8775 ||  0.982485 ||  0.982886 ||    17.097967 || 
    Epoch 23    --      1.199760 ||   0.7064 ||   0.9011 ||  0.985683 ||  0.986685 ||      1.294573 ||      0.6702 ||   0.8831 ||  0.985031 ||  0.985230 ||    17.118616 || 
    Epoch 24    --      1.145851 ||   0.7174 ||   0.9082 ||  0.987559 ||  0.988488 ||      1.222213 ||      0.6893 ||   0.9007 ||  0.985577 ||  0.986069 ||    17.353832 || 
    Epoch 25    --      1.083966 ||   0.7345 ||   0.9164 ||  0.988296 ||  0.989184 ||      1.170490 ||      0.7015 ||   0.9084 ||  0.988056 ||  0.988255 ||    17.049773 || 
    Epoch 26    --      1.024403 ||   0.7474 ||   0.9231 ||  0.990012 ||  0.990792 ||      1.136011 ||      0.7048 ||   0.9140 ||  0.989120 ||  0.988994 ||    16.678654 || 
    Epoch 27    --      0.966063 ||   0.7666 ||   0.9298 ||  0.991150 ||  0.991902 ||      1.089707 ||      0.7313 ||   0.9160 ||  0.988889 ||  0.989182 ||    17.154612 || 
    Epoch 28    --      0.921029 ||   0.7776 ||   0.9361 ||  0.991889 ||  0.992586 ||      1.052410 ||      0.7272 ||   0.9203 ||  0.990679 ||  0.990770 ||    16.995986 || 
    Epoch 29    --      0.927488 ||   0.7745 ||   0.9366 ||  0.991862 ||  0.992550 ||      1.018428 ||      0.7425 ||   0.9265 ||  0.991321 ||  0.991429 ||    16.913003 || 
    Epoch 30    --      0.876510 ||   0.7890 ||   0.9409 ||  0.992646 ||  0.993290 ||      0.943844 ||      0.7616 ||   0.9354 ||  0.991736 ||  0.992057 ||    16.778658 || 
    Epoch 31    --      0.829542 ||   0.8032 ||   0.9466 ||  0.993462 ||  0.994044 ||      0.902341 ||      0.7749 ||   0.9431 ||  0.992679 ||  0.992934 ||    16.903658 || 
    Epoch 32    --      0.783008 ||   0.8159 ||   0.9502 ||  0.994210 ||  0.994760 ||      0.868170 ||      0.7786 ||   0.9441 ||  0.993239 ||  0.993462 ||    16.446153 || 
    Epoch 33    --      0.737627 ||   0.8234 ||   0.9569 ||  0.995017 ||  0.995503 ||      0.823680 ||      0.7988 ||   0.9460 ||  0.994061 ||  0.994209 ||    16.758160 || 
    Epoch 34    --      0.701367 ||   0.8377 ||   0.9590 ||  0.995225 ||  0.995677 ||      0.761403 ||      0.8173 ||   0.9559 ||  0.995453 ||  0.995495 ||    16.808563 || 
    Epoch 35    --      0.661117 ||   0.8473 ||   0.9637 ||  0.995879 ||  0.996283 ||      0.765060 ||      0.8041 ||   0.9580 ||  0.995602 ||  0.995505 ||    17.131178 || 
    Epoch 36    --      0.661490 ||   0.8456 ||   0.9634 ||  0.995833 ||  0.996235 ||      0.734927 ||      0.8215 ||   0.9589 ||  0.995772 ||  0.995798 ||    16.702950 || 
    Epoch 37    --      0.621538 ||   0.8589 ||   0.9685 ||  0.996463 ||  0.996836 ||      0.727591 ||      0.8197 ||   0.9576 ||  0.995540 ||  0.995562 ||    17.481078 || 
    Epoch 38    --      0.620006 ||   0.8587 ||   0.9689 ||  0.996386 ||  0.996750 ||      0.721642 ||      0.8258 ||   0.9604 ||  0.995395 ||  0.995568 ||    16.746002 || 
    Epoch 39    --      0.590716 ||   0.8668 ||   0.9707 ||  0.996897 ||  0.997248 ||      0.645701 ||      0.8508 ||   0.9679 ||  0.996598 ||  0.996737 ||    16.950691 || 
    Epoch 40    --      0.552076 ||   0.8791 ||   0.9728 ||  0.997336 ||  0.997641 ||      0.647173 ||      0.8407 ||   0.9698 ||  0.996832 ||  0.996811 ||    16.756631 || 
    Epoch 41    --      0.551685 ||   0.8766 ||   0.9744 ||  0.997357 ||  0.997649 ||      0.645559 ||      0.8422 ||   0.9671 ||  0.996755 ||  0.996668 ||    17.209281 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
