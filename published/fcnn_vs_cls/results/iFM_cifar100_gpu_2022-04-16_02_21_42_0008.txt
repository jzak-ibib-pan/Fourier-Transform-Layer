Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.182943 ||   0.1596 ||   0.3733 ||  0.715098 ||  0.717472 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.778647 ||   0.1528 ||   0.3719 ||  0.744918 ||  0.751306 ||      3.628057 ||      0.1756 ||   0.4078 ||  0.836185 ||  0.840785 ||    15.162224 || 
    Epoch 01    --      3.393181 ||   0.2224 ||   0.4742 ||  0.860063 ||  0.867579 ||      3.398062 ||      0.2206 ||   0.4738 ||  0.864931 ||  0.867877 ||    14.146055 || 
    Epoch 02    --      3.208750 ||   0.2548 ||   0.5229 ||  0.879662 ||  0.886190 ||      3.236411 ||      0.2547 ||   0.5159 ||  0.878366 ||  0.882281 ||    14.907614 || 
    Epoch 03    --      3.061269 ||   0.2851 ||   0.5597 ||  0.892449 ||  0.898214 ||      3.097944 ||      0.2698 ||   0.5460 ||  0.892980 ||  0.896008 ||    15.072721 || 
    Epoch 04    --      2.927871 ||   0.3103 ||   0.5871 ||  0.904290 ||  0.909615 ||      2.979956 ||      0.3020 ||   0.5748 ||  0.903071 ||  0.904316 ||    13.533370 || 
    Epoch 05    --      2.802979 ||   0.3348 ||   0.6150 ||  0.913639 ||  0.918357 ||      2.833839 ||      0.3232 ||   0.6066 ||  0.914230 ||  0.916458 ||    13.082193 || 
    Epoch 06    --      2.674361 ||   0.3612 ||   0.6460 ||  0.922945 ||  0.927084 ||      2.753318 ||      0.3405 ||   0.6248 ||  0.919631 ||  0.921695 ||    14.347121 || 
    Epoch 07    --      2.565139 ||   0.3839 ||   0.6675 ||  0.930202 ||  0.933908 ||      2.610024 ||      0.3724 ||   0.6583 ||  0.930151 ||  0.931435 ||    14.364026 || 
    Epoch 08    --      2.450535 ||   0.4084 ||   0.6924 ||  0.937087 ||  0.940540 ||      2.520783 ||      0.3925 ||   0.6776 ||  0.935766 ||  0.936758 ||    12.511359 || 
    Epoch 09    --      2.335677 ||   0.4338 ||   0.7144 ||  0.943511 ||  0.946616 ||      2.411474 ||      0.4051 ||   0.6969 ||  0.942769 ||  0.943213 ||    13.187049 || 
    Epoch 10    --      2.225343 ||   0.4579 ||   0.7343 ||  0.949033 ||  0.951989 ||      2.291939 ||      0.4331 ||   0.7264 ||  0.948660 ||  0.949658 ||    14.139822 || 
    Epoch 11    --      2.122820 ||   0.4780 ||   0.7529 ||  0.954205 ||  0.956794 ||      2.242397 ||      0.4489 ||   0.7297 ||  0.951300 ||  0.950812 ||    14.163491 || 
    Epoch 12    --      2.025043 ||   0.5026 ||   0.7699 ||  0.958684 ||  0.961116 ||      2.129235 ||      0.4775 ||   0.7518 ||  0.956063 ||  0.955609 ||    13.966877 || 
    Epoch 13    --      1.935536 ||   0.5239 ||   0.7869 ||  0.962346 ||  0.964580 ||      2.062114 ||      0.4817 ||   0.7630 ||  0.960650 ||  0.960076 ||    14.222005 || 
    Epoch 14    --      1.837947 ||   0.5487 ||   0.8028 ||  0.966347 ||  0.968305 ||      1.944542 ||      0.5129 ||   0.7878 ||  0.963820 ||  0.964020 ||    14.490684 || 
    Epoch 15    --      1.753825 ||   0.5666 ||   0.8192 ||  0.969480 ||  0.971288 ||      1.828761 ||      0.5415 ||   0.8051 ||  0.968545 ||  0.969257 ||    14.609736 || 
    Epoch 16    --      1.666010 ||   0.5856 ||   0.8330 ||  0.972727 ||  0.974391 ||      1.765882 ||      0.5548 ||   0.8150 ||  0.970358 ||  0.970889 ||    14.208369 || 
    Epoch 17    --      1.590236 ||   0.6046 ||   0.8458 ||  0.975115 ||  0.976723 ||      1.672501 ||      0.5789 ||   0.8272 ||  0.973383 ||  0.973780 ||    14.396234 || 
    Epoch 18    --      1.519415 ||   0.6218 ||   0.8555 ||  0.977451 ||  0.978903 ||      1.603299 ||      0.5985 ||   0.8408 ||  0.976609 ||  0.976652 ||    14.204796 || 
    Epoch 19    --      1.434416 ||   0.6447 ||   0.8677 ||  0.979927 ||  0.981238 ||      1.572226 ||      0.6001 ||   0.8511 ||  0.976747 ||  0.976848 ||    14.406559 || 
    Epoch 20    --      1.367312 ||   0.6586 ||   0.8785 ||  0.982071 ||  0.983356 ||      1.477452 ||      0.6275 ||   0.8644 ||  0.979442 ||  0.979501 ||    14.249048 || 
    Epoch 21    --      1.304648 ||   0.6794 ||   0.8858 ||  0.983185 ||  0.984386 ||      1.380470 ||      0.6438 ||   0.8780 ||  0.983271 ||  0.983249 ||    13.713478 || 
    Epoch 22    --      1.232304 ||   0.6951 ||   0.8949 ||  0.985359 ||  0.986399 ||      1.341794 ||      0.6615 ||   0.8816 ||  0.983482 ||  0.983702 ||    13.546446 || 
    Epoch 23    --      1.172712 ||   0.7124 ||   0.9066 ||  0.986564 ||  0.987550 ||      1.262756 ||      0.6787 ||   0.8900 ||  0.985055 ||  0.985416 ||    14.405910 || 
    Epoch 24    --      1.119169 ||   0.7235 ||   0.9109 ||  0.988174 ||  0.989134 ||      1.196297 ||      0.7012 ||   0.9060 ||  0.986152 ||  0.986518 ||    14.656480 || 
    Epoch 25    --      1.062299 ||   0.7390 ||   0.9186 ||  0.988958 ||  0.989788 ||      1.149181 ||      0.7095 ||   0.9108 ||  0.988590 ||  0.988709 ||    14.513050 || 
    Epoch 26    --      1.005944 ||   0.7530 ||   0.9264 ||  0.990562 ||  0.991327 ||      1.114360 ||      0.7159 ||   0.9131 ||  0.989316 ||  0.989188 ||    14.742040 || 
    Epoch 27    --      0.952105 ||   0.7686 ||   0.9326 ||  0.991356 ||  0.992067 ||      1.070763 ||      0.7301 ||   0.9208 ||  0.989350 ||  0.989556 ||    13.879634 || 
    Epoch 28    --      0.906408 ||   0.7788 ||   0.9384 ||  0.992103 ||  0.992762 ||      1.022106 ||      0.7380 ||   0.9265 ||  0.990957 ||  0.991136 ||    14.420905 || 
    Epoch 29    --      0.863093 ||   0.7906 ||   0.9428 ||  0.992941 ||  0.993571 ||      0.954148 ||      0.7527 ||   0.9350 ||  0.991885 ||  0.991989 ||    14.332277 || 
    Epoch 30    --      0.818533 ||   0.8059 ||   0.9484 ||  0.993579 ||  0.994158 ||      0.889504 ||      0.7796 ||   0.9400 ||  0.993354 ||  0.993577 ||    14.085787 || 
    Epoch 31    --      0.772984 ||   0.8169 ||   0.9521 ||  0.994443 ||  0.994960 ||      0.840841 ||      0.7916 ||   0.9463 ||  0.993810 ||  0.994052 ||    13.704638 || 
    Epoch 32    --      0.731152 ||   0.8283 ||   0.9564 ||  0.995076 ||  0.995560 ||      0.808798 ||      0.8040 ||   0.9524 ||  0.993641 ||  0.993838 ||    12.890313 || 
    Epoch 33    --      0.689257 ||   0.8380 ||   0.9619 ||  0.995571 ||  0.996014 ||      0.785980 ||      0.8077 ||   0.9540 ||  0.994633 ||  0.994623 ||    13.668929 || 
    Epoch 34    --      0.652500 ||   0.8490 ||   0.9645 ||  0.995919 ||  0.996319 ||      0.720382 ||      0.8245 ||   0.9602 ||  0.996032 ||  0.996089 ||    13.908951 || 
    Epoch 35    --      0.618364 ||   0.8586 ||   0.9684 ||  0.996347 ||  0.996707 ||      0.719390 ||      0.8149 ||   0.9649 ||  0.996226 ||  0.996129 ||    14.404274 || 
    Epoch 36    --      0.616410 ||   0.8594 ||   0.9688 ||  0.996414 ||  0.996774 ||      0.699199 ||      0.8304 ||   0.9638 ||  0.996387 ||  0.996456 ||    14.297750 || 
    Epoch 37    --      0.582536 ||   0.8697 ||   0.9723 ||  0.996979 ||  0.997321 ||      0.673557 ||      0.8328 ||   0.9661 ||  0.996003 ||  0.996062 ||    14.183807 || 
    Epoch 38    --      0.547908 ||   0.8786 ||   0.9755 ||  0.997135 ||  0.997431 ||      0.646429 ||      0.8417 ||   0.9671 ||  0.996762 ||  0.996712 ||    15.060669 || 
    Epoch 39    --      0.525744 ||   0.8823 ||   0.9761 ||  0.997343 ||  0.997640 ||      0.578680 ||      0.8628 ||   0.9749 ||  0.997674 ||  0.997677 ||    14.190955 || 
    Epoch 40    --      0.486635 ||   0.8970 ||   0.9792 ||  0.997839 ||  0.998081 ||      0.567853 ||      0.8673 ||   0.9745 ||  0.997233 ||  0.997259 ||    13.213233 || 
    Epoch 41    --      0.460005 ||   0.9028 ||   0.9823 ||  0.998130 ||  0.998330 ||      0.531449 ||      0.8796 ||   0.9757 ||  0.997587 ||  0.997561 ||    13.761252 || 
    Epoch 42    --      0.434104 ||   0.9097 ||   0.9840 ||  0.998298 ||  0.998489 ||      0.504027 ||      0.8876 ||   0.9781 ||  0.997993 ||  0.998071 ||    13.991125 || 
    Epoch 43    --      0.413018 ||   0.9154 ||   0.9857 ||  0.998548 ||  0.998723 ||      0.466274 ||      0.8974 ||   0.9810 ||  0.998457 ||  0.998516 ||    14.447778 || 
    Epoch 44    --      0.384958 ||   0.9233 ||   0.9869 ||  0.998716 ||  0.998871 ||      0.451345 ||      0.8983 ||   0.9837 ||  0.998351 ||  0.998357 ||    15.020001 || 
    Epoch 45    --      0.389736 ||   0.9216 ||   0.9863 ||  0.998558 ||  0.998713 ||      0.468579 ||      0.8916 ||   0.9853 ||  0.998842 ||  0.998749 ||    14.588935 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
