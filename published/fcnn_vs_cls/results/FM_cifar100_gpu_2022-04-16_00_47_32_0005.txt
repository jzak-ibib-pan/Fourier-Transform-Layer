Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     41.989525 ||   0.1609 ||   0.3773 ||  0.597180 ||  0.597221 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.408199 ||   0.0940 ||   0.2534 ||  0.621072 ||  0.622561 ||     12.092153 ||      0.1354 ||   0.3373 ||  0.647681 ||  0.646113 ||    14.683589 || 
    Epoch 01    --     10.143072 ||   0.2096 ||   0.4588 ||  0.700781 ||  0.701514 ||     11.026013 ||      0.1928 ||   0.4347 ||  0.684239 ||  0.684199 ||    13.078960 || 
    Epoch 02    --      9.502513 ||   0.2600 ||   0.5286 ||  0.725351 ||  0.726015 ||     10.838143 ||      0.2339 ||   0.5078 ||  0.706357 ||  0.705973 ||    12.563989 || 
    Epoch 03    --      8.857284 ||   0.3095 ||   0.5932 ||  0.751350 ||  0.751846 ||     10.557101 ||      0.2753 ||   0.5466 ||  0.719521 ||  0.719314 ||    13.119733 || 
    Epoch 04    --      8.265156 ||   0.3553 ||   0.6453 ||  0.772145 ||  0.772710 ||      8.898826 ||      0.3404 ||   0.6289 ||  0.762799 ||  0.762104 ||    14.096702 || 
    Epoch 05    --      7.599609 ||   0.3968 ||   0.6945 ||  0.792008 ||  0.792428 ||      8.409695 ||      0.3778 ||   0.6627 ||  0.777000 ||  0.775905 ||    13.566251 || 
    Epoch 06    --      7.050663 ||   0.4370 ||   0.7337 ||  0.807283 ||  0.807538 ||      8.279594 ||      0.3993 ||   0.6963 ||  0.784603 ||  0.786133 ||    13.443565 || 
    Epoch 07    --      6.551877 ||   0.4709 ||   0.7654 ||  0.823910 ||  0.824180 ||      7.242387 ||      0.4519 ||   0.7434 ||  0.810353 ||  0.810121 ||    13.354008 || 
    Epoch 08    --      6.116251 ||   0.5050 ||   0.7929 ||  0.835755 ||  0.835950 ||      7.957005 ||      0.4428 ||   0.7412 ||  0.801806 ||  0.799496 ||    13.309499 || 
    Epoch 09    --      6.122301 ||   0.5028 ||   0.7940 ||  0.836297 ||  0.836577 ||      6.993746 ||      0.4668 ||   0.7728 ||  0.818692 ||  0.817599 ||    13.644706 || 
    Epoch 10    --      5.640764 ||   0.5371 ||   0.8199 ||  0.849399 ||  0.849680 ||      6.427469 ||      0.4983 ||   0.8019 ||  0.829898 ||  0.831034 ||    13.242672 || 
    Epoch 11    --      5.335642 ||   0.5596 ||   0.8384 ||  0.858856 ||  0.859155 ||      6.477533 ||      0.5069 ||   0.8099 ||  0.832820 ||  0.831859 ||    13.357706 || 
    Epoch 12    --      4.943161 ||   0.5848 ||   0.8596 ||  0.868353 ||  0.868604 ||      6.153610 ||      0.5291 ||   0.8296 ||  0.843413 ||  0.842678 ||    12.360142 || 
    Epoch 13    --      4.618006 ||   0.6111 ||   0.8740 ||  0.876730 ||  0.876951 ||      5.559641 ||      0.5675 ||   0.8509 ||  0.854932 ||  0.854906 ||    13.725278 || 
    Epoch 14    --      4.391352 ||   0.6284 ||   0.8891 ||  0.884505 ||  0.884697 ||      5.972489 ||      0.5569 ||   0.8547 ||  0.848759 ||  0.847498 ||    13.749307 || 
    Epoch 15    --      4.356044 ||   0.6316 ||   0.8885 ||  0.886022 ||  0.886225 ||      5.662210 ||      0.5734 ||   0.8540 ||  0.857390 ||  0.857276 ||    13.351773 || 
    Epoch 16    --      4.139901 ||   0.6484 ||   0.9002 ||  0.890510 ||  0.890677 ||      5.304997 ||      0.5855 ||   0.8777 ||  0.862464 ||  0.862549 ||    13.084646 || 
    Epoch 17    --      3.944384 ||   0.6629 ||   0.9084 ||  0.896252 ||  0.896477 ||      4.536147 ||      0.6264 ||   0.9007 ||  0.884766 ||  0.883668 ||    13.105750 || 
    Epoch 18    --      3.648802 ||   0.6842 ||   0.9201 ||  0.903981 ||  0.904209 ||      4.636060 ||      0.6344 ||   0.8957 ||  0.883688 ||  0.882224 ||    13.365591 || 
    Epoch 19    --      3.478723 ||   0.6975 ||   0.9294 ||  0.908810 ||  0.908985 ||      4.263453 ||      0.6582 ||   0.9043 ||  0.892211 ||  0.890857 ||    12.391845 || 
    Epoch 20    --      3.304554 ||   0.7120 ||   0.9356 ||  0.912785 ||  0.912909 ||      4.216354 ||      0.6623 ||   0.9196 ||  0.895522 ||  0.893724 ||    11.513520 || 
    Epoch 21    --      3.166427 ||   0.7235 ||   0.9417 ||  0.916950 ||  0.917052 ||      3.588644 ||      0.6931 ||   0.9327 ||  0.906772 ||  0.906811 ||    13.749135 || 
    Epoch 22    --      3.015570 ||   0.7324 ||   0.9484 ||  0.920329 ||  0.920444 ||      3.531669 ||      0.7065 ||   0.9338 ||  0.909151 ||  0.909134 ||    13.767606 || 
    Epoch 23    --      2.831959 ||   0.7485 ||   0.9539 ||  0.925741 ||  0.925870 ||      3.308779 ||      0.7242 ||   0.9444 ||  0.914854 ||  0.915474 ||    13.072448 || 
    Epoch 24    --      2.738987 ||   0.7588 ||   0.9571 ||  0.928734 ||  0.928765 ||      3.102784 ||      0.7354 ||   0.9496 ||  0.920271 ||  0.920054 ||    12.841511 || 
    Epoch 25    --      2.592493 ||   0.7710 ||   0.9608 ||  0.932438 ||  0.932563 ||      3.556507 ||      0.7215 ||   0.9452 ||  0.911639 ||  0.911040 ||    13.926471 || 
    Epoch 26    --      2.611682 ||   0.7686 ||   0.9607 ||  0.932499 ||  0.932664 ||      2.933070 ||      0.7467 ||   0.9586 ||  0.925672 ||  0.925501 ||    13.810802 || 
    Epoch 27    --      2.515605 ||   0.7779 ||   0.9649 ||  0.934754 ||  0.934850 ||      2.943492 ||      0.7474 ||   0.9562 ||  0.922537 ||  0.923100 ||    13.634713 || 
    Epoch 28    --      2.514539 ||   0.7774 ||   0.9643 ||  0.935167 ||  0.935285 ||      3.186292 ||      0.7376 ||   0.9565 ||  0.918585 ||  0.918521 ||    13.292332 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
