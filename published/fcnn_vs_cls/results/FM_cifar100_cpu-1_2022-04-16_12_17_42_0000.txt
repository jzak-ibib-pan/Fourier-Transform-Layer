Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     38.270798 ||   0.1650 ||   0.3904 ||  0.604011 ||  0.604104 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.377928 ||   0.0967 ||   0.2550 ||  0.622952 ||  0.624426 ||     11.920473 ||      0.1375 ||   0.3423 ||  0.650546 ||  0.649438 ||    18.046966 || 
    Epoch 01    --     10.114736 ||   0.2130 ||   0.4608 ||  0.701952 ||  0.702721 ||     11.380479 ||      0.1851 ||   0.4347 ||  0.682330 ||  0.681826 ||    16.197737 || 
    Epoch 02    --      9.464778 ||   0.2647 ||   0.5337 ||  0.728808 ||  0.729404 ||     10.802040 ||      0.2411 ||   0.5014 ||  0.708151 ||  0.708122 ||    16.571571 || 
    Epoch 03    --      8.818158 ||   0.3126 ||   0.5976 ||  0.751299 ||  0.751817 ||      9.932911 ||      0.2879 ||   0.5623 ||  0.728594 ||  0.728384 ||    16.522677 || 
    Epoch 04    --      8.221648 ||   0.3601 ||   0.6484 ||  0.772830 ||  0.773374 ||      9.188950 ||      0.3272 ||   0.6193 ||  0.756437 ||  0.755626 ||    16.566025 || 
    Epoch 05    --      7.589362 ||   0.3988 ||   0.6934 ||  0.791800 ||  0.792201 ||      8.363933 ||      0.3709 ||   0.6685 ||  0.774322 ||  0.774240 ||    16.554262 || 
    Epoch 06    --      7.029092 ||   0.4408 ||   0.7327 ||  0.809570 ||  0.809832 ||      8.086683 ||      0.4068 ||   0.7093 ||  0.787681 ||  0.787938 ||    16.749045 || 
    Epoch 07    --      6.569395 ||   0.4735 ||   0.7651 ||  0.823357 ||  0.823679 ||      7.387256 ||      0.4384 ||   0.7436 ||  0.805573 ||  0.805144 ||    16.443513 || 
    Epoch 08    --      6.062212 ||   0.5054 ||   0.7964 ||  0.837879 ||  0.838061 ||      7.764331 ||      0.4494 ||   0.7492 ||  0.805359 ||  0.804429 ||    16.374844 || 
    Epoch 09    --      5.684090 ||   0.5361 ||   0.8219 ||  0.848446 ||  0.848734 ||      6.736828 ||      0.4992 ||   0.7923 ||  0.829947 ||  0.829190 ||    16.632279 || 
    Epoch 10    --      5.321668 ||   0.5599 ||   0.8410 ||  0.859300 ||  0.859548 ||      6.172850 ||      0.5216 ||   0.8155 ||  0.840188 ||  0.840694 ||    16.660034 || 
    Epoch 11    --      4.955920 ||   0.5857 ||   0.8599 ||  0.868834 ||  0.869103 ||      6.058699 ||      0.5361 ||   0.8309 ||  0.846912 ||  0.847589 ||    16.465392 || 
    Epoch 12    --      4.661513 ||   0.6085 ||   0.8755 ||  0.876099 ||  0.876303 ||      6.011612 ||      0.5488 ||   0.8472 ||  0.849785 ||  0.848615 ||    16.484834 || 
    Epoch 13    --      4.377553 ||   0.6293 ||   0.8896 ||  0.883540 ||  0.883731 ||      5.492108 ||      0.5770 ||   0.8604 ||  0.861924 ||  0.861527 ||    16.479276 || 
    Epoch 14    --      4.116954 ||   0.6489 ||   0.9022 ||  0.891966 ||  0.892113 ||      5.295162 ||      0.5975 ||   0.8753 ||  0.865521 ||  0.864460 ||    16.498178 || 
    Epoch 15    --      3.845728 ||   0.6708 ||   0.9130 ||  0.898026 ||  0.898155 ||      4.616789 ||      0.6275 ||   0.8937 ||  0.881488 ||  0.881955 ||    16.779776 || 
    Epoch 16    --      3.704978 ||   0.6830 ||   0.9186 ||  0.902900 ||  0.903056 ||      4.913251 ||      0.6217 ||   0.8944 ||  0.876740 ||  0.876987 ||    16.710038 || 
    Epoch 17    --      3.694937 ||   0.6837 ||   0.9218 ||  0.903228 ||  0.903470 ||      4.382963 ||      0.6489 ||   0.9022 ||  0.887347 ||  0.886836 ||    16.715010 || 
    Epoch 18    --      3.492293 ||   0.6997 ||   0.9297 ||  0.908132 ||  0.908370 ||      4.593232 ||      0.6388 ||   0.9082 ||  0.882696 ||  0.882284 ||    16.300202 || 
    Epoch 19    --      3.499442 ||   0.6994 ||   0.9286 ||  0.908011 ||  0.908184 ||      3.869009 ||      0.6744 ||   0.9215 ||  0.902739 ||  0.901759 ||    16.277282 || 
    Epoch 20    --      3.377654 ||   0.7116 ||   0.9348 ||  0.911511 ||  0.911623 ||      3.953825 ||      0.6752 ||   0.9227 ||  0.898366 ||  0.897401 ||    16.307596 || 
    Epoch 21    --      3.348750 ||   0.7124 ||   0.9335 ||  0.912394 ||  0.912487 ||      3.950009 ||      0.6794 ||   0.9262 ||  0.896639 ||  0.896440 ||    16.693859 || 
    Epoch 22    --      3.174716 ||   0.7247 ||   0.9417 ||  0.917804 ||  0.917881 ||      3.332636 ||      0.7068 ||   0.9356 ||  0.911615 ||  0.912192 ||    16.384004 || 
    Epoch 23    --      2.970808 ||   0.7386 ||   0.9473 ||  0.921801 ||  0.921921 ||      3.452520 ||      0.7085 ||   0.9411 ||  0.912059 ||  0.912073 ||    16.499790 || 
    Epoch 24    --      2.865147 ||   0.7493 ||   0.9524 ||  0.925571 ||  0.925590 ||      3.627737 ||      0.7089 ||   0.9393 ||  0.906649 ||  0.906991 ||    16.503886 || 
    Epoch 25    --      2.857524 ||   0.7472 ||   0.9513 ||  0.925111 ||  0.925258 ||      3.629842 ||      0.6966 ||   0.9404 ||  0.905452 ||  0.905536 ||    16.287131 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
