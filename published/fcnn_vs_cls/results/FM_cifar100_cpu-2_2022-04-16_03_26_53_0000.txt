Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     38.221558 ||   0.1613 ||   0.3906 ||  0.600686 ||  0.600781 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.353832 ||   0.0945 ||   0.2510 ||  0.621891 ||  0.623350 ||     11.515176 ||      0.1395 ||   0.3441 ||  0.655260 ||  0.654305 ||    88.764904 || 
    Epoch 01    --     10.151229 ||   0.2109 ||   0.4556 ||  0.700906 ||  0.701642 ||     11.023789 ||      0.1900 ||   0.4415 ||  0.685183 ||  0.684923 ||    86.764927 || 
    Epoch 02    --      9.509989 ||   0.2603 ||   0.5286 ||  0.727111 ||  0.727713 ||     10.471565 ||      0.2449 ||   0.5083 ||  0.714455 ||  0.713737 ||    86.499313 || 
    Epoch 03    --      8.849541 ||   0.3089 ||   0.5920 ||  0.750875 ||  0.751421 ||     10.215966 ||      0.2781 ||   0.5532 ||  0.724356 ||  0.724832 ||    90.030542 || 
    Epoch 04    --      8.201986 ||   0.3571 ||   0.6492 ||  0.773786 ||  0.774310 ||      9.421087 ||      0.3252 ||   0.6105 ||  0.755062 ||  0.753726 ||    89.686790 || 
    Epoch 05    --      7.688327 ||   0.3912 ||   0.6888 ||  0.789411 ||  0.789809 ||      8.381877 ||      0.3691 ||   0.6604 ||  0.776741 ||  0.775130 ||    88.514931 || 
    Epoch 06    --      7.076501 ||   0.4374 ||   0.7304 ||  0.808725 ||  0.809007 ||      8.084474 ||      0.4012 ||   0.6989 ||  0.784080 ||  0.784831 ||    88.843024 || 
    Epoch 07    --      6.636120 ||   0.4681 ||   0.7625 ||  0.821472 ||  0.821776 ||      7.433430 ||      0.4387 ||   0.7432 ||  0.804640 ||  0.803948 ||    89.296177 || 
    Epoch 08    --      6.111211 ||   0.5047 ||   0.7922 ||  0.836037 ||  0.836221 ||      7.124679 ||      0.4668 ||   0.7626 ||  0.817713 ||  0.815983 ||    87.264998 || 
    Epoch 09    --      5.771524 ||   0.5314 ||   0.8164 ||  0.846387 ||  0.846676 ||      6.779891 ||      0.4889 ||   0.7915 ||  0.826166 ||  0.825255 ||    87.936910 || 
    Epoch 10    --      5.337780 ||   0.5562 ||   0.8379 ||  0.857457 ||  0.857714 ||      5.983587 ||      0.5224 ||   0.8191 ||  0.842468 ||  0.843449 ||    87.226019 || 
    Epoch 11    --      5.025939 ||   0.5796 ||   0.8569 ||  0.866550 ||  0.866802 ||      5.892247 ||      0.5460 ||   0.8291 ||  0.848420 ||  0.847972 ||    87.171287 || 
    Epoch 12    --      4.760201 ||   0.6043 ||   0.8716 ||  0.873737 ||  0.873963 ||      6.085156 ||      0.5377 ||   0.8454 ||  0.846342 ||  0.844194 ||    87.178731 || 
    Epoch 13    --      4.722818 ||   0.5992 ||   0.8738 ||  0.873711 ||  0.873921 ||      5.530867 ||      0.5727 ||   0.8507 ||  0.857782 ||  0.857611 ||    86.827544 || 
    Epoch 14    --      4.445491 ||   0.6231 ||   0.8859 ||  0.883343 ||  0.883501 ||      5.916366 ||      0.5601 ||   0.8454 ||  0.850781 ||  0.849671 ||    87.265036 || 
    Epoch 15    --      4.429819 ||   0.6275 ||   0.8867 ||  0.883193 ||  0.883396 ||      5.068614 ||      0.5950 ||   0.8678 ||  0.867985 ||  0.867987 ||    86.780667 || 
    Epoch 16    --      4.198170 ||   0.6457 ||   0.8984 ||  0.889879 ||  0.890073 ||      5.527860 ||      0.5783 ||   0.8745 ||  0.859868 ||  0.859556 ||    87.093125 || 
    Epoch 17    --      4.209890 ||   0.6450 ||   0.8968 ||  0.889624 ||  0.889877 ||      5.028280 ||      0.6101 ||   0.8749 ||  0.874123 ||  0.873213 ||    84.905657 || 
    Epoch 18    --      3.953057 ||   0.6633 ||   0.9106 ||  0.896177 ||  0.896423 ||      5.083674 ||      0.6149 ||   0.8845 ||  0.871413 ||  0.870905 ||    85.421299 || 
    Epoch 19    --      3.692205 ||   0.6819 ||   0.9197 ||  0.902986 ||  0.903196 ||      4.290892 ||      0.6438 ||   0.9032 ||  0.890186 ||  0.889884 ||    85.899691 || 
    Epoch 20    --      3.525553 ||   0.6948 ||   0.9247 ||  0.907608 ||  0.907712 ||      4.143426 ||      0.6596 ||   0.9143 ||  0.891490 ||  0.891509 ||    84.622892 || 
    Epoch 21    --      3.341574 ||   0.7073 ||   0.9322 ||  0.912131 ||  0.912273 ||      4.456290 ||      0.6513 ||   0.9134 ||  0.885158 ||  0.886163 ||    85.294693 || 
    Epoch 22    --      3.384617 ||   0.7056 ||   0.9327 ||  0.911202 ||  0.911313 ||      3.810810 ||      0.6866 ||   0.9197 ||  0.899206 ||  0.899531 ||    85.660861 || 
    Epoch 23    --      3.168915 ||   0.7230 ||   0.9399 ||  0.916660 ||  0.916798 ||      3.687083 ||      0.6930 ||   0.9264 ||  0.904827 ||  0.904650 ||    85.294745 || 
    Epoch 24    --      3.062704 ||   0.7340 ||   0.9456 ||  0.920319 ||  0.920363 ||      3.426154 ||      0.7168 ||   0.9385 ||  0.913199 ||  0.913157 ||    86.951021 || 
    Epoch 25    --      2.879379 ||   0.7461 ||   0.9507 ||  0.924707 ||  0.924867 ||      3.450310 ||      0.7145 ||   0.9406 ||  0.912032 ||  0.911565 ||    87.404132 || 
    Epoch 26    --      2.898728 ||   0.7465 ||   0.9509 ||  0.924396 ||  0.924604 ||      3.459401 ||      0.7111 ||   0.9400 ||  0.911079 ||  0.909693 ||    87.169713 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
