Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     44.556564 ||   0.1647 ||   0.3867 ||  0.600570 ||  0.600601 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.301909 ||   0.0962 ||   0.2545 ||  0.622436 ||  0.623904 ||     11.870436 ||      0.1401 ||   0.3424 ||  0.651283 ||  0.649675 ||    85.076268 || 
    Epoch 01    --     10.100626 ||   0.2186 ||   0.4629 ||  0.701845 ||  0.702577 ||     11.404933 ||      0.1920 ||   0.4317 ||  0.681257 ||  0.680351 ||    84.444824 || 
    Epoch 02    --      9.455978 ||   0.2652 ||   0.5362 ||  0.728648 ||  0.729274 ||     11.175380 ||      0.2363 ||   0.5015 ||  0.704602 ||  0.703403 ||    83.920103 || 
    Epoch 03    --      8.816751 ||   0.3119 ||   0.5946 ||  0.750457 ||  0.750975 ||     10.098004 ||      0.2806 ||   0.5528 ||  0.728838 ||  0.727964 ||    84.701350 || 
    Epoch 04    --      8.171380 ||   0.3591 ||   0.6488 ||  0.773696 ||  0.774275 ||      8.998552 ||      0.3288 ||   0.6237 ||  0.759151 ||  0.757791 ||    84.670105 || 
    Epoch 05    --      7.572594 ||   0.3999 ||   0.6949 ||  0.792436 ||  0.792854 ||      8.459942 ||      0.3666 ||   0.6635 ||  0.771417 ||  0.770626 ||    84.263865 || 
    Epoch 06    --      6.989967 ||   0.4434 ||   0.7371 ||  0.810553 ||  0.810835 ||      7.936764 ||      0.4004 ||   0.7040 ||  0.789223 ||  0.789380 ||    84.498270 || 
    Epoch 07    --      6.577619 ||   0.4704 ||   0.7666 ||  0.820563 ||  0.820871 ||      7.403391 ||      0.4400 ||   0.7403 ||  0.806980 ||  0.806147 ||    84.701395 || 
    Epoch 08    --      6.109180 ||   0.5040 ||   0.7963 ||  0.835655 ||  0.835810 ||      7.057284 ||      0.4710 ||   0.7658 ||  0.818272 ||  0.817699 ||    84.873264 || 
    Epoch 09    --      5.606678 ||   0.5384 ||   0.8202 ||  0.850072 ||  0.850321 ||      6.601265 ||      0.4978 ||   0.7934 ||  0.829304 ||  0.828493 ||    84.367828 || 
    Epoch 10    --      5.342592 ||   0.5615 ||   0.8406 ||  0.858547 ||  0.858794 ||      6.134634 ||      0.5255 ||   0.8197 ||  0.839144 ||  0.839792 ||    85.029531 || 
    Epoch 11    --      4.983125 ||   0.5842 ||   0.8595 ||  0.868139 ||  0.868417 ||      6.353179 ||      0.5289 ||   0.8260 ||  0.840716 ||  0.840796 ||    85.288385 || 
    Epoch 12    --      4.658743 ||   0.6098 ||   0.8762 ||  0.875814 ||  0.876067 ||      5.841491 ||      0.5531 ||   0.8454 ||  0.851887 ||  0.850511 ||    84.560772 || 
    Epoch 13    --      4.422393 ||   0.6252 ||   0.8883 ||  0.882632 ||  0.882869 ||      5.283707 ||      0.5899 ||   0.8626 ||  0.865993 ||  0.865441 ||    84.935814 || 
    Epoch 14    --      4.104684 ||   0.6520 ||   0.9026 ||  0.891743 ||  0.891883 ||      5.386631 ||      0.5917 ||   0.8687 ||  0.864794 ||  0.863824 ||    85.206802 || 
    Epoch 15    --      3.868436 ||   0.6684 ||   0.9135 ||  0.899165 ||  0.899315 ||      4.535728 ||      0.6373 ||   0.8907 ||  0.886154 ||  0.886331 ||    84.765791 || 
    Epoch 16    --      3.670176 ||   0.6856 ||   0.9198 ||  0.903474 ||  0.903629 ||      4.487031 ||      0.6394 ||   0.9053 ||  0.882519 ||  0.881860 ||    85.029577 || 
    Epoch 17    --      3.520403 ||   0.6963 ||   0.9290 ||  0.907575 ||  0.907786 ||      4.339387 ||      0.6473 ||   0.9116 ||  0.889696 ||  0.888689 ||    85.545248 || 
    Epoch 18    --      3.343387 ||   0.7122 ||   0.9351 ||  0.911950 ||  0.912194 ||      4.269626 ||      0.6607 ||   0.9169 ||  0.890899 ||  0.890198 ||    85.967073 || 
    Epoch 19    --      3.109055 ||   0.7272 ||   0.9420 ||  0.917713 ||  0.917855 ||      3.991939 ||      0.6752 ||   0.9259 ||  0.896102 ||  0.895386 ||    85.326503 || 
    Epoch 20    --      3.062639 ||   0.7338 ||   0.9461 ||  0.919932 ||  0.920023 ||      3.508968 ||      0.6963 ||   0.9389 ||  0.908369 ||  0.908200 ||    86.000555 || 
    Epoch 21    --      2.799384 ||   0.7512 ||   0.9532 ||  0.926368 ||  0.926444 ||      4.022545 ||      0.6885 ||   0.9288 ||  0.900256 ||  0.899791 ||    85.967142 || 
    Epoch 22    --      2.906299 ||   0.7436 ||   0.9520 ||  0.923928 ||  0.924036 ||      3.491167 ||      0.7159 ||   0.9429 ||  0.910926 ||  0.910885 ||    85.201547 || 
    Epoch 23    --      2.674528 ||   0.7606 ||   0.9583 ||  0.929389 ||  0.929530 ||      3.293311 ||      0.7236 ||   0.9457 ||  0.916125 ||  0.915909 ||    87.060881 || 
    Epoch 24    --      2.610142 ||   0.7703 ||   0.9597 ||  0.931545 ||  0.931571 ||      3.488535 ||      0.7201 ||   0.9482 ||  0.911548 ||  0.911873 ||    87.576513 || 
    Epoch 25    --      2.628684 ||   0.7678 ||   0.9608 ||  0.930803 ||  0.930965 ||      3.125911 ||      0.7379 ||   0.9491 ||  0.921226 ||  0.920824 ||    87.920260 || 
    Epoch 26    --      2.531527 ||   0.7746 ||   0.9649 ||  0.934938 ||  0.935084 ||      3.205900 ||      0.7393 ||   0.9517 ||  0.921144 ||  0.920202 ||    86.389038 || 
    Epoch 27    --      2.385256 ||   0.7885 ||   0.9670 ||  0.937947 ||  0.938059 ||      2.958692 ||      0.7523 ||   0.9554 ||  0.923354 ||  0.923069 ||    87.748394 || 
    Epoch 28    --      2.324693 ||   0.7944 ||   0.9702 ||  0.939351 ||  0.939462 ||      3.248451 ||      0.7488 ||   0.9502 ||  0.919415 ||  0.919055 ||    86.836587 || 
    Epoch 29    --      2.333612 ||   0.7939 ||   0.9684 ||  0.940193 ||  0.940172 ||      3.268590 ||      0.7376 ||   0.9581 ||  0.918904 ||  0.918766 ||    88.170310 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
