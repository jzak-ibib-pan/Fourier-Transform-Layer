Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -           ['conv2d', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.075814 ||   0.0244 ||   0.0890 ||  0.534907 ||  0.549995 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      4.607505 ||   0.0561 ||   0.1349 ||  0.559795 ||  0.561274 ||      4.606982 ||      0.0092 ||   0.0442 ||  0.500292 ||  0.486399 ||    90.654377 || 
    Epoch 01    --      4.606215 ||   0.0097 ||   0.0485 ||  0.494434 ||  0.495923 ||      4.605809 ||      0.0086 ||   0.0459 ||  0.501815 ||  0.492099 ||    90.560652 || 
    Epoch 02    --      4.602853 ||   0.0103 ||   0.0499 ||  0.494633 ||  0.496172 ||      4.604501 ||      0.0093 ||   0.0467 ||  0.501880 ||  0.487695 ||    89.873151 || 
    Epoch 03    --      4.600554 ||   0.0110 ||   0.0501 ||  0.496126 ||  0.498566 ||      4.598876 ||      0.0100 ||   0.0455 ||  0.503983 ||  0.490983 ||    90.053253 || 
    Epoch 04    --      4.597292 ||   0.0109 ||   0.0512 ||  0.496083 ||  0.498607 ||      4.593667 ||      0.0104 ||   0.0480 ||  0.507157 ||  0.497849 ||    89.841968 || 
    Epoch 05    --      4.592719 ||   0.0127 ||   0.0515 ||  0.499733 ||  0.502190 ||      4.593142 ||      0.0110 ||   0.0503 ||  0.508873 ||  0.499947 ||    90.013857 || 
    Epoch 06    --      4.586514 ||   0.0137 ||   0.0546 ||  0.501135 ||  0.503559 ||      4.599714 ||      0.0104 ||   0.0492 ||  0.510230 ||  0.498237 ||    89.694147 || 
    Epoch 07    --      4.581677 ||   0.0144 ||   0.0559 ||  0.503933 ||  0.506599 ||      4.580861 ||      0.0135 ||   0.0531 ||  0.512832 ||  0.499947 ||    90.029470 || 
    Epoch 08    --      4.574926 ||   0.0155 ||   0.0573 ||  0.506576 ||  0.510002 ||      4.578006 ||      0.0137 ||   0.0520 ||  0.512819 ||  0.502473 ||    90.470536 || 
    Epoch 09    --      4.569167 ||   0.0162 ||   0.0605 ||  0.510043 ||  0.512933 ||      4.574060 ||      0.0179 ||   0.0605 ||  0.517208 ||  0.511716 ||    90.692916 || 
    Epoch 10    --      4.563437 ||   0.0193 ||   0.0643 ||  0.513341 ||  0.517459 ||      4.560242 ||      0.0172 ||   0.0579 ||  0.518560 ||  0.509924 ||    90.255148 || 
    Epoch 11    --      4.553875 ||   0.0203 ||   0.0664 ||  0.515445 ||  0.520023 ||      4.549016 ||      0.0182 ||   0.0623 ||  0.524217 ||  0.514561 ||    90.029572 || 
    Epoch 12    --      4.542589 ||   0.0228 ||   0.0686 ||  0.518217 ||  0.522819 ||      4.545382 ||      0.0227 ||   0.0650 ||  0.525015 ||  0.517237 ||    90.217031 || 
    Epoch 13    --      4.531515 ||   0.0250 ||   0.0731 ||  0.523654 ||  0.529004 ||      4.527284 ||      0.0243 ||   0.0707 ||  0.530630 ||  0.523562 ||    90.951435 || 
    Epoch 14    --      4.526513 ||   0.0268 ||   0.0748 ||  0.525148 ||  0.531258 ||      4.513913 ||      0.0263 ||   0.0745 ||  0.534840 ||  0.530851 ||    92.791485 || 
    Epoch 15    --      4.508676 ||   0.0293 ||   0.0772 ||  0.531380 ||  0.537717 ||      4.520361 ||      0.0255 ||   0.0722 ||  0.532520 ||  0.529258 ||    90.935838 || 
    Epoch 16    --      4.502313 ||   0.0308 ||   0.0807 ||  0.534009 ||  0.541646 ||      4.489835 ||      0.0298 ||   0.0795 ||  0.540718 ||  0.538156 ||    91.529584 || 
    Epoch 17    --      4.479980 ||   0.0347 ||   0.0874 ||  0.540188 ||  0.547007 ||      4.509497 ||      0.0269 ||   0.0763 ||  0.535179 ||  0.534754 ||    90.435544 || 
    Epoch 18    --      4.470777 ||   0.0369 ||   0.0892 ||  0.543911 ||  0.551028 ||      4.488453 ||      0.0319 ||   0.0850 ||  0.546413 ||  0.547753 ||    90.998376 || 
    Epoch 19    --      4.455013 ||   0.0391 ||   0.0941 ||  0.548968 ||  0.557797 ||      4.442120 ||      0.0391 ||   0.0949 ||  0.558401 ||  0.559693 ||    90.092146 || 
    Epoch 20    --      4.437865 ||   0.0418 ||   0.0970 ||  0.553857 ||  0.563242 ||      4.471231 ||      0.0388 ||   0.0926 ||  0.554016 ||  0.555523 ||    90.514012 || 
    Epoch 21    --      4.423041 ||   0.0456 ||   0.1023 ||  0.560268 ||  0.569432 ||      4.418883 ||      0.0446 ||   0.1000 ||  0.560680 ||  0.562125 ||    90.931121 || 
    Epoch 22    --      4.398831 ||   0.0506 ||   0.1089 ||  0.567203 ||  0.577566 ||      4.419718 ||      0.0441 ||   0.1008 ||  0.560238 ||  0.564616 ||    90.498425 || 
    Epoch 23    --      4.390179 ||   0.0517 ||   0.1129 ||  0.568070 ||  0.579681 ||      4.372240 ||      0.0557 ||   0.1094 ||  0.576864 ||  0.579669 ||    89.772668 || 
    Epoch 24    --      4.370273 ||   0.0567 ||   0.1161 ||  0.573519 ||  0.585250 ||      4.370292 ||      0.0568 ||   0.1136 ||  0.576597 ||  0.581198 ||    90.255422 || 
    Epoch 25    --      4.344805 ||   0.0609 ||   0.1217 ||  0.580841 ||  0.592995 ||      4.381006 ||      0.0546 ||   0.1150 ||  0.575432 ||  0.581027 ||    91.298700 || 
    Epoch 26    --      4.339818 ||   0.0633 ||   0.1255 ||  0.581702 ||  0.594536 ||      4.321308 ||      0.0653 ||   0.1250 ||  0.586008 ||  0.589686 ||    90.920291 || 
    Epoch 27    --      4.317750 ||   0.0667 ||   0.1311 ||  0.587480 ||  0.600742 ||      4.315431 ||      0.0638 ||   0.1308 ||  0.594962 ||  0.603251 ||    91.404711 || 
    Epoch 28    --      4.298629 ||   0.0698 ||   0.1353 ||  0.592478 ||  0.604896 ||      4.298044 ||      0.0690 ||   0.1362 ||  0.603595 ||  0.609143 ||    90.701618 || 
    Epoch 29    --      4.279192 ||   0.0750 ||   0.1411 ||  0.595403 ||  0.609242 ||      4.272594 ||      0.0724 ||   0.1371 ||  0.602847 ||  0.612667 ||    91.141440 || 
    Epoch 30    --      4.258416 ||   0.0777 ||   0.1441 ||  0.601225 ||  0.616005 ||      4.256201 ||      0.0788 ||   0.1448 ||  0.608079 ||  0.616264 ||    91.605529 || 
    Epoch 31    --      4.244431 ||   0.0806 ||   0.1452 ||  0.603628 ||  0.619436 ||      4.246572 ||      0.0814 ||   0.1516 ||  0.609728 ||  0.617889 ||    91.654747 || 
    Epoch 32    --      4.228199 ||   0.0844 ||   0.1526 ||  0.608072 ||  0.622955 ||      4.245740 ||      0.0790 ||   0.1501 ||  0.611019 ||  0.623945 ||    91.059097 || 
    Epoch 33    --      4.224966 ||   0.0848 ||   0.1539 ||  0.608483 ||  0.624196 ||      4.181977 ||      0.0913 ||   0.1599 ||  0.622338 ||  0.630229 ||    91.357880 || 
    Epoch 34    --      4.196746 ||   0.0911 ||   0.1617 ||  0.614212 ||  0.630806 ||      4.284925 ||      0.0749 ||   0.1446 ||  0.598011 ||  0.606461 ||    91.389163 || 
    Epoch 35    --      4.176439 ||   0.0960 ||   0.1653 ||  0.620930 ||  0.637088 ||      4.251840 ||      0.0807 ||   0.1492 ||  0.603181 ||  0.613001 ||    90.967299 || 
    Epoch 36    --      4.159256 ||   0.0994 ||   0.1688 ||  0.621990 ||  0.640051 ||      4.214553 ||      0.0982 ||   0.1753 ||  0.632375 ||  0.641439 ||    91.458202 || 
    Epoch 37    --      4.144909 ||   0.1017 ||   0.1747 ||  0.624484 ||  0.642224 ||      4.201045 ||      0.0995 ||   0.1674 ||  0.626582 ||  0.639470 ||    91.514182 || 
    Epoch 38    --      4.136312 ||   0.1035 ||   0.1755 ||  0.626794 ||  0.644857 ||      4.142741 ||      0.1025 ||   0.1780 ||  0.634146 ||  0.646958 ||    92.826669 || 
    Epoch 39    --      4.125673 ||   0.1059 ||   0.1799 ||  0.629463 ||  0.647537 ||      4.119591 ||      0.1074 ||   0.1763 ||  0.633735 ||  0.644504 ||    92.076696 || 
    Epoch 40    --      4.100667 ||   0.1100 ||   0.1819 ||  0.634381 ||  0.651340 ||      4.121585 ||      0.1059 ||   0.1831 ||  0.637376 ||  0.651099 ||    92.123574 || 
    Epoch 41    --      4.100368 ||   0.1102 ||   0.1822 ||  0.634502 ||  0.652879 ||      4.067650 ||      0.1111 ||   0.1860 ||  0.646232 ||  0.658322 ||    91.201732 || 
    Epoch 42    --      4.085788 ||   0.1129 ||   0.1846 ||  0.637680 ||  0.656707 ||      4.090733 ||      0.1107 ||   0.1851 ||  0.641693 ||  0.652874 ||    91.357975 || 
    Epoch 43    --      4.082917 ||   0.1141 ||   0.1864 ||  0.636580 ||  0.655106 ||      4.118112 ||      0.1067 ||   0.1818 ||  0.638147 ||  0.653130 ||    93.414066 || 
Layers list:
	conv2d                                   -                    (17, 17, 3, 24)|(24,)
	conv2d-filters                           -                                       24
	conv2d-kernel_size                       -                                       17
	conv2d-strides                           -                                   (1, 1)
	conv2d-padding                           -                                    valid
	conv2d-data_format                       -                                     None
	conv2d-dilation_rate                     -                                   (1, 1)
	conv2d-groups                            -                                        1
	conv2d-activation                        -                                     relu
	conv2d-use_bias                          -                                     True
	conv2d-kernel_initializer                -                                he_normal
	conv2d-bias_initializer                  -                                    zeros
	conv2d-kernel_regularizer                -                                     None
	conv2d-bias_regularizer                  -                                     None
	conv2d-activity_regularizer              -                                     None
	conv2d-kernel_constraint                 -                                     None
	conv2d-bias_constraint                   -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 16, 16, 24)        20832     
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 635,332
Trainable params: 635,332
Non-trainable params: 0
_________________________________________________________________
