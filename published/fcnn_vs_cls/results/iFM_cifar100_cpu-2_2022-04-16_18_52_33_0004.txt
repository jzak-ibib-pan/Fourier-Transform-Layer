Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --      7.268095 ||   0.1627 ||   0.3721 ||  0.713915 ||  0.716214 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --      3.772967 ||   0.1524 ||   0.3700 ||  0.747447 ||  0.753984 ||      3.617247 ||      0.1759 ||   0.4091 ||  0.837015 ||  0.841459 ||    99.560598 || 
    Epoch 01    --      3.397633 ||   0.2193 ||   0.4718 ||  0.859299 ||  0.866854 ||      3.411012 ||      0.2137 ||   0.4738 ||  0.863340 ||  0.866053 ||    98.107515 || 
    Epoch 02    --      3.216530 ||   0.2518 ||   0.5189 ||  0.879012 ||  0.885564 ||      3.252195 ||      0.2506 ||   0.5138 ||  0.876523 ||  0.880542 ||    97.871057 || 
    Epoch 03    --      3.072973 ||   0.2843 ||   0.5551 ||  0.891204 ||  0.897038 ||      3.115123 ||      0.2689 ||   0.5417 ||  0.891220 ||  0.894165 ||    98.029493 || 
    Epoch 04    --      2.944621 ||   0.3060 ||   0.5816 ||  0.902725 ||  0.908087 ||      3.000419 ||      0.2977 ||   0.5738 ||  0.901320 ||  0.902842 ||    97.763862 || 
    Epoch 05    --      2.824553 ||   0.3313 ||   0.6122 ||  0.912083 ||  0.916939 ||      2.861696 ||      0.3201 ||   0.5991 ||  0.912417 ||  0.914308 ||    97.951370 || 
    Epoch 06    --      2.699390 ||   0.3562 ||   0.6399 ||  0.921223 ||  0.925325 ||      2.784318 ||      0.3359 ||   0.6158 ||  0.917403 ||  0.919872 ||    97.250883 || 
    Epoch 07    --      2.592611 ||   0.3795 ||   0.6627 ||  0.928221 ||  0.932000 ||      2.634460 ||      0.3650 ||   0.6519 ||  0.928425 ||  0.929784 ||    97.701424 || 
    Epoch 08    --      2.480410 ||   0.4034 ||   0.6862 ||  0.935009 ||  0.938579 ||      2.532395 ||      0.3903 ||   0.6735 ||  0.934487 ||  0.935425 ||    97.904650 || 
    Epoch 09    --      2.365520 ||   0.4271 ||   0.7075 ||  0.941755 ||  0.944971 ||      2.436419 ||      0.4046 ||   0.6918 ||  0.940624 ||  0.941055 ||    97.440400 || 
    Epoch 10    --      2.250190 ||   0.4512 ||   0.7301 ||  0.948166 ||  0.951152 ||      2.323724 ||      0.4360 ||   0.7161 ||  0.946040 ||  0.947281 ||    97.795286 || 
    Epoch 11    --      2.145093 ||   0.4769 ||   0.7480 ||  0.953326 ||  0.955942 ||      2.264068 ||      0.4420 ||   0.7269 ||  0.949478 ||  0.949804 ||    97.826566 || 
    Epoch 12    --      2.049037 ||   0.4994 ||   0.7670 ||  0.957573 ||  0.959987 ||      2.139450 ||      0.4709 ||   0.7474 ||  0.956114 ||  0.955891 ||    99.867595 || 
    Epoch 13    --      1.951201 ||   0.5222 ||   0.7842 ||  0.961665 ||  0.963907 ||      2.091835 ||      0.4732 ||   0.7583 ||  0.959275 ||  0.958500 ||    98.768873 || 
    Epoch 14    --      1.851806 ||   0.5455 ||   0.8013 ||  0.965631 ||  0.967641 ||      1.963096 ||      0.5087 ||   0.7858 ||  0.963678 ||  0.963709 ||    97.857903 || 
    Epoch 15    --      1.767025 ||   0.5651 ||   0.8138 ||  0.968976 ||  0.970810 ||      1.839611 ||      0.5440 ||   0.8041 ||  0.968076 ||  0.968679 ||    97.829623 || 
    Epoch 16    --      1.676068 ||   0.5870 ||   0.8306 ||  0.972300 ||  0.974023 ||      1.782315 ||      0.5514 ||   0.8136 ||  0.969900 ||  0.970124 ||    98.816348 || 
    Epoch 17    --      1.600466 ||   0.6041 ||   0.8418 ||  0.975054 ||  0.976619 ||      1.674511 ||      0.5743 ||   0.8293 ||  0.973756 ||  0.974026 ||   101.142637 || 
    Epoch 18    --      1.518710 ||   0.6238 ||   0.8548 ||  0.977619 ||  0.979032 ||      1.603808 ||      0.6036 ||   0.8376 ||  0.975792 ||  0.976048 ||   100.639145 || 
    Epoch 19    --      1.433463 ||   0.6450 ||   0.8694 ||  0.979981 ||  0.981273 ||      1.576533 ||      0.6056 ||   0.8473 ||  0.976723 ||  0.976611 ||   101.567295 || 
    Epoch 20    --      1.362579 ||   0.6629 ||   0.8781 ||  0.982088 ||  0.983359 ||      1.483562 ||      0.6283 ||   0.8581 ||  0.979479 ||  0.979238 ||    99.596094 || 
    Epoch 21    --      1.301518 ||   0.6791 ||   0.8860 ||  0.983329 ||  0.984489 ||      1.380551 ||      0.6453 ||   0.8780 ||  0.983192 ||  0.982910 ||   101.161431 || 
    Epoch 22    --      1.227316 ||   0.6982 ||   0.8958 ||  0.985354 ||  0.986349 ||      1.324156 ||      0.6714 ||   0.8826 ||  0.983574 ||  0.983998 ||    99.826748 || 
    Epoch 23    --      1.164537 ||   0.7157 ||   0.9060 ||  0.986863 ||  0.987791 ||      1.255349 ||      0.6856 ||   0.8918 ||  0.985134 ||  0.985369 ||   104.566822 || 
    Epoch 24    --      1.108980 ||   0.7258 ||   0.9140 ||  0.988465 ||  0.989341 ||      1.195719 ||      0.7009 ||   0.9019 ||  0.986231 ||  0.986542 ||    99.498648 || 
    Epoch 25    --      1.047144 ||   0.7440 ||   0.9211 ||  0.989294 ||  0.990090 ||      1.145330 ||      0.7050 ||   0.9105 ||  0.988215 ||  0.988368 ||    98.123699 || 
    Epoch 26    --      0.994786 ||   0.7559 ||   0.9280 ||  0.990622 ||  0.991332 ||      1.094350 ||      0.7236 ||   0.9190 ||  0.989480 ||  0.989437 ||    97.592471 || 
    Epoch 27    --      0.938908 ||   0.7728 ||   0.9346 ||  0.991501 ||  0.992178 ||      1.031284 ||      0.7408 ||   0.9230 ||  0.990269 ||  0.990418 ||    97.842495 || 
    Epoch 28    --      0.889410 ||   0.7851 ||   0.9392 ||  0.992553 ||  0.993218 ||      0.994892 ||      0.7438 ||   0.9299 ||  0.991668 ||  0.991778 ||    97.766132 || 
    Epoch 29    --      0.843821 ||   0.7980 ||   0.9457 ||  0.993165 ||  0.993758 ||      0.941814 ||      0.7624 ||   0.9363 ||  0.991956 ||  0.992015 ||    97.748783 || 
    Epoch 30    --      0.795107 ||   0.8120 ||   0.9513 ||  0.993986 ||  0.994516 ||      0.906593 ||      0.7655 ||   0.9433 ||  0.993113 ||  0.993123 ||    97.748759 || 
    Epoch 31    --      0.755753 ||   0.8215 ||   0.9546 ||  0.994588 ||  0.995066 ||      0.825823 ||      0.7984 ||   0.9480 ||  0.994214 ||  0.994430 ||    97.436287 || 
    Epoch 32    --      0.711586 ||   0.8348 ||   0.9590 ||  0.995318 ||  0.995762 ||      0.790672 ||      0.8042 ||   0.9530 ||  0.994375 ||  0.994449 ||    98.174709 || 
    Epoch 33    --      0.669269 ||   0.8462 ||   0.9640 ||  0.995894 ||  0.996306 ||      0.751708 ||      0.8178 ||   0.9567 ||  0.995014 ||  0.995052 ||    97.561324 || 
    Epoch 34    --      0.632804 ||   0.8553 ||   0.9667 ||  0.996198 ||  0.996554 ||      0.716971 ||      0.8211 ||   0.9619 ||  0.996330 ||  0.996265 ||    97.936321 || 
    Epoch 35    --      0.599917 ||   0.8625 ||   0.9706 ||  0.996748 ||  0.997068 ||      0.677501 ||      0.8344 ||   0.9673 ||  0.996682 ||  0.996568 ||    97.561327 || 
    Epoch 36    --      0.560314 ||   0.8744 ||   0.9736 ||  0.997133 ||  0.997427 ||      0.650715 ||      0.8420 ||   0.9676 ||  0.996895 ||  0.996811 ||    98.039210 || 
    Epoch 37    --      0.526847 ||   0.8827 ||   0.9765 ||  0.997606 ||  0.997867 ||      0.624671 ||      0.8548 ||   0.9699 ||  0.996353 ||  0.996340 ||    97.717615 || 
    Epoch 38    --      0.495666 ||   0.8950 ||   0.9787 ||  0.997852 ||  0.998088 ||      0.577891 ||      0.8614 ||   0.9742 ||  0.997165 ||  0.997258 ||    97.819299 || 
    Epoch 39    --      0.472670 ||   0.9004 ||   0.9808 ||  0.997972 ||  0.998202 ||      0.542162 ||      0.8738 ||   0.9789 ||  0.997854 ||  0.997834 ||    97.674256 || 
    Epoch 40    --      0.441182 ||   0.9075 ||   0.9828 ||  0.998389 ||  0.998594 ||      0.503589 ||      0.8882 ||   0.9810 ||  0.998300 ||  0.998270 ||    98.342621 || 
    Epoch 41    --      0.413669 ||   0.9137 ||   0.9859 ||  0.998682 ||  0.998829 ||      0.489584 ||      0.8905 ||   0.9810 ||  0.998133 ||  0.998076 ||    98.014553 || 
    Epoch 42    --      0.389976 ||   0.9197 ||   0.9870 ||  0.998708 ||  0.998863 ||      0.467871 ||      0.8932 ||   0.9829 ||  0.998578 ||  0.998549 ||    98.155145 || 
    Epoch 43    --      0.369496 ||   0.9255 ||   0.9882 ||  0.998908 ||  0.999043 ||      0.424744 ||      0.9096 ||   0.9853 ||  0.998883 ||  0.998857 ||    98.678393 || 
    Epoch 44    --      0.346999 ||   0.9316 ||   0.9897 ||  0.999085 ||  0.999202 ||      0.415490 ||      0.9088 ||   0.9884 ||  0.998898 ||  0.998841 ||    98.717669 || 
    Epoch 45    --      0.349047 ||   0.9309 ||   0.9898 ||  0.999077 ||  0.999197 ||      0.414277 ||      0.9062 ||   0.9880 ||  0.998837 ||  0.998798 ||   100.295783 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                     True
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
