Build arguments
	model_type                               -                                   custom
	input_shape                              -                              (32, 32, 3)
	noof_classes                             -                                      100
	weights                                  -                                     None
	freeze                                   -                                        0
	layers                                   -              ['ftl', 'flatten', 'dense']
Compile arguments
	optimizer                                -                                     adam
	loss                                     -                 categorical_crossentropy
	run_eagerly                              -                                    False
	metrics_000                              -                     categorical_accuracy
	metrics_001                              -                                    top-5
	metrics_002                              -                                     mAUC
	metrics_003                              -                                     uAUC
Train arguments
	epochs                                   -                                      100
	batch                                    -                                        8
	call_time                                -                                     True
	call_stop                                -                                     True
	call_stop_kwargs-baseline                -                                      0.1
	call_stop_kwargs-monitor                 -                 val_categorical_accuracy
	call_stop_kwargs-patience                -                                        2
	call_stop_kwargs-min_delta               -                                    0.001
	call_stop_kwargs-restore_best            -                                     True
	call_checkpoint                          -                                    False
	call_checkpoint_kwargs-monitor           -                 val_categorical_accuracy
	call_checkpoint_kwargs-mode              -                                     auto
	call_checkpoint_kwargs-save_freq         -                                    epoch
	call_checkpoint_kwargs-save_weights_only -                                     True
	call_checkpoint_kwargs-save_best_only    -                                     True
	save_memory                              -                                     True
	save_final                               -                                     True
	validation_split                         -                                      0.2
	verbose                                  -                                        1
	dataset_size                             -                                    50000
CPU - local PC (IP: 180)
Evaluation: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    || 
    Epoch 0     --     38.245174 ||   0.1687 ||   0.4007 ||  0.605283 ||  0.605309 || 
Training history: 
     epochs     --      loss     || cat_acc  ||   top5   ||    mAU    ||    uAU    ||    val_loss   || val_cat_acc || val_top5 ||  val_mAU  ||  val_uAU  ||     time     || 
    Epoch 00    --     11.353750 ||   0.0976 ||   0.2549 ||  0.622799 ||  0.624184 ||     11.384702 ||      0.1484 ||   0.3590 ||  0.659143 ||  0.657523 ||    18.359614 || 
    Epoch 01    --     10.077921 ||   0.2200 ||   0.4685 ||  0.704277 ||  0.705031 ||     11.327976 ||      0.1864 ||   0.4326 ||  0.682063 ||  0.681227 ||    16.508847 || 
    Epoch 02    --      9.449741 ||   0.2661 ||   0.5383 ||  0.729138 ||  0.729767 ||     11.056638 ||      0.2363 ||   0.5045 ||  0.707169 ||  0.706020 ||    16.615526 || 
    Epoch 03    --      8.805184 ||   0.3174 ||   0.6016 ||  0.753777 ||  0.754292 ||     10.498738 ||      0.2868 ||   0.5612 ||  0.726907 ||  0.726008 ||    16.326181 || 
    Epoch 04    --      8.135027 ||   0.3655 ||   0.6554 ||  0.776464 ||  0.777019 ||      9.192580 ||      0.3300 ||   0.6215 ||  0.758302 ||  0.757182 ||    16.581228 || 
    Epoch 05    --      7.506498 ||   0.4073 ||   0.7020 ||  0.796016 ||  0.796473 ||      8.265188 ||      0.3841 ||   0.6802 ||  0.777414 ||  0.777066 ||    16.811733 || 
    Epoch 06    --      6.983990 ||   0.4444 ||   0.7432 ||  0.811978 ||  0.812242 ||      7.721873 ||      0.4221 ||   0.7175 ||  0.796578 ||  0.797124 ||    16.405634 || 
    Epoch 07    --      6.512020 ||   0.4805 ||   0.7725 ||  0.826315 ||  0.826623 ||      7.616017 ||      0.4327 ||   0.7432 ||  0.800534 ||  0.799964 ||    16.498435 || 
    Epoch 08    --      6.019365 ||   0.5107 ||   0.8020 ||  0.839434 ||  0.839617 ||      7.494361 ||      0.4655 ||   0.7600 ||  0.808262 ||  0.806456 ||    16.320097 || 
    Epoch 09    --      5.658079 ||   0.5393 ||   0.8267 ||  0.850454 ||  0.850704 ||      6.309902 ||      0.5118 ||   0.8072 ||  0.837843 ||  0.837397 ||    16.865450 || 
    Epoch 10    --      5.238027 ||   0.5722 ||   0.8464 ||  0.860754 ||  0.861054 ||      6.194740 ||      0.5362 ||   0.8222 ||  0.843909 ||  0.844161 ||    16.413123 || 
    Epoch 11    --      4.934595 ||   0.5937 ||   0.8630 ||  0.869288 ||  0.869571 ||      5.884616 ||      0.5433 ||   0.8386 ||  0.850093 ||  0.849378 ||    16.494610 || 
    Epoch 12    --      4.564311 ||   0.6181 ||   0.8807 ||  0.879287 ||  0.879520 ||      5.788847 ||      0.5630 ||   0.8513 ||  0.854894 ||  0.853392 ||    16.621367 || 
    Epoch 13    --      4.356909 ||   0.6351 ||   0.8913 ||  0.885742 ||  0.885952 ||      5.317957 ||      0.5960 ||   0.8699 ||  0.867049 ||  0.865330 ||    16.560853 || 
    Epoch 14    --      4.090436 ||   0.6580 ||   0.9046 ||  0.892643 ||  0.892781 ||      4.961740 ||      0.6154 ||   0.8836 ||  0.875814 ||  0.875130 ||    16.121420 || 
    Epoch 15    --      3.858469 ||   0.6726 ||   0.9150 ||  0.898870 ||  0.899057 ||      4.403650 ||      0.6370 ||   0.9028 ||  0.884045 ||  0.884357 ||    16.547273 || 
    Epoch 16    --      3.639752 ||   0.6916 ||   0.9232 ||  0.905257 ||  0.905405 ||      4.459191 ||      0.6448 ||   0.9095 ||  0.883157 ||  0.883765 ||    16.666118 || 
    Epoch 17    --      3.439788 ||   0.7041 ||   0.9316 ||  0.910123 ||  0.910305 ||      4.083306 ||      0.6686 ||   0.9158 ||  0.896429 ||  0.896034 ||    16.563155 || 
    Epoch 18    --      3.287957 ||   0.7188 ||   0.9387 ||  0.913814 ||  0.913991 ||      4.202533 ||      0.6730 ||   0.9228 ||  0.894559 ||  0.894545 ||    16.301641 || 
    Epoch 19    --      3.100091 ||   0.7315 ||   0.9475 ||  0.918007 ||  0.918175 ||      3.920382 ||      0.6808 ||   0.9289 ||  0.898308 ||  0.897231 ||    16.394979 || 
    Epoch 20    --      2.974092 ||   0.7418 ||   0.9481 ||  0.922779 ||  0.922892 ||      3.585702 ||      0.7112 ||   0.9432 ||  0.909995 ||  0.909692 ||    16.578687 || 
    Epoch 21    --      2.818535 ||   0.7561 ||   0.9536 ||  0.926722 ||  0.926808 ||      3.923179 ||      0.6952 ||   0.9396 ||  0.901522 ||  0.901861 ||    16.207688 || 
    Epoch 22    --      2.809750 ||   0.7542 ||   0.9560 ||  0.926912 ||  0.927003 ||      3.804947 ||      0.7024 ||   0.9358 ||  0.904220 ||  0.904499 ||    16.312861 || 
Layers list:
	ftl                                      -                           (1, 32, 32, 3)
	ftl-activation                           -                                     relu
	ftl-kernel_initializer                   -                                he_normal
	ftl-train_imaginary                      -                                    False
	ftl-inverse                              -                                    False
	ftl-use_bias                             -                                    False
	ftl-bias_initializer                     -                                    zeros
	ftl-calculate_abs                        -                                    False
	ftl-normalize_to_image_shape             -                                    False
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	flatten_2                                -                                         
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	dense_2                                  -                       (6144, 100)|(100,)
	dense_2-units                            -                                      100
	dense_2-activation                       -                                  softmax
	dense_2-use_bias                         -                                     True
	dense_2-kernel_initializer               -                           glorot_uniform
	dense_2-bias_initializer                 -                                    zeros
	dense_2-kernel_regularizer               -                                     None
	dense_2-bias_regularizer                 -                                     None
	dense_2-activity_regularizer             -                                     None
	dense_2-kernel_constraint                -                                     None
	dense_2-bias_constraint                  -                                     None
	######################################## - XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
ftl (FTL)                    (None, 32, 32, 6)         3072      
_________________________________________________________________
flatten_2 (Flatten)          (None, 6144)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               614500    
=================================================================
Total params: 617,572
Trainable params: 617,572
Non-trainable params: 0
_________________________________________________________________
